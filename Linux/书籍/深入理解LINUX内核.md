## 第1章 绪论

Linux是类Unix (Unix-like)操作系统大家族中的一名成员。1991年，Linus Torvalds开发出最初的Linux，它作为一个适用于基于Intel 80386微处理器的IBM PC兼容机的操作系统。Linux最吸引人的一个优点就在于它不是商业操作系统：它的源代码在GNU公共许可证(General Pwblic License， GPL)下是开放的，任何人都可以获得源代码并研究它。

从技术角度来说，Linux是一个真正的Unix内核，但它不是一个完全的Unix操作系统，这是因为它不包含全部的Unix应用程序，诸如文件系统实用程序、窗口系统及图形化桌面、系统管理员命令、文本编辑程序、编译程序等等。不过，因为以上大部分应用程序都可在GNU许可证下免费获得，因此，可以把它们安装在任何一个基于Linux内核的系统中。

### Linux 和 其他类Unix内核的比较

Linux与一些著名的商用Unix内核到底如何竞争，下面给予描述：

- 单块结构的内核(Monolithic kernel)

  它是一个庞大、复杂的自我完善(do-it-yourself)程序，由几个逻辑上独立的成分构成。在这一点上，它是相当传统的，大多数商用Unix变体也是单块结构。(一个显著的例外是Apple的Mac Os X和GNU的Hurd操作系统，它们都是从卡耐基-梅隆大学的Mach演变而来的，都遵循微内核的方法。)


- 编泽并静态连接的传统Unix内核

  大部分现代操作系统内核可以动态地装载和卸载部分内核代码(典型的例子如设备驱动程序)，通常把这部分代码称做模块(module)。Linux对模块的支持是很好的，因为它能自动按需装载或卸载模块。在主要的商用Unix变体中，只有SVR4.2和Solaris内核有类似的特点。


- 内核线程

  一些Unix内核，如Solaris和SVR4.2/MP，被组织成一组内核线程( kernel thread )。内核线程是一个能被独立调度的执行环境(context)，也许它与用户程序有关，也许仅仅执行一些内核函数。线程之间的上下文切换比普通进程之间的上下文切换花费的代价要少得多，因为前者通常在同一个地址空间执行。Linux以一种十分有限的方式使用内核线程来周期性地执行几个内核函数，但是，它们并不代表基本的执行上下文的抽象(这就是下面要讨论的议题)。


- 多线程应用程序支持

  大多数现代操作系统在某种程度上都支持多线程应用程序，也就是说，这些用户程序是根据很多相对独立的执行流来设计的，而这些执行流之间共享应用程序的大部分数据结构。一个多线程用户程序由很多轻量级进程(lightweight process， LWP)组成，这些进程可能对共同的地址空间、共同的物理内存页、共同的打开文件等等进行操作。Linux定义了自己的轻量级进程版本，这与SVR4， Solaris等其他系统上所使用的类型有所不同。当LWP的所有商用Unix变体都基于内核线程时，Linux却把轻量级进程当作基本的执行上下文，通过非标准的clone()系统调用来处理它们。


- 抢占式(preemptive) 

  当采用“可抢占的内核”选项来编译内核时，Linux2.6可以随意交错执行处于特权模式的执行流。除了Linux 2.6，还有其他一些传统的、通用的Unix系统(如Solaris和Mach3.0)是完全的抢占式内核。S V R4.2/M P通过引入一些固定抢占点(fixed preemption goint)的方法获得有限的抢占能力。


- 多处理器支持

  几种Unix内核变体都利用了多处理器系统。Linux 2.6支持不同存储模式的对称多处理(SMP)，包括NUMA：系统不仅可以使用多处理器，而且每个处理器可以毫无区别地处理任何一个任务。尽管通过一个单独的“大内核锁”使得内核中的少数代码依然串行执行，但公平地说，Linux 2.6以几乎最优化的方式使用SMP。


- 文件系统

  Linux标准文件系统呈现出多种风格。如果你没有特殊需要，就可以使用普通的Ext2文件系统。如果你想避免系统崩溃后冗长的文件系统检查，就可以切换到Ext3。如果你不得不处理很多小文件，ReiserFS文件系统可能就是最好的选择。除了Ext3和ReiserFS，还可以在Linux中使用另外几个日志文件系统，这些文件系统包括IBM AIX的日志文件系统(Journaling File System， JFS)和SGI公司IRIX 系统上的XFS文件系统。有了强大的面向对象虚拟文件系统技术(为Solaris和SVR4所采用)，把外部文件系统移植到Linux比移植到其他内核相对要容易。


- STREAMS

  尽管现在大部分的Unix内核内包含了SRV4引入的STREAMS I/O子系统，并且 已变成编写设备驱动程序、终端驱动程序及网络协议的首选接口，但是Linux并没有与此类似的子系统。

与商业化的操作系统相比，Linux已经具备足够的竞争力。而且，Linux一些独具特色的特点使其成为一种趣味盎然的操作系统。商业化的Unix内核为了赢得更大的市场份额通常也引入了新特征，但这些特征本是可有可无，其稳定性和效率都值得商榷。事实上，现代Unix内核有向更臃肿变化的倾向，而Linux以及其他开放源代码的操作系统不受市场因素的制约，因此可以根据设计者的想法(主要是Linus Torvalds的想法)自由地演进。尤其是，与商用竞争对手相比，Linux有如下优势：

- Linux是免费的。除硬件之外，你无需任何花费就能安装一套完整的Linux系统。

- Linux的所有成分都可以充分地定制。通过内核编译选项，你可以选择自己真正需要的特征来定制内核。
- Linux可以运行在低档、便宜的硬件平台上。你可以用一个4MB内存的旧Intel 80386系统构建网络服务器。
- Linux是强大的。由于充分挖掘了硬件部分的特点，使得Linux系统速度非常快。Linux的主要目标是效率，所以，商用系统的许多设计选择由于有降低性能的隐患而被Linus舍弃，如STREAMSI/O子系统。
- Linux的开发者都是非常出色的程序员。Linux系统非常稳定，有非常低的故障率和非常少系统维护时间。
- Linux内核非常小，而且紧凑。我们甚至可以把一个内核映像和一些系统程序放在一张1.4MB的软盘上!据我们所知，没有一个商用Unix变体能从一张软盘上启动。
- Linux与很多通用操作系统商度兼容。Linux可以让你直接安装以下文件系统的所有版本：MS-DOS和MS Windows， SVR4， OS/2，  Mac OS X， Solaris， SunOS， NEXTSTEP，还有很多BSD变体等等。另外， Linux也能对很多网络层进行操作，这些网络层如以太网[如：快速以太网和高速(Gbit/s及lOGbit/s)以太网]、光纤分布式数据接口(Fiber Distributed Data Interface， FDDI)、高性能并行接口( High Performance Parallel Interface，  HIPPI ) ，  IEEE 802.11(无线局域网)和IEEE802.15(蓝牙)。。通过使用适当的库函数，Linux系统甚至能直接运行为其他操作系统所编写的程序。例如，Linux能执行为以下操作系统所编写的应用程序： MS-DOS，  MS Windows， SVR3及SV R4， 4.4BSD，  SCO Unix， Xenix，以及其他在Intel 80x86平台上运行的操作系统。
- Linux有很好的技术支持。

### 硬件的依赖性

Linux试图在硬件无关的源代码与硬件相关的源代码之间保持清晰的界限。为了做到这点，在arch和include目录下包含了23个子目录，以对应Linux所支持的不同硬件平台。这些平台的标准名字如下：

| 平台            | 简介                                       |
| ------------- | ---------------------------------------- |
| alpha         | HP的Alpha工作站，最早属于Digital公司，后来属于Cpmpag公司，现在不再生产。 |
| arm，arm26     | 基于ARM处理器的计算机（如PDA）和嵌入式设备。                |
| cris          | Axis在它的瘦服务器中使用的“代码精简指令集（Code Reduced Instruction Set）”CPU，用在诸如Web摄像机或开发主板中。 |
| frv           | 基于Fujitsu FR-V系列微处理器的嵌入式系统。              |
| h8300         | Hitachi h8/300 和 h8S的8位和16位RISC微处理器。     |
| i386          | 基于80x86微处理器的IBM兼容个人计算机。                  |
| ia64          | 基于64位Itanium微处理器的工作站。                    |
| m32r          | 基于Renesas M32R系列微处理器的计算机。                |
| m68k，m68nommu | 基于Motorola MC680x0微处理器的个人计算机。            |
| mips          | 基于MIPS微处理器的工作站。                          |
| parisc        | 基于HP公司HP 9000 PA-RISC微处理器的工作站。           |
| ppc，ppc64     | 基于Motorola-IBM PowerPC32位和64位微处理器的工作站。   |
| s390          | IBM ESA/390及zSeries大型机。                  |
| sh，sh64       | 基于Hitachi和STMicroelectronics联合开发的SuperH微处理器的嵌入式系统。 |
| sparc，sparc64 | 基于Sun公司SPARC和64位Ultra SPARC微处理器的工作站。     |
| um            | 用户态的Linux——一个允许开发者在用户态下运行内核的虚拟平台。        |
| v850          | 集成了基于Harvard体系结构的32位RISC核心的NEC V850微控制器。 |
| x86_64        | 基于AMD的64位微处理器的工作站。                       |

### Linux版本

一直到2.5版本的内核，Linux都通过简单的编号来区别内核的稳定版和开发版。每个版本号用三个数字描述，由圆点分隔。前两个数字用来表示版本号，第三个数字表示发布号。第二位版本号表示内核的类型：如果为偶数，表示稳定的内核；否则，表示开发中的内核。

在Linux内核2.6版的开发过程中，内核版本的编号方式发生了很大的变化。主
要变化在于第二个数字已经不再用于表示一个内核是稳定版本还是正在开发的版本。因此，现在内核开发者都在当前的2.6版本中对内核进行大幅改进。只有在内核开发者必须对内核的重大修改进行测试时，才会采用一个新的内核分支。这种分支要么产生一个新的内核版本，要么干脆丢弃所修改的部分而回退到2.6版。

Linux这种新的开发模式意味着两种内核具有相同的版本号，但却有不同的发布号，如2.6.10和2.6.11内核就可能在核心部件和基本算法上有很大的差别。这样一来，具有新发布号的内核可能潜藏着不稳定性和各种错误。为了解决这个问题，内核开发者可能发布带有补丁程序的内核版本，并且用第四位数字表示带有不同补丁的内核版本。例如，2.6.11.12。

必须强调的是本书描述的是Linux2.6.11版的内核。

### 操作系统基本概念

任何计算机系统都包含一个名为操作系统的基本程序集合。在这个集合里，最重要的程序称为内核(kernel)。当操作系统启动时，内核被装入到RAM中，内核中包含了系统运行所必不可少的很多核心过程(procedure)。其他程序是一些不太重要的实用程序，尽管这些程序为用户提供了与计算机进行广泛交流的经验(以及用户买计算机要做的所有工作)，但系统根本的样子和能力还是由内核决定。内核也为系统中所有事情提供了主要功能，并决定高层软件的很多特性。因此，我们将经常使用术语“操作系统”作为“内核”的同义词。操作系统必须完成两个主要目标：

- 与硬件部分交互，为包含在硬件平台上的所有低层可编程部件提供服务。
- 为运行在计算机系统上的应用程序(即所谓用户程序)提供执行环境。

一些操作系统允许所有的用户程序都直接与硬件部分进行交互(典型的例子是MS-DOS)。与此相反，类Unix操作系统把与计算机物理组织相关的所有低层细节都对用户运行的程序隐藏起来。当程序想使用硬件资源时，必须向操作系统发出一个请求。内核对这个请求进行评估，如果允许使用这个资源，那么，内核代表应用程序与相关的硬件部分进行交互。

为了实施这种机制，现代操作系统依靠特殊的硬件特性来禁止用户程序直接与低层硬件部分进行交互，或者禁止直接访问任意的物理地址。特别是，硬件为CPU引入了至少两种不同的执行模式：用户程序的非特权模式和内核的特权模式。Unix把它们分别称为用户态(User Mode)和内核态(Kernel Mode )。

#### 多用户系统

多用户系统(multiuser system)就是一台能并发和独立地执行分别属于两个或多个用户的若干应用程序的计算机。

- “并发”(concurrently)意味着几个应用程序能同时处于活动状态并竟争各种资源，如CPU、内存、硬盘等等。
- “独立”( independently)意味着每个应用程序能执行自己的任务，而无需考虑其他用户的应用程序在干些什么。

当然，从应用程序切换会使每个应用程序的速度有所减慢，从而影响用户看到的响应时间。现代操作系统内核提供的许多复杂特性减少了强加在每个程序上的延迟时间，给用户提供了尽可能快的响应时间。

多用户操作系统必须包含以下几个特点：

- 核实用户身份的认证机制。
- 防止有错误的用户程序防碍其他应用程序在系统中运行的保护机制。
- 防止有恶意的用户程序干涉或窥视其他用户的活动的保护机制。
- 限制分配给每个用户的资源数的计账机制。

为了确保能实现这些安全保护机制，操作系统必须利用与CPU特权模式相关的硬件保护机制，否则，用户程序将能直接访问系统电路并克服强加于它的这些限制。unix是实施系统资源硬件保护的多用户系统。

#### 用户和组

在多用户系统中，每个用户在机器上都有私用空间；典型地，他拥有一定数量的磁盘空间来存储文件、接收私人邮件信息等等。操作系统必须保证用户空间的私有部分仅仅对其拥有者是可见的。特别是必须能保证，没有用户能够开发一个用于侵犯其他用户私有空间的系统应用程序。

所有的用户由一个惟一的数字来标识，这个数字叫用户标识符(User ID，  UID)。通常一个计算机系统只能由有限的人使用。当其中的某个用户开始一个工作会话时，操作系统要求输入一个登录名和口令，如果用户输入的信息无效，则系统拒绝访问。因为口令是不公开的，所以用户的保密性得到了保证。

为了和其他用户有选择地共享资料，每个用户是一个或多个用户组的一名成员，组由唯一的用户组标识符(user group ID)标识。每个文件也恰好与一个组相对应。例如，可以设置这样的访问权限，拥有文件的用户具有对文件的读写权限，同组用户仅有只读权限，而系统中的其他用户没有对文件的任何访问权限。

任何类Unix操作系统都有一个特殊的用户，叫做root，即超级用户(superuser)。系统管理员必须以root的身份登录，以便处理用户账号，完成诸如系统备份、程序升级等维护任务。root用户几乎无所不能，因为操作系统对他不使用通常的保护机制。尤其是，root用户能访向系统中的每一个文件，能干涉每一个正在执行的用户程序的活动。

#### 进程

所有的操作系统都使用一种基本的抽象：进程(process)。一个进程可以定义为：“程序执行时的一个实例”，或者一个运行程序的“执行上下文”。在传统的操作系统中，一个进程在地址空间(address space)中执行一个单独的指令序列。地址空间是允许进程引用的内存地址集合。现代操作系统允许具有多个执行流的进程，也就是说，在相同的地址空间可执行多个指令序列。

多用户系统必须实施一种执行环境，在这种环境里，几个进程能并发活动，并能竟争系统资源(主要是CPU )。允许进程并发活动的系统称为多道程序系统(multiprogramming)或多处理系统(multiprocessing)。区分程序和进程是非常重要的：几个进程能并发地执行同一程序，而同一个进程能顺序地执行几个程序。

在单处理器系统上，只有一个进程能占用CPU，因此，在某一时刻只能有一个执行流。一般来说，CPU的个数总是有限的，因而只有少数几个进程能同时执行。操作系统中叫做调度程序(scheduler)的部分决定哪个进程能执行。一些操作系统只允许有非抢占式(nonpreemptable)进程，这就意味着，只有当进程自愿放弃CPU时，调度程序才被调用。但是，多用户系统中的进程必须是抢占式的(preemptable )。操作系统记录下每个进程占有的CPU时间，并周期性地激活调度程序。

Unix是具有抢占式进程的多处理操作系统。即使没有用户登录，没有程序运行，也还是有几个系统进程在监视外围设备。尤其是，有几个进程在监听系统终端等待用户登录。当用户输入一个登录名，监听进程就运行一个程序来验证用户的口令。如果用户身份得到证实，那么监听进程就创建另一个进程来执行shell，此时在shell下可以输入命令。当一个图形化界面被激活时，有一个进程就运行窗口管理器，界面上的每个窗口通常都由一个单独的进程来执行。如果用户创建了一个图形化shell，那么，一个进程运行图形化窗口，而第二个进程运行用户可以输入命令的shell。对每一个用户命令，shell进程都创建执行相应程序的另一个进程。

Unix操作系统采用进程/内核模式。每个进程都自以为它是系统中唯一的进程，可以独占操作系统所提供的服务。只要进程发出系统调用(即对内核提出请求)，硬件就会把特权模式由用户态变成内核态，然后进程以非常有限的目的开始一个内核过程的执行。这样，操作系统在进程的执行上下文中起作用，以满足进程的请求。一旦这个请求完全得到满足，内核过程将迫使硬件返回到用户态，然后进程从系统调用的下一条指令继续执行。

#### 内核体系结构

如前所述，大部分Unix内核是单块结构：每一个内核层都被集成到整个内核程序中，并代表当前进程在内核态下运行。相反，微内核(microkernel)操作系统只需要内核有一个很小的函数集，通常包括几个同步原语、一个简单的调度程序和进程间通信机制。运行在微内核之上的几个系统进程实现从前操作系统级实现的功能，如内存分配程序、设备驱动程序、系统调用处理程序等等。

尽管关于操作系统的学术研究都是面向微内核的，但这样的操作系统一般比单块内核的效率低，因为操作系统不同层次之间显式的消息传递要花费一定的代价。不过，微内核操作系统比单块内核有一定的理论优势。微内核操作系统迫使系统程序员采用模块化的方法，因为任何操作系统层都是一个相对独立的程序，这种程序必须通过定义明确而清晰的软件接口与其他层交互。此外，已有的微内核操作系统可以很容易地移植到其他的体系结构上，因为所有与硬件相关的部分都被封装进微内核代码中。最后，微内核操作系统比单块内核更加充分地利用了RAM，因为暂且不需要执行的系统进程可以被调出或撤消。

为了达到微内核理论上的很多优点而又不影响性能，Linux内核提供了模块(module) 。模块是一个目标文件，其代码可以在运行时链接到内核或从内核解除链接。这种目标代码通常由一组函数组成，用来实现文件系统、驱动程序或其他内核上层功能。与微内核操作系统的外层不同，模块不是作为一个特殊的进程执行的。相反，与任何其他静态链接的内核函数一样，它代表当前进程在内核态下执行。

使用模块的主要优点包括：

- 摸块化方法

  任何模块都可以在运行时被链接或解除链接，因此，系统程序员必须提出良定义的软件接口以访问由模块处理的数据结构。这使得开发新模块变得容易。

- 平台无关性

  即使模块依赖于某些特殊的硬件特点，但它不依赖于某个固定的硬件平台。例如，符合SCSI标淮的磁盘驱动程序模块，在IBM兼容PC与HP的Alpha机上都能很好地工作。

- 节省内存使用

  当需要模块功能时，把它链接到正在运行的内核中，否则，将该模块解除链接。这种机制对于小型嵌入式系统是非常有用的。

- 无性能损关

  模块的目标代码一旦被链接到内核，其作用与静态链接的内核的目标代码完全等价。因此，当模块的函数被调用时，无需显式地进行消息传递。

### Unix文件系统概述

Unix操作系统的设计集中反映在其文件系统上。

#### 文件

Unix文件是以字节序列组成的信息载体(container)，内核不解释文件的内容。很多编程的库函数实现了更高级的抽象，例如，由字段构成的记录以及基于关键字编址的记录。然而，这些库中的程序必须依靠内核提供的系统调用。从用户的观点来看，文件被组织在一个树结构的命名空间中。

除了叶节点之外，树的所有节点都表示目录名。目录节点包含它下面文件及目录的所有信息。文件或目录名由除“/”和空字符“\0”之外的任意ASCII字符序列组成。大多数文件系统对文件名的长度都有限制，通常不能超过255个字符。与树的根相对应的目录被称为根目录(root directory)。按照惯例，它的名字是“/”。在同一目录中的文件名不能相同，而在不同目录中的文件名可以相同。

Unix的每个进程都有一个当前工作目录，它属于进程执行上下文(execution context)，标识出进程所用的当前目录。为了标识一个特定的文件，进程使用路径名(pcothname )，路径名由斜杠及一列指向文件的目录名交替组成。如果路径名的第一个字符是斜杠，那么这个路径就是所谓的绝对路径，因为它的起点是根目录。否则，如果第一项是目录名或文件名，那么这个路径就是所谓的相对路径，因为它的起点是进程的当前目录。

当标识文件名时，也用符号“.”和“..”。它们分别标识当前工作目录和父目录。如果当前工作目录是根目录，“.”和“..”就是一致的。

#### 硬链接和软链接

包含在目录中的文件名就是一个文件的硬链接(hard link)，或简称链接(Link)。在同一目录或不同的目录中，同一文件可以有几个链接，因此对应几个文件名。

```shell
ln P1 P2
```

用来创建一个新的硬链接，即为由路径P1标识的文件创建一个路径名为P2的硬链接。
硬链接有两方面的限制：

- 不允许用户给目录创建硬链接。因为这可能把目录树变为环形图，从而就不可能通过名字定位一个文件。
- 只有在同一文件系统中的文件之间才能创建链接。这带来比较大的限制，因为现代Unix系统可能包含了多种文件系统，这些文件系统位于不同的磁盘和/或分区，用户也许无法知道它们之间的物理划分。

为了克服这些限制，引入了软链接(soft link)[也称符号链接(symbolic link)]。符号链接是短文件，这些文件包含有另一个文件的任意一个路径名。路径名可以指向位于任意一个文件系统的任意文件或目录，甚至可以指向一个不存在的文件。

```shell
ln -s  P1  P2
```

创建一个路径名为P2的新软链接，P2指向路径名P1。当这个命令执行时，文件系统抽出P2的目录部分，并在那个目录下创建一个名为P2的符号链接类型的新项。这个新文件包含路径名P1。这样，任何对P2的引用都可以被自动转换成指向P1的一个引用。

#### 文件类型

- 普通文件(regular file)
- 目录
- 符号链接
- 面向块的设备文件(block-oriented device file)
- 面向字符的设备文件(character-oriented device file)
- 管道(pipe)和命名管道(named pipe )(也叫F1F0 )
- 套接字(socket)

前三种文件类型是所有Unix文件系统的基本类型。

设备文件与I/O设备以及集成到内核中的设备驱动程序相关。例如，当程序访问设备文件时，它直接访问与那个文件相关的I/O设备。

管道和套接字是用于进程间通信的特殊文件。

#### 文件描述符与索引点

Unix对文件的内容和描述文件的信息给出了清楚的区分。除了设备文件和特殊文件系统文件外，每个文件都由字符序列组成。文件内容不包含任何控制信息，如文件长度或文件结束(end-of-file，EOF )符。
文件系统处理文件需要的所有信息包含在一个名为索引节点(inode)的数据结构中。每个文件都有自己的索引节点，文件系统用索引节点来标识文件。

虽然文件系统及内核函数对索引节点的处理可能随Unix系统的不同有很大的差异，但它们必须至少提供在POSIX标准中指定的如下属性：

- 文件类型
- 与文件相关的硬链接个数
- 以字节为单位的文件长度
- 设备标识符(即包含文件的设备的标识符)
- 在文件系统中标识文件的索引节点号
- 文件拥有者的UID
- 文件的用户组ID
- 几个时间戳，表示索引节点状态改变的时间、最后访问时间及最后修改时间
- 访问权限和文件模式

#### 访问权限和文件模式

文件的潜在用户分为三种类型：

- 作为文件所有者的用户
- 同组用户，不包括所有者
- 所有剩下的用户(其他)

有三种类型的访问权限——读、写及执行，每组用户都有这三种权限。因此，文件访问权限的组合就用9种不同的二进制来标记。还有三种附加的标记，即：uid (Set User ID)，sgid (Set Group ID)，及sticky用来定义文件的模式。当这些标记应用到可执行文件时有如下含义：

- suid

  进程执行一个文件时通常保持进程拥有者的UID。然而，如果设置了可执行文件suid的标志位，进程就获得了该文件拥有者的UID。

- sgid

  进程执行一个文件时保持进程组的用户组ID。然而，如果设置了可执行文件sgid的标志位，进程就获得了该文件用户组的ID。

- sticky

  设置了sticky标志位的可执行文件相当于向内核发出一个请求，当程序执行结   束以后，依然将它保留在内存(已过时)。

当文件由一个进程创建时，文件拥有者的ID就是该进程的UID。而其用户组ID可以是进程创建者的ID，也可以是父目录的ID，这取决于父目录sgid标志位的值。

#### 文件操作的系统调用

当用户访问一个普通文件或目录文件的内容时，他实际上是访问存储在硬件块设备上的一些数据。从这个意义上说，文件系统是硬盘分区物理组织的用户级视图。因为处于用户态的进程不能直接与低层硬件交互，所以每个实际的文件操作必须在内核态下进行。因此，Unix操作系统定义了几个与文件操作有关的系统调用。

所有Unix内核都对硬件块设备的处理效率给予极大关注，其目的是为了获得非常好的系统整体性能。在后面的章节中，我们将描述Linux与文件操作相关的主题，尤其是讨论内核如何对文件相关的系统调用作出反应。为了理解这些内容，你需要知道如何使用文件操作的主要系统调用。下面对此给予描述。

##### 打开文件

进程只能访问“打开的”文件。为了打开一个文件，进程调用系统调用：

```c
fd = open(path， flag， mode);
```

- path：表示被打开文件的(相对或绝对)路径。
- flag：指定文件打开的方式(例如，读、写、读/写、追加)。它也指定是否应当创建一个不存在的文件。
- mode：指定新创建文件的访问权限。

这个系统调用创建一个“打开文件”对象，并返回所谓文件描述符(file descriptor)的标识符。一个打开文件对象包括：

- 文件操作的一些数据结构，如指定文件打开方式的一组标志;表示文件当前位置的offset字段，从这个位置开始将进行下一个操作(即所谓的文件指针)，等等。
- 进程可以调用的一些内核函数指针。这组允许调用的函数集合由参数flag的值决定。

POSIX语义所指定的一般特性：

-  文件描述符表示进程与打开文件之间的交互，而打开文件对象包含了与这种交互相关的数据。同一打开文件对象也许由同一个进程中的几个文件描述符标识。
-  几个进程也许同时打开同一文件。在这种情况下，文件系统给每个文件分配一个单独的打开文件对象以及单独的文件描述符。当这种情况发生时，Unix文件系统对进程在同一文件上发出的I/O操作之间不提供任何形式的同步机制。然而，有几个系统调用，如flock()，可用来让进程在整个文件或部分文件上对I/O操作实施同步。

为了创建一个新的文件，进程也可以调用create()系统调用，它与open()非常相似，都是由内核来处理。

##### 访问打开的文件

对普通Unix文件，可以顺序地访问，也可以随机地访问，而对设备文件和命名管道文件，通常只能顺序地访问。在这两种访问方式中，内核把文件指针存放在打开文件对象中，也就是说，当前位置就是下一次进行读或写操作的位置。

顺序访问是文件的默认访问方式，即read()和write()系统调用总是从文件指针的当前位置开始读或写。为了修改文件指针的值，必须在程序中显式地调用lseek ()系统调用。当打开文件时，内核让文件指针指向文件的第一个字节(偏移量为0)。

```c
newoffset=lseek(fd， offset，whence);
```

- fd：表示打开文件的文件描述符。
- offset：指定一个有符号整数值，用来计算文件指针的新位置。
- whence：指定文件指针新位置的计算方式。可以是offset加0，表示文件指针从文件头移动，也可以是offset加文件指针的当前位置，表示文件指针从当前位置移动;还可以是offset加文件最后一个字节的位置，表示文件指针从文件末尾开始移动。

```c
nread= read(fd， buf，count);
```

- fd：表示打开文件的文件描述符。
- buf：指定在进程地址空间中缓冲区的地址，所读的数据就放在这个缓冲区。
- count：表示所读的字节数。

当处理这样的系统调用时，内核会尝试从拥有文件描述符fd的文件中读count个字节，其起始位置为打开文件的offset字段的当前值。在某些情况下可能遇到文件结束、空管道等等，因此内核无法成功地读出全部count个字节。返回的nead值就是实际所读的字节数。给原来的值加上nread就会更新文件指针。write()的参数与read()相似。

##### 关闭文件

当进程无需再访问文件的内容时，就调用系统调用：

```c
res=close(fd);
```

释放与文件描述符fd相对应的打开文件对象。当一个进程终止时，内核会关闭其所有仍然打开着的文件。

##### 更名及删除文件

要重新命名或删除一个文件时，进程不需要打开它。实际上，这样的操作并没有对这个文件的内容起作用，而是对一个或多个目录的内容起作用。

```c
/*改变了文件链接的名字*/
res= rename(oldpath， newpath);
```

```c
/*减少了文件链接数，删除了相应的目录项。只有当链接数为0时，文件才被真正删除。*/
res= unlink(pathname);
```

### Unix 内核概述

Unix内核提供了应用程序可以运行的执行环境。因此，内核必须实现一组服务及相应的接口。应用程序使用这些接口，而且通常不会与硬件资源直接交互。

#### 进程/内核模式

CPU既可以运行在用户态下，也可以运行在内核态下。实际上，一些CPU可以有两种以上的执行状态。例如，Intel 80x86微处理器有四种不同的执行状态。但是，所有标准的Unix内核都仅仅利用了内核态和用户态。

当一个程序在用户态下执行时，它不能直接访问内核数据结构或内核的程序。然而，当应用程序在内核态下运行时，这些限制不再有效。每种CPU模型都为从用户态到内核态的转换提供了特殊的指令，反之亦然。一个程序执行时，大部分时间都处在用户态下，只有需要内核所提供的服务时才切换到内核态。当内核满足了用户程序的请求后，它让程序又回到用户态下。

进程是动态的实体，在系统内通常只有有限的生存期。创建、撤消及同步现有进程的任务都委托给内核中的一组例程来完成。

内核本身并不是一个进程，而是进程的管理者。进程/内核模式假定：请求内核服务的进程使用所谓系统调用(system call)的特殊编程机制。每个系统调用都设置了一组识别进程请求的参数，然后执行与硬件相关的CPU指令完成从用户态到内核态的转换。

除用户进程之外，Unix系统还包括几个所谓内核线程(kernel thread)的特权进程(被赋予特殊权限的进程)，它们具有以下特点：

- 以内核态运行在内核地址空间。
- 不与用户直接交互，因此不需要终端设备。
- 通常在系统启动时创建，然后一直处于活跃状态直到系统关闭。

在单处理器系统中，任何时候只有一个进程在运行，或处于用户态，或处于内核态。如果进程运行在内核态，处理器就执行一些内核例程。

Unix内核做的工作远不止处理系统调用。实际上，可以有几种方式激活内核例程：

- 进程调用系统调用。
- 正在执行进程的CPU发出一个异常(exception)信号，异常是一些反常情况，例如一个无效的指令。内核代表产生异常的进程处理异常。
- 外围设备向CPU发出一个中断(interrupt)信号以通知一个事件的发生，如一个要求注意的请求、一个状态的变化或一个I/O操作已经完成等。每个中断信号都是由内核中的中断处理程序(interrupt handler)来处理的。因为外围设备与CPU异步操作，因此，中断在不可预知的时间发生。
- 内核线程被执行。因为内核线程运行在内核态，因此必须认为其相应程序是内核的一部分。

#### 进程实现

为了让内核管理进程，每个进程由一个进程描述符(process descriptor)表示，这个描述符包含有关进程当前状态的信息。
当内核暂停一个进程的执行时，就把几个相关处理器寄存器的内容保存在进程描述符中。这些寄存器包括：

- 程序计数器(PC) 和 栈指针(SP)寄存器
- 通用寄存器
- 浮点寄存器
- 包含CPU状态信息的处理器控制寄存器(处理器状态字，Processor Status Word)
- 用来跟踪进程对RAM访问的内存管理寄存器

当内核决定恢复执行一个进程时，它用进程描述符中合适的字段来装载CPU寄存器。因为程序计数器中所存的值指向下一条将要执行的指令，所以进程从它停止的地方恢复执行。

当一个进程不在CPU上执行时，它正在等待某一事件。Unix内核可以区分很多等待状态，这些等待状态通常由进程描述符队列实现。每个(可能为空)队列对应一组等待特定事件的进程。

#### 可重入内核

所有的Unix内核都是可重入的(reentrant)，这意味着若干个进程可以同时在内核态下执行。当然，在单处理器系统上只有一个进程在真正运行，但是有许多进程可能在等待CPU或某一I/O操作完成时在内核态下被阻塞。例如，当内核代表某一进程发出一个读磁盘请求后，就让磁盘控制器处理这个请求并且恢复执行其他进程。当设备满足了读请求时，有一个中断就会通知内核，从而以前的进程可以恢复执行。

提供可重入的一种方式是编写函数，以便这些函数只能修改局部变量，而不能改变全局数据结构，这样的函数叫可重入函数。但是可重入内核不仅仅局限于这样的可重入函数(尽管一些实时内核正是如此实现的)。相反，可重入内核可以包含非重入函数，并且利用锁机制保证一次只有一个进程执行一个非重入函数。

如果一个硬件中断发生，可重入内核能挂起当前正在执行的进程，即使这个进程处于内核态。这种能力是非常重要的，因为这能提高发出中断的设备控制器的吞吐量。一旦设备已发出一个中断，它就一直等待直到CPU应答它为止。如果内核能够快速应答，设备控制器在CPU处理中断时就能执行其他任务。

现在，让我们看一下内核的可重入性及它对内核组织的影响。内核控制路径(kernel control path)表示内核处理系统调用、异常或中断所执行的指令序列。在最简单的情况下，CPU从第一条指令到最后一条指令顺序地执行内核控制路径。然而，当下述事件之一发生时，CPU交错执行内核控制路径：

- 运行在用户态下的进程调用一个系统调用，而相应的内核控制路径证实这个请求无法立即得到满足，然后，内核控制路径调用调度程序选择一个新的进程投入运行。结果，进程切换发生。第一个内核控制路径还没完成，而CPU又重新开始执行其他的内核控制路径。在这种情况下，两条控制路径代表两个不同的进程在执行。
- 当运行一个内核控制路径时，CPU检测到一个异常(例如，访问一个不在RAM中的页)。第一个控制路径被挂起，而CPU开始执行合适的过程。在我们的例子中，这种过程能给进程分配一个新页，并从磁盘读它的内容。当这个过程结束时，第一个控制路径可以恢复执行。在这种情况下，两个控制路径代表同一个进程在执行。
- 当CPU正在运行一个启用了中断的内核控制路径时，一个硬件中断发生。第一个内核控制路径还没执行完，CPU开始执行另一个内核控制路径来处理这个中断。当这个中断处理程序终止时，第一个内核控制路径恢复。在这种情况下，两个内核控制路径运行在同一进程的可执行上下文中，所花费的系统CPU时间都算给这个进程。然而，中断处理程序无需代表这个进程运行。
- 在支持抢占式调度的内核中，CPU正在运行，而一个更高优先级的进程加入就绪队列，则中断发生。在这种情况下，第一个内核控制路径还没有执行完，CPU代表高优先级进程又开始执行另一个内核控制路径。只有把内核编译成支持抢占式调度之后，才可能出现这种情况。

下图显示了非交错的和交错的内核控制路径的几个例子。考虑以下三种不同的CPU状态：

- 在用户态下运行一个进程(User)
- 运行一个异常处理程序或系统调用处理程序(Excp)
- 运行一个中断处理程序(Intr)

![内核控制路径交错执行.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E5%86%85%E6%A0%B8%E6%8E%A7%E5%88%B6%E8%B7%AF%E5%BE%84%E4%BA%A4%E9%94%99%E6%89%A7%E8%A1%8C.jpg?raw=true)

#### 进程地址空间

每个进程运行在它的私有地址空间。在用户态下运行的进程涉及到私有栈、数据区和代码区。当在内核态运行时，进程访问内核的数据区和代码区，但使用另外的私有栈。

因为内核是可重入的，因此几个内核控制路径(每个都与不同的进程相关)可以轮流执行。在这种情况下，每个内核控制路径都引用它自己的私有内核栈。

尽管看起来每个进程访问一个私有地址空间，但有时进程之间也共享部分地址空间。在一些情况下，这种共享由进程显式地提出；在另外一些情况下，由内核自动完成共享以节约内存。

如果同一个程序(比如说编辑程序)由几个用户同时使用，则这个程序只被装人内存一次，其指令由所有需要它的用户共享。当然，其数据不被共享，因为每个用户将有独立的数据。这种共享的地址空间由内核自动完成以节省内存。

进程间也能共享部分地址空间，以实现一种进程间通信，这就是由System V引入并且已经被Linux支持的“共享内存”技术。
最后，Linux支持mmap()系统调用，该系统调用允许存放在块设备上的文件或信息的一部分映射到进程的部分地址空间。内存映射为正常的读写传送数据方式提供了另一种选择。如果同一文件由几个进程共享，那么共享它的每个进程地址空间都包含有它的内存映射。

#### 同步和临界区

实现可重入内核需要利用同步机制：如果内核控制路径对某个内核数据结构进行操作时被挂起，那么，其他的内核控制路径就不应当再对该数据结构进行操作，除非它已被重新设置成一致性(consistent)状态。否则，两个控制路径的交互作用将破坏所存储的信息。

当某个计算结果取决于如何调度两个或多个进程时，相关代码就是不正确的。我们说存在一种竟争条件(race condition)。
一般来说，对全局变量的安全访问通过原子操作(atomic operation)来保证。临界区(critical region)是这样的一段代码，进入这段代码的进程必须完成，之后另一个进程才能进入。

这些问题不仅出现在内核控制路径之间，也出现在共享公共数据的进程之间。几种同步技术已经被采用。以下将集中讨论怎样同步内核控制路径。

##### 非抢占式内核

在寻找彻底、简单地解决同步问题的方案中，大多数传统的Unix内核都是非抢占式的：当进程在内核态执行时，它不能被任意挂起，也不能被另一个进程代替。因此，在单处理器系统上，中断或异常处理程序不能修改的所有内核数据结构，内核对它们的访问都是安全的。

当然，内核态的进程能自愿放弃CPU，但是在这种情况下，它必须确保所有的数据结构都处于一致性状态。此外，当这种进程恢复执行时，它必须重新检查以前访问过的数据结构的值，因为这些数据结构有可能被改变。

如果内核支持抢占，那么在应用同步机制时，确保进入临界区前禁止抢占，退出临界区时启用抢占。

非抢占能力在多处理器系统上是低效的，因为运行在不同CPU上的两个内核控制路径本可以并发地访问相同的数据结构。
禁止中断单处理器系统上的另一种同步机制是：在进入一个临界区之前禁止所有硬件中断，离开时再重新启用中断。这种机制尽管简单，但远不是最佳的。如果临界区比较大，那么在一个相对较长的时间内持续禁止中断就可能使所有的硬件活动处于冻结状态。

此外，由于在多处理器系统中禁止本地CPU上的中断是不够的，所以必须使用其他的同步技术。

##### 信号量

广泛使用的一种机制是信号量(semaphore)，它在单处理器系统和多处理器系统上都有效。信号量仅仅是与一个数据结构相关的计数器。所有内核线程在试图访问这个数据结构之前，都要检查这个信号量。可以把每个信号量看成一个对象，其组成如下：

- 一个整数变量
- 一个等待进程的链表
- 两个原子方法：down()和up()

down()方法对信号量的值减1，如果这个新值小于0，该方法就把正在运行的进程加入到这个信号量链表，然后阻塞该进程(即调用调度程序)。up()方法对信号量的值加1，如果这个新值大于或等于0，则激活这个信号量链表中的一个或多个进程。

每个要保护的数据结构都有它自己的信号量，其初始值为1。当内核控制路径希望访问这个数据结构时，它在相应的信号量上执行down()方法。如果信号量的当前值不是负数，则允许访问这个数据结构。否则，把执行内核控制路径的进程加入到这个信号量的链表并阻塞该进程。当另一个进程在那个信号量上执行up()方法时，允许信号量链表上的一个进程继续执行。

##### 自旋锁

在多处理器系统中，信号量并不总是解决同步问题的最佳方案。系统不允许在不同CPU上运行的内核控制路径同时访问某些内核数据结构，在这种情况下，如果修改数据结构所需的时间比较短，那么，信号量可能是很低效的。为了检查信号量，内核必须把进程插入到信号量链表中，然后挂起它。因为这两种操作比较费时，完成这些操作时，其他的内核控制路径可能已经释放了信号量。在这些情况下，多处理器操作系统使用了自旋锁(spin lock)。自旋锁与信号量非常相似，但没有进程链表。当一个进程发现锁被另一个进程锁着时，它就不停地“旋转”，执行一个紧凑的循环指令直到锁打开。

当然，自旋锁在单处理器环境下是无效的。当内核控制路径试图访问一个上锁的数据结构时，它开始无休止循环。因此，内核控制路径可能因为正在修改受保护的数据结构而没有机会继续执行，也没有机会释放这个自旋锁。最后的结果可能是系统挂起。

##### 避免死锁

与其他控制路径同步的进程或内核控制路径很容易进入死锁(deadlock)状态。

只要涉及到内核设计，当所用内核信号量的数量较多时，死锁就成为一个突出问题。在这种情况下，很难保证内核控制路径在各种可能方式下的交错执行不出现死锁状态。有几种操作系统(包括Linux)通过按规定的顺序请求信号量来避免死锁。

#### 信号和进程间通信

Unix信号(signal)提供了把系统事件报告给进程的一种机制。每种事件都有自己的信号编号，通常用一个符号常量来表示，例如SIGTERM。有两种系统事件：

- 异步通告

  例如，当用户在终端按下中断键(通常为CTRL-C)时，即向前台进程发出中断信号SIGINT。

- 同步错误或异常

  例如，当进程访问内存非法地址时，内核向这个进程发送一个SIGSEGV信号。

POSIX标准定义了大约20种不同的信号，其中，有两种是用户自定义的，可以当作用户态下进程通信和同步的原语机制。一般来说，进程可以以两种方式对接收到的信号做出反应：

- 忽略该信号。

- 异步地执行一个指定的过程(信号处理程序)。

如果进程不指定选择何种方式，内核就根据信号的编号执行一个默认操作。五种可能的默认操作是：

- 终止进程。

- 将执行上下文和进程地址空间的内容写入一个文件(核心转储，core dump)，并终止进程。
- 忽略信号。
- 挂起进程。
- 如果进程曾被暂停，则恢复它的执行。

因为POSIX语义允许进程暂时阻塞信号，因此内核信号的处理相当精细。此外，SIGKILL和SIGSTOP信号不能直接由进程处理，也不能由进程忽略。

AT&T的Unix System V引入了在用户态下其他种类的进程间通信机制，很多Unix内核也采用了这些机制：信号量、消息队列及共享内存。它们被统称为System V lPC。

内核把它们作为IPC资源来实现：进程要获得一个资源，可以调用shmget()，semget()或msgget()系统调用。与文件一样，IPC资源是持久不变的，进程创建者、进程拥有者或超级用户进程必须显式地释放这些资源。

这里的信号量与本章“同步和临界区”一节中所描述的信号量是相似的，只是它们用在用户态下的进程中。消息队列允许进程利用msgsnd()及msgget()系统调用交换消息，msgsnd()表示把消息插入到指定的队列中，msgget()表示从队列中提取消息。

POSIX标准(IEEE Std 1003.1-2001)定义了一种基于消息队列的IPC机制，这就是所谓的POSIX消息队列。它们和System V IPC消息队列是相似的，但是，它们对应用程序提供一个更简单的基于文件的接口。

共享内存为进程之间交换和共享数据提供了最快的方式。通过调用shmget()系统调用来创建一个新的共享内存，其大小按需设置。在获得IPC资源标识符后，进程调用shmat()系统调用，其返回值是进程的地址空间中新区域的起始地址。当进程希望把共享内存从其地址空间分离出去时，就调用shmdt()系统调用。共享内存的实现依赖于内核对进程地址空间的实现。

#### 进程管理

Unix在进程和它正在执行的程序之间做出一个清晰的划分。fork()和_exit()系统调用分别用来创建一个新进程和终止一个进程，而调用exec()类系统调用则是装入一个新程序。当这样一个系统调用执行以后，进程就在所装入程序的全新地址空间恢复运行。

调用fork()的进程是父进程，而新进程是它的子进程。父子进程能互相找到对方，因为进程描述符包含有两个指针，一个直接指向它的父进程，另一个直接指向它的子进程。

实现fork()一种简单的方式就是将父进程的数据与代码都复制，并把这个拷贝赋予子进程。这会相当费时。当前依赖硬件分页单元的内核采用写时复制(Copy-On-Write)技术，即把页的复制延迟到最后一刻(也就是说，直到父或子进程需要时才写进页)。

_exit()系统调用终止一个进程。内核对这个系统调用的处理是通过释放进程所拥有的资源并向父进程发送SIGCHILD信号(默认操作为忽略)来实现的。

##### 僵死进程(zombie process)

父进程如何查询其子进程是否终止了呢?wait4()系统调用允许进程等待，直到其中的一个子进程结束。它返回已终止子进程的进程标识符(Process ID，  PID)。

内核在执行这个系统调用时，检查子进程是否已经终止。引入僵死进程的特殊状态是为了表示终止的进程：父进程执行完wait4()系统调用之前，进程就一直停留在那种状态。系统调用处理程序从进程描述符字段中获取有关资源使用的一些数据；一旦得到数据，就可以释放进程描述符。当进程执行wait4()系统调用时如果没有子进程结束，内核就通常把该进程设置成等待状态，一直到子进程结束。

很多内核也实现了waitpid()系统调用，它允许进程等待一个特殊的子进程。其他wait4()系统调用的变体也是相当通用的。
在父进程发出wait4()调用之前，让内核保存子进程的有关信息是一个良好的习惯，但是，假设父进程终止而没有发出wait4()调用呢?这些信息占用了一些内存中非常有用的位置，而这些位置本来可以用来为活动着的进程提供服务。例如，很多shell允许用户在后台启动一个命令然后退出。正在运行这个shell命令的进程终止，但它的子进程继续运行。

解决的办法是使用一个名为init的特殊系统进程，它在系统初始化的时候被创建。当一个进程终止时，内核改变其所有现有子进程的进程描述符指针，使这些子进程成为init的孩子。init监控所有子进程的执行，并且按常规发布wait4()系统调用，其副作用就是除掉所有僵死的进程。

##### 进程组和登录会话

现代Unix操作系统引入了进程组(process group)的概念，以表示一种“作业(job)"的抽象。例如，为了执行命令行：

```shell
ls | sort | mort
```


Shell支持进程组，例如bash，为三个相应的进程ls，sort及more创建了一个新的组。shell以这种方式作用于这三个进程，就好像它们是一个单独的实体(更准确地说是作业)。每个进程描述符包括一个包含进程组ID的字段。每一进程组可以有一个领头进程(即其PID与这个进程组的ID相同的进程)。新创建的进程最初被插入到其父进程的进程组中。

现代Unix内核也引入了登录会话(login session)。非正式地说，一个登录会话包含在指定终端已经开始工作会话的那个进程的所有后代进程——通常情况下，登录会话就是shell进程为用户创建的第一条命令。进程组中的所有进程必须在同一登录会话中。一个登录会话可以让几个进程组同时处于活动状态，其中，只有一个进程组一直处于前台，这意味着该进程组可以访问终端，而其他活动着的进程组在后台。当一个后台进程试图访问终端时，它将收到SIGTTIN或SIGTTOUT信号。在很多shell命令中，用内部命令bg和fg把一个进程组放在后台或者前台。

#### 内存管理

内存管理是迄今为止Unix内核中最复杂的活动。

##### 虚拟内存

所有新近的Unix系统都提供了一种有用的抽象，叫虚拟内存(virtual memory)。虚拟内存作为一种逻辑层，处于应用程序的内存请求与硬件内存管理单元(Memory Management Unit，  MMU)之间。虚拟内存有很多用途和优点：

- 若干个进程可以并发地执行。
- 应用程序所需内存大于可用物理内存时也可以运行。
- 程序只有部分代码装入内存时进程可以执行它。
- 允许每个进程访问可用物理内存的子集。
- 进程可以共享库函数或程序的一个单独内存映像。
- 程序是可重定位的，也就是说，可以把程序放在物理内存的任何地方。
- 程序员可以编写与机器无关的代码，因为他们不必关心有关物理内存的组织结构。

虚拟内存子系统的主要成分是虚拟地址空间(virtual address space)的概念。进程所用的一组内存地址不同于物理内存地址。当进程使用一个虚拟地址时，内核和MMU协同定位其在内存中的实际物理位置。

现在的CPU包含了能自动把虚拟地址转换成物理地址的硬件电路。为了达到这个目标，把可用RAM划分成长度为4KB或8KB的页框(page frame)，并且引入一组页表来指定虚拟地址与物理地址之间的对应关系。这些电路使内存分配变得简单，因为一块连续的虚拟地址请求可以通过分配一组非连续的物理地址页框而得到满足。

##### 随机访问存储器(RAM)的使用

所有的Unix操作系统都将RAM毫无疑义地划分为两部分，其中若干兆字节专门用于存放内核映像(也就是内核代码和内核静态数据结构)。RAM的其余部分通常由虚拟内存系统来处理，并且用在以下三种可能的方面：

- 满足内核对缓冲区、描述符及其他动态内核数据结构的请求。
- 满足进程对一般内存区的请求及对文件内存映射的请求。
- 借助于高速缓存从磁盘及其他缓冲设备获得较好的性能。

每种请求类型都是重要的。但从另一方面来说，因为可用RAM是有限的，所以必须在请求类型之间做出平衡，尤其是当可用内存没有剩下多少时。此外，当可用内存达到临界阈值时，可以调用页框回收(page-frame-reclaiming)算法释放其他内存。

虚拟内存系统必须解决的一个主要问题是内存碎片。理想情况下，只有当空闲页框数太少时，内存请求才失败。然而，通常要求内核使用物理上连续的内存区域，因此，即使有足够的可用内存，但它不能作为一个连续的大块使用时，内存的请求也会失败。

##### 内核内存分配器

内核内存分配器(Kernel Memory Allocator， KMA)是一个子系统，它试图满足系统中所有部分对内存的请求。其中一些请求来自内核其他子系统，它们需要一些内核使用的内存，还有一些请求来自于用户程序的系统调用，用来增加用户进程的地址空间。一个好的KMA应该具有下列特点：

- 必须快。实际上，这是最重要的属性，因为它由所有的内核子系统(包括中断处理程序)调用。
- 必须把内存的浪费减到最少。
- 必须努力减轻内存的碎片(fragmentation)问题。
- 必须能与其他内存管理子系统合作，以便借用和释放页框。

基于各种不同的算法技术，已经提出了几种KMA，包括：

- 资源图分配算法(allocator)

- 2的幕次方空闲链表
- McKusick-Karels分配算法
- 伙伴(Buddy)系统
- Mach的区域(Zone)分配算法
- Dynix分配算法
- Solaris的Slab分配算法

Linux的KMA在伙伴系统之上采用了Slab分配算法。

##### 进程虚拟地址空间处理

进程的虚拟地址空间包括了进程可以引用的所有虚拟内存地址。内核通常用一组内存区描述符描述进程虚拟地址空间。例如，当进程通过exec()类系统调用开始某个程序的执行时，内核分配给进程的虚拟地址空间由以下内存区组成：

- 程序的可执行代码
- 程序的初始化数据
- 程序的未初始化数据
- 初始程序栈(即用户态栈)
- 所需共享库的可执行代码和数据
- 堆(由程序动态请求的内存)

所有现代Unix操作系统都采用了所谓请求调页(demand paging)的内存分配策略。有了请求调页，进程可以在它的页还没有在内存时就开始执行。当进程访问一个不存在的页时，MMU产生一个异常；异常处理程序找到受影响的内存区，分配一个空闲的页，并用适当的数据把它初始化。同理，当进程通过调用malloc()或brk()(由malloc()在内部调用)系统调用动态地请求内存时，内核仅仅修改进程的堆内存区的大小。只有试图引用进程的虚拟内存地址而产生异常时，才给进程分配页框。虚拟地址空间也采用其他更有效的策略，如前面提到的写时复制策略。例如，当一个新进程被创建时，内核仅仅把父进程的页框赋给子进程的地址空间，但是把这些页框标记为只读。一旦父或子进程试图修改页中的内容时，一个异常就会产生。异常处理程序把新页框赋给受影响的进程，并用原来页中的内容初始化新页框。

##### 高速缓存

物理内存的一大优势就是用作磁盘和其他块设备的高速缓存。这是因为硬盘非常慢：磁盘的访问需要数毫秒，与RAM的访问时间相比，这太长了。因此，磁盘通常是影响系统性能的瓶颈。通常，在最早的Unix系统中就已经实现的一个策略是：尽可能地推迟写磁盘的时间，因此，从磁盘读入内存的数据即使任何进程都不再使用它们，它们也继续留在RAM中。
这一策略的前题是有好机会摆在面前：新进程请求从磁盘读或写的数据，就是被撤消进程曾拥有的数据。当一个进程请求访问磁盘时，内核首先检查进程请求的数据是否在缓存中，如果在(把这种情况叫做缓存命中)，内核就可以为进程请求提供服务而不用访问磁盘。

sync()系统调用把所有“脏”的缓冲区(即缓冲区的内容与对应磁盘块的内容不一样)写入磁盘来强制磁盘同步。为了避免数据丢失，所有的操作系统都会注意周期性地把脏缓冲区写回磁盘。

#### 设备驱动程序

内核通过设备驱动程序(device driver)与I/O设备交互。设备驱动程序包含在内核中，由控制一个或多个设备的数据结构和函数组成，这些设备包括硬盘、键盘、鼠标、监视器、网络接口及连接到SCSI总线上的设备。通过特定的接口，每个驱动程序与内核中的其余部分(甚至与其他驱动程序)相互作用这种方式具有以下优点：

- 可以把特定设备的代码封装在特定的模块中。
- 厂商可以在不了解内核源代码而只知道接口规范的情况下，就能增加新的设备。
- 内核以统一的方式对待所有的设备，并且通过相同的接口访问这些设备。
- 可以把设备驱动程序写成模块，并动态地把它们装进内核而不需要重新启动系统。
- 不再需要时，也可以动态地卸下模块，以减少存储在RAM中的内核映像的大小。

下图说明了设备驱动程序与内核其他部分及进程之间的接口。

![设备驱动程序接口.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3.jpg?raw=true)

一些用户程序(P)希望操作硬件设备。这些程序就利用常用的、与文件相关的系统调用及在/dev目录下能找到的设备文件向内核发出请求。实际上，设备文件是设备驱动程序接口中用户可见的部分。每个设备文件都有专门的设备驱动程序，它们由内核调用以执行对硬件设备的请求操作。

这里值得一提的是，在Unix刚出现的时候，图形终端是罕见而且昂贵的，因此Unix内核只直接处理字符终端。当图形终端变得非常普遍时，一些如XWindow系统那样的特别的应用就出现了，它们以标准进程的身份运行，并且能直接访问图形界面的I/O端口和RAM的视频区域。一些新近的Unix内核，例如Linux 2.6，对图形卡的帧缓冲提供了一种抽象，从而允许应用软件无需了解图形界面的I/O端口的任何知识就能对其进行访问。

## 第2章 内存寻址

### 内存地址

逻辑地址（logical address）：机器语言指令中用来指定一个操作数或一条指令的地址。每一个逻辑地址都由一个段（segment）和偏移量（offset或displacement）组成，偏移量指明了从段开始的地方到实际地址之间的距离。这种寻址方式在80x86著名的分段结构中表现的尤为具体，它促使MS-DOS或Windows程序员把程序分成若干段。

线性地址（linear address）（或 虚拟地址 virtual address）：一个32位无符号整数，可以用来表示高达4GB（0x0000 0000 —— 0xffff ffff）的地址，也就是高达 4 * 1024 * 1024 * 1024个内存单元（字节）。

物理地址（physical address）：芯片级内存单元寻址。它们与从微处理器的地址引脚发送到内存总线上的电信号相对应。物理地址由32位或36位（开启PAE）无符号整数表示。

内存管理单元（MMU）通过分段单元（segmentation unit）把逻辑地址转换成线性地址；然后，通过分页单元（paging unit）把线性地址转换成物理地址。分段单元和分页单元都是一种硬件电路。

在多处理器系统中，所有CPU都共享同一内存。这意味着RAM芯片可以由独立的CPU并发地访问。因为在RAM芯片上的读或写操作必须串行地执行，因此一种所谓内存仲裁器(memory arbiter)的硬件电路插在总线和每个RAM芯片之间。其作用是如果某个RAM芯片空闲，就准予一个CPU访问，如果该芯片忙于为另一个处理器提出的请求服务，就延迟这个CPU的访问。即使在单处理器上也使用内存仲裁器，因为单处理器系统中包含一个叫做 DMA控制器 的特殊处理器，而DMA控制器与CPU并发操作（RAM）。在多处理器系统的情况下，因为仲裁器有多个输入端口，所以其结构更加复杂。例如，双Pentium在每个芯片的入口维持一个两端口仲裁器，并在试图使用公用总线前请求两个CPU交换同步信息。从编程观点看，因为仲裁器由硬件电路管理，因此它是隐藏的。

### 硬件中的分段

80286之前仅有实模式：数据总线16位，地址总线20位，寄存器16位。

从80286模型开始，Intel微处理器以两种不同的方式执行地址转换：实模式（real mode）和保护模式（protected mode）（数据总线、地址总线32位，寄存器32位）。实模式存在的主要原因是要维持处理器与早期模型兼容，并让操作系统自举。

#### 段选择符和段寄存器

一个逻辑地址由两部分组成：段标识符和指定段内相对地址的偏移量。段标识符是一个16位长的字段，称为段选择符（Segment Selector），而偏移量是一个32位长的字段。

![段选择符.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E6%AE%B5%E9%80%89%E6%8B%A9%E7%AC%A6.jpg?raw=true)

为了快速方便地找到段选择符，处理器提供段寄存器，段寄存器的唯一目的是 存放段选择符。这些段寄存器称为cs，  ss， ds， es，  fs和gs。尽管只有6个段寄存器，但程序可以把同一个段寄存器用于不同的目的，方法是先将其值保存在内存中，用完后再恢复。
6个段寄存器中3个有专门的用途：
cs	代码段寄存器，指向包含程序指令的段。
ss	栈段寄存器，指向包含当前程序栈的段。
ds	数据段寄存器，指向包含静态数据或者全局数据段（初始化数据）。
其他3个段寄存器作一般用途，可以指向任意的数据段。
cs寄存器还有一个很重要的功能：它含有一个 两位的字段，用以指明CPU的 当前特权级(Current Privilege Level， CPL)。值为0代表最高优先级，而值为3代表最低优先级。Linux只用0级和3级，分别称之为内核态和用户态。

#### 段描述符

每个段由一个 8字节 的段描述符（Segment Descriptor）表示，它描述了段的特征。段描述符放在全局描述符表（Global Descriptor Table， GDT）或局部描述符表（Local Descriptor Table， LDT）中。

通常只定义一个GDT，而每个进程除了存放在GDT中段之外如果还需要创建附加的段，就可以有自己的LDT。GDT在主存中的地址和大小存放在gdtr控制寄存器中，当前正被使用的LDT地址和大小放在ldtr控制寄存器中。

![段描述符通用格式.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6%E9%80%9A%E7%94%A8%E6%A0%BC%E5%BC%8F.jpg?raw=true)

![几种段描述符格式.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E5%87%A0%E7%A7%8D%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%A0%BC%E5%BC%8F.jpg?raw=true)

| 字段名   | 描述                                       |
| ----- | ---------------------------------------- |
| Base  | 包含段的首字节的线性地址 （32 bit）                    |
| G     | 粒度标志；置0，则段大小以字节为单位，否则以4096字节的倍数计         |
| Limit | 存放段中最后一个内存单元的偏移量，从而决定段的长度（20 bit）。如果G被置为0，则一个段的大小在1个字节到1MB之间变化；否则，将在4KB到4GB之间变化 |
| S     | 系统标志；置0，系统段，存储诸如LDT这种关键的数据结构，否则它是一个普通的代码段或数据段 |
| Type  | 描述了段的类型特征和它的存取权限                         |
| DPL   | 描述符特权级（Descriptor Privilege Level）字段；用于限制对这个段的存取。表示访问这个段要求的CPU最小的优先级 |
| P     | Segment-Present标志；为0表示段当前不在主存中。Linux总是把这个标志（第47位）设为1，因为它从来不把整个段交换到磁盘上去 |
| D或B   | 取决于是代码段还是数据段                             |
| AVL   | 操作系统使用，但被Linux忽略                         |

代码段和数据段描述符类型如下表：

![代码段和数据段描述符类型.png](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E4%BB%A3%E7%A0%81%E6%AE%B5%E5%92%8C%E6%95%B0%E6%8D%AE%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%B1%BB%E5%9E%8B.png?raw=true)

代码段描述符：代表一个代码段，它可以放在GDT或LDT中。该描述符置S标志为1(非系统段)。

数据段描述符：代表一个数据段，它可以放在GDT或LDT中。该描述符置S标志为1。栈段是通过一般的数据段实现的。

任务状态段描述符（TSSD）：代表一个任务状态段(Task State Segment， TSS )，也就是说这个段用于保存处理器寄存器的内容。它只能出现在GDT中。根据相应的进程是否正在CPU上运行，其Type字段的值分别为11或9。这个描述符的S标志置为0。

局部描述符表描述符（LDTD）：代表一个包含LDT的段，它只出现在GDT中。相应的Type字段的值为2，S标志置为0。

段选择符字段：

| 字段名   | 描述                                       |
| ----- | ---------------------------------------- |
| index | 指定了放在GDT或LDT中的相应段描述符的入口                  |
| TI    | TI（Table Indicator）标志，指明段描述符是在GDT中（TI=0）或在LDT中（TI=1） |
| RPL   | 请求者特权级，当相应的段选择符装入到cs寄存器中时指示出CPU当前的特权级，它还可以用于在访问数据段时有选择地削弱处理器的特权级 |

#### 快速访问段描述符

重申：逻辑地址由16位段选择符和32位偏移量组成，段寄存器仅仅存放段选择符。

为了加速逻辑地址到线性地址的转换，80x86处理器提供一种附加的非编程（不能被编程者设置）的寄存器，供6个可编程的段寄存器使用。每一个非编程的寄存器含有8个字节的段描述符，由相应的段寄存器中的段选择符来指定。每当一个段选择符被装入段寄存器时，相应的段描述符就由内存装入到对应的非编程CPU寄存器。之后，针对那个段的逻辑地址转换就可以不访问主存中的GDT或LDT，处理器只需直接引用存放段描述符的CPU寄存器即可。仅当段寄存器的内容改变时，才有必要访问GDT或LDT。

![段选择符和段描述符.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E6%AE%B5%E9%80%89%E6%8B%A9%E7%AC%A6%E5%92%8C%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6.jpg?raw=true)

由于一个段描述符是8字节，因此它在GDT或LDT内的相对地址是由段选择符的最高13位（index）的值乘以8得到的。例如：如果GDT在0x0002 0000（这个值保存在gdtr寄存器中），且由段选择符所指定的索引号为2，那么相应的段描述符地址是0x0002 0000 + (2 * 8)，或0x0002 0010。

GDT的第一项总是设为0。这就确保空段选择符的逻辑地址会被认为是无效的，因此引起一个处理器异常。能够保存在GDT中的段描述符的最大数目是8191，即2^13^ - 1。

#### 分段单元

下图显示一个逻辑地址转换的详细过程，分段单元（segmentation unit）执行以下操作：

- 先检查段选择符的TI字段，以决定段描述符保存在哪一个描述符表中。GDT中，分段单元从gdtr寄存器得到GDT的线性基地址；LDT中，分段单元从ldtr寄存器得到LDT的线性基地址。
- 从段选择符的index字段计算段描述符的地址，index字段的值乘以8（一个段描述符的大小），这个结果与gdtr或ldtr寄存器中的内容相加。
- 把逻辑地址的偏移量与段描述符Base字段的值相加就得到了线性地址。

![逻辑地址转换.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E9%80%BB%E8%BE%91%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2.jpg?raw=true)

有了与段寄存器相关的不可编程寄存器，只有当段寄存器的内容被改变时才需要执行前两个操作。

### Linux中的分段

分段使程序划分成逻辑上相关的实体，例如子程序或者全局与局部数据区。然而，Linux以非常有限的方式使用分段。实际上，分段和分页在某种程度上有点多余因为它们都可以划分进程的物理地址空间：分段可以给每个进程分配不同的线性地址空间，而分页可以把同一线性地址空间映射到不同的物理地址空间（页表映射不同）。与分段相比，Linux更喜欢用分页的方式：

- 当所有的进程使用相同的段寄存器值时，内存管理变得更简单，也就是它们能共享同样的一组线性地址。
- Linux设计目标之一是可以把它移植到绝大多数流行的处理器平台上。然而，RISC体系结构对分段的支持有限。

2.6版的Linux只有在x86结构下才需要分段。

运行在用户态的所有Linux进程都使用一对相同的段来对指令和数据寻址。这两个段就是所谓的用户代码段和用户数据段。类似地，运行在内核态的所有Linux进程都使用一对相同的段对指令和数据寻址：它们分别叫做内核代码段和内核数据段。

下表显示了这4个重要段的段描述符字段的值：

| 段     | Base        | G    | Limit   | S    | Type | DPL  | D/B  | p    |
| ----- | ----------- | ---- | ------- | ---- | ---- | ---- | ---- | ---- |
| 用户代码段 | 0x0000 0000 | 1    | 0xfffff | 1    | 10   | 3    | 1    | 1    |
| 用户数据段 | 0x0000 0000 | 1    | 0xfffff | 1    | 2    | 3    | 1    | 1    |
| 内核代码段 | 0x0000 0000 | 1    | 0xfffff | 1    | 10   | 0    | 1    | 1    |
| 内核数据段 | 0x0000 0000 | 1    | 0xfffff | 1    | 2    | 0    | 1    | 1    |

G为1，粒度为4KB，Limit为 0xfffff，则空间为 4GB

相应的段选择符由宏定义。

```c
  __USER_CS、__USER_DS、__KERNEL_CS、__KERNEL_DS
```

为了对内核代码段寻址，内核只需把\_\_KERNEL_CS宏产生的值装进cs段寄存器即可。

注意，与段相关的线性地址从0开始，达到2^23^ - 1的寻址限长。这就意味着在用户态或内核态下的所有进程可以使用相同的逻辑地址。

所有段都从0x0000 0000 开始，那么，在Linux下逻辑地址与线性地址是一致的，即逻辑地址的偏移量字段的值与相应的线性地址的值总是一致的。

如前所述，CPU的当前特权级(CPL)反映了进程是在用户态还是内核态，并由存放在cs寄存器中的段选择符的RPL字段指定。只要当前特权级被改变，一些段寄存器必须相应地更新。例如，当CPL=3时(用户态)，ds寄存器必须含有用户数据段的段选择符，而当CPL=0时，ds寄存器必须含有内核数据段的段选择符。

类似的情况也出现在ss寄存器中。当CPL为3时，它必须指向一个用户数据段中的用户栈，而当CPL为0时，它必须指向内核数据段中的一个内核栈。当从用户态切换到内核态时，Linux总是确保ss寄存器装有内核数据段的段选择符。

当对指向指令或者数据结构的指针进行保存时，内核根本不需要为其设置逻辑地址的段选择符，因为cs寄存器就含有当前的段选择符。例如，当内核调用一个函数时，它执行一条call汇编语言指令，该指令仅指定其逻辑地址的偏移量部分，而段选择符不用设置，它已经隐含在cs寄存器中了。因为“在内核态执行”的段只有一种，叫做代码段，由宏\_\_KERNEL_CS定义，所以只要当CPU切换到内核态时将\_\_KERNEL_CS装载进cs就足够了。同样的道理也适用于指向内核数据结构的指针(隐含地使用ds寄存器)以及指向用户数据结构的指针(内核显式地使用es寄存器)。

#### Linux GDT

在单处理器系统中只有一个GDT，而在多处理器系统中每个CPU对应一个GDT。所有的GDT都存放在cpu_gdt_table数组中，而所有GDT的地址和它们的大小(当初始化gdtr寄存器时使用)被存放在cpu_gdt_descr数组中。如果你到源代码索引中查看，可以看到这些符号都在文件arch/i386/kernel/head.S中被定义。

下图是GDT的布局示意图。每个GDT包含18个段描述符和14个空的，未使用的，或保留的项。插入未使用的项的目的是为了使经常一起访问的描述符能够处于同一个32字节的硬件高速缓存行中。

![全局描述符表.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E5%85%A8%E5%B1%80%E6%8F%8F%E8%BF%B0%E7%AC%A6%E8%A1%A8.jpg?raw=true)

每一个GDT中包含的18个段描述符指同下列的段：

- 用户态和内核态下的代码段和数据段，共4个。
- 任务状态段（TSS），每个处理器有1个。每个TSS相应的线性地址空间都是内核数据段相应线性地址空间的一个小子集。所有的任务状态段都顺序地存放在init_tss数组中，值得特别说明的是，第n个CPU的TSS描述符的Base字段指向init_tss数组的第n个元素。G(粒度)标志被清0，而Limit字段置为0xeb，  因为TSS段是236字节长。Type字段置为9或11(可用的32位TSS)，且DPL置    为0，因为不允许用户态下的进程访问TSS段。
- 1个包括缺省局部描述符表的段，这个段通常被所有进程共享。
- 3个局部线程存储（Thread-Local Storage， TLS）段：这种机制允许多线程应用程序使用最多3个局部于线程的数据段。系统使用set_thread_area()和get_thread_area()分别为正在执行的进程创建和撤销一个TLS段。
- 与高级电源管理（APM）相关的3个段：由于BIOS代码使用段，所以当Linux APM驱动程序调用BIOS函数来获取或者设置APM设备的状态时，就可以使用自定义的代码段和数据段。
- 与支持即插即用（PnP）功能的BIOS服务程序相关的5个段。
- 被内核用来处理“双重错误”异常（处理一个异常时可能会引发另一个异常）的特殊TSS段。

系统中每个处理器都有一个GDT副本。除少数几种情况外，所有GDT的副本都存放相同的表项：

- 每个处理器都有它自己的TSS段。
- GDT中只有少数项可能依赖于CPU正在执行的进程（LDT和TLS段描述符）。
- 在某些情况下，处理器可能临时修改GDT副本里的某个项，例如，当调用APM的BIOS例程时就会发生这种情况。

#### Linux LDT

大多数用户态下的Linux程序不使用局部描述符表，这样内核就定义了一个缺省的LDT供大多数进程共享。缺省的局部描述符表存放在default_ldt数组中。它包含5个项，但内核仅仅有效地使用了其中的两个项：用于iBCS执行文件的调用门和Solaris/x86可执行文件的调用门。调用门是80x86微处理器提供的一种机制，用于在调用预定义函数时改变CPU的特权级（参考Intel文档以获取更多详情）。

在某些情况下，进程仍然需要创建自己的LDT，像Wine那样的程序，它们执行面向段的微软Windows应用程序。modify_ldt()系统调用允许进程创建自己的LDT。任何被modify_ldt()创建的自定义局部描述符表仍然需要它自己的段。当处理器开始执行拥有自定义局部描述符表的进程时，该CPU的GDT副本中的LDT表项相应地就被修改了。

用户态下的程序同样也利用modify_ldt()来分配新的段，但内核却从不使用这些段，它也不需要了解相应的段描述符，因为这些段描述符被包含在进程自定义的局部描述符表中了。

### 硬件中的分页

分页单元(paging unit)把线性地址转换成物理地址。其中的一个关键任务是把所请求的访问类型与线性地址的访问权限相比较，如果这次内存访问是无效的，就产生一个缺页异常。

为了效率起见，线性地址被分成以固定长度为单位的组，称为页（page）。页内部连续的线性地址被映射到连续的物理地址中。这样，内核可以指定一个页的物理地址和其存取权限，而不用指定页所包含的全部线性地址的存取权限。我们遵循通常习惯，使用术语“页”既指一组线性地址，又指包含在这组地址中的数据。

分页单元把所有的RAM分成固定长度的叶框（page frame）（也叫做物理页）。每一个叶框包含一个页，也就是说叶框的长度与一个页的长度一致。页框是主存的一部分，因此也是一个存储区域。区分一页和一个页框是很重要的，前者只是一个数据块，可以存放在任何页框或磁盘中。

把线性地址映射到物理地址的数据结构称为页表(page table )。页表存放在主存中，并在启用分页单元之前必须由内核对页表进行适当的初始化。

从80386开始，所有的80x86处理器都支持分页，它通过设置cr0寄存器的PG标志启用。当PG=0时，线性地址就被解释成物理地址。<需要了解控制寄存器(cr0~cr3)的结构及作用>

#### 常规分页

从80386起，Intel处理器的分页单元处理4KB的页。32位的线性地址被分成3个域：

- Directory（目录）：最高10位
- Table（页表）：中间10位
- Offset（偏移量）：最低12位

线性地址的转换分两步完成，每一步都基于一种转换表，第一种转换表称为页目录表(page directory)，第二种转换表称为页表(page table )(小写的‘page table’表示保存线性地址和物理地址之间映射的页，而利用‘Page Table’表示在上层页表中的页)。

页目录 及 页表都分别存放在1个页中（4KB），其中每个表项也都是4个字节。

使用这种二级模式的目的在于减少每个进程页表所需RAM的数量。如果使用简单的一级页表，那将需要高达2^20^个表项(也就是，在每项4个字节时，需要4MB RAM)来表示每个进程的页表(如果进程使用全部4GB线性地址空间)，即使一个进程并不使用那个范围内的所有地址。二级模式通过只为进程实际使用的那些虚拟内存区请求页表来减少内存容量。

每个活动进程必须有一个分配给它的页目录。不过，没有必要马上为进程的所有页表都分配RAM。只有在进程实际需要一个页表时才给该页表分配RAM会更为有效率。

正在使用的页目录的物理地址存放在控制寄存器cr3中。线性地址内的Directory字段决定页目录中的目录项，而目录项指向适当的页表。地址的Table字段依次又决定页表中的表项，而表项含所有页所在页框的物理地址。Offset字段决定页框内的相对位置。由于它是12位长，故每一页含有4096字节的数据。

![x86处理器的分页.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/x86%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%E5%88%86%E9%A1%B5.jpg?raw=true)

Directory字段和Table字段都是10位长，因此页目录和页表都可以多达1024项。那么一个页目录可以寻址到高达1024 * 1024 * 4096 = 2^32^个存储单元，这和你对32位地址所期望的一样。

页目录项和页表项有相同的结构，每项都包含下面的字段：

| 字段                | 描述                                       |
| ----------------- | ---------------------------------------- |
| Present标志         | 置为1，所指的页（或页表）就在主存中；为0，则这一页不在主存，此时这个表项剩余的位可由操作系统用于自己的目的。如果只需一个地址转换所需的页表项或页目录项中Present标志被清0，那么分页单元就把该线性地址存放在控制寄存器cr2中，并产生14号异常：缺页异常。 |
| 包含页框物理地址最高20位的字段  | 由于每一个页框有4KB的容量，它的物理地址必须是4096的倍数，因此物理地址的最低12位总是为0。若这个字段指向一个页目录，相应的页框就含有一个页表，若指向一个页表，相应的页框就含有一页数据。 |
| Accessed标志        | 每当分页单元对相应页框进行寻址时就设置这个标志。当选中的页被交换出去时，这一标志由操作系统使用。分页单元从来不重置这个标志，而是必须由操作系统去做。 |
| Dirty标志           | 只应用于页表项中。每当对一个页框进行写操作时就设置这个标志。与Accessed标志一样，“当选中…………系统去做”。 |
| Read/Write标志      | 含有页或页表的存取权限。                             |
| User/Supervisor标志 | 含有访问页或页表所需的特权级。                          |
| PCD和PWT标志         | 控制硬件高速缓存处理页或页表的方式。                       |
| Page Size标志       | 只应用于页目录项。置为1，则页目录指的是2MB或4MB的页框。          |
| Global标志          | 只应用于页表项。这个标志是在Pentium Pro中引入的，用来防止常用页从TLB（俗称“快表”）高速缓存中刷新出去。只有在cr4寄存器的页全局启用（Page Global Enable， PGE）标志置位时这个标志才起作用。 |

#### 扩展分页

从Pentium模型开始，80x86微处理器引入了扩展分页（extended paging），它允许页框大小为4MB而不是4KB。扩展分页用于把大段连续的线性地址转换成相应的物理地址，在这些情况下，内核可以不用中间页表进行地址转换，从而节省内存并保留TLB项。

通过设置页目录项的Page Size标志启用扩展分页功能。分页单元吧32位线性地址分成两个字段：

- Directory：最高10位
- Offset：其余22位

扩展分页和正常分页的目录项基本相同，除了：

- Page Size标志必须被设置。
- 32位物理地址字段只有最高10位是有意义的。这是因为每一个物理地址都是在以4MB为边界的地方开始的，故这个地址的最低22位为0。

通过设置cr4处理器寄存器的PSE标志能使扩展分页与常规分页共存。

#### 硬件保护方案

分页单元和分段单元的保护方案不同。尽管x86处理器允许一个段使用4种可能的特权级别，但与页和页表相关的特权级只有两个，因为特权由User/Supervisor标志所控制。若这个标志为0，只有当CPL小于3(这意味着对于Linux而言，处理器处于内核态)时才能对页寻址。若该标志为1，则总能对页寻址。

此外，与段的3种存取权限（读、写、执行）不同的是，页的存取权限只有两种（读、写）。如果页目录项或页表项的Read/Write标志等于0，说明相应的页表或页是只读的，否则是可读写的。

#### 常规分页举例

假定内核已给一个正在允许的进程分配的线性地址空间范围是0x2000 0000 到 0x2003 ffff。这个空间正好由64页组成。从分配给进程的线性地址的最高10位（Directory字段）开始。这两个地址都以2开头后面跟着0，因此高10位有相同的值，即0x080或十进制128。因此，这两个地址的Directory字段都指向进程页目录的第129项。相应的目录项中必须包含分配给该进程的页表的物理地址。如果没有给这个进程分配其他的线性地址，则页目录的其余1023项都填为0.

中间10位的值（Table）范围从0到0x03f，十进制的0到63，因而只有页表的前64个表项是有意义的，其余960个表项都填0。

假设进程需要读取线性地址 0x2002 1406中的字节，则处理方法：

1. Directory字段的0x80用于选择页目录的第0x80目录项，此目录项指向和该进程的页相关的页表。
2. Table字段0x21用于选择页表的第0x21表项，此表项指向包含所需页的页框。
3. 最后，Offset字段0x406用于在目标页框中读偏移量为0x406中的字节。

如果页表第0x21表项的Present标志为0，则此页就不在主存中，这种情况下，分页单元在线性地址转换的同时产生一个缺页异常。无论何时，当进程试图访问限定在0x2000 0000 到 0x2003 ffff范围之外的线性地址时，都将产生一个缺页异常，因为这些页表项都填充了0，尤其是它们的Present标志都被清0。

#### 物理地址扩展（PAE）分页机制

处理器所支持的RAM容量受连接到地址总线上的地址管脚数限制。早期Intel处理器从80386到Pentium使用32位物理地址。从理论上讲，这样的系统上可以安装高达4GB的RAM；而实际上，由于用户进程线性地址空间的需要，内核不能直接对1GB以上的RAM进行寻址，我们将会在后而"Linux中的分页”一节中看到这一点。

然而，大型服务器需要大于4GB的RAM来同时运行数以千计的进程，所以必须扩展32位x86结构所支持的RAM容量。Intel通过在它的处理器上把管脚数从32增加到36已经满足了这些需求。寻址能力可达到2^36^ = 64GB。不过，只有引入一种新的分页机制把32位线性地址转换为36位物理地址才能使用所增加的物理地址。

从Pentium Pro处理器开始，Intel引入一种叫做 物理地址扩展（Physical Address Extension， PAE）的机制。另外一种叫做页大小扩展[Page Size Extension (PSE-36)]的机制在Pentium 3处理器中引入，但是Linux并没有采用这种机制。

通过设置cr4控制寄存器中的物理地址扩展（PAE）标志激活PAE。页目录项中的页大小标志PS启用大尺寸页(在PAE启用时为2MB)。

Intel为了支持PAE已经改变了分页机制：

- 64GB的RAM被分为2^24^个页框（4KB），页表项的物理地址字段从20位扩展到了24位。因为PAE页表项必须包含12个标志位(在前面已描述)和24个物理地址位，总数之和为36，页表项大小从32位变为64位增加了一倍。结果，一个4KB的页表包含512个表项而不是1024个表项。
- 引入一个叫做页目录指针表(Page Directory Pointer Table，  PDPT)的页表新级别，它由4个64位表项组成。
- cr3控制寄存器包含一个27位的页目录指针表(PDPT)基地址字段。因为PDPT存放在RAM的前4GB中，并在32字节(2^5^)的倍数上对齐，因此27位足以表示这种表的基地址。
- 当把线性地址映射到4KB的页时(页目录项中的PS标志清0)， 32位线性地址按下列方式解释：
  - cr3：指向一个PDPT
  - 位31-30：指向PDPT中4个项中的一个
  - 位29-21：指向页目录中512个项目中的一个
  - 位20-12：指向页表中512项中的一个
  - 位11-0：4KB页中的偏移量
- 当把线性地址映射到2MB的页时(页目录项中的PS标志置为1)， 32位线性地址按下列方式解释：
  - cr3：指向一个PDPT
  - 位31-30：指向PDPT中4个项中的一个
  - 位29-21：指向页目录中512个项中的一个
  - 位20-0：2MB页中的偏移量

总之，一旦cr3被设置，就可能寻址高达4GB RAM。如果我们希望对更多的RAM寻址，就必须在cr3中放置一个新值，或改变PDPT的内容。然而，使用PAE的主要问题是线性地址仍然是32位长。这就迫使内核编程人员用同一线性地址映射不同的RAM区。很明显，PAE并没有扩大进程的线性地址空间，因为它只处理物理地址。此外，只有内核能够修改进程的页表，所以在用户态下运行的进程不能使用大于4GB的物理地址空间。另一方面，PAE允许内核使用容量高达64GB的RAM，从而显著增加了系统中的进程数量。

#### 64位系统中的分页

32位微处理器普遍采用两级分页。然而两级分页并不适用于采用64位系统的计算机。让我们用一种思维实验来解释为什么：首先假设一个大小为4KB的标准页。因为1KB覆盖2^10^个地址的范围，4KB覆盖2^12^个地址，所以offset字段是12位。这样线性地址就剩下52位分配给Table和Directory字段。如果我们现在决定仅仅使用64位中的48位来寻址(这个限制仍然使我们自在地拥有256TB的寻址空间！)，剩下的48-12=36位将被分配给Table和Directory字段。如果我们现在决定为两个字段各预留18位，那么每个进程的页目录和页表都含有2^18^个项，即超过256000个项。

由于这个原因，所有64位处理器的硬件分页系统都使用了额外的分页级别。使用的级别数量取决于处理器的类型。

| 平台名称   | 页大小  | 寻址使用的位数 | 分页级别数 | 线性地址分级      |
| ------ | ---- | ------- | ----- | ----------- |
| alpha  | 8KB  | 43      | 3     | 10+10+10+13 |
| ia64   | 4KB  | 39      | 3     | 9+9+9+12    |
| ppc64  | 4KB  | 41      | 3     | 10+10+9+12  |
| sh64   | 4KB  | 41      | 3     | 10+10+9+12  |
| x86_64 | 4KB  | 48      | 4     | 9+9+9+9+12  |

#### 硬件高速缓存

当今的微处理器时钟频率接近几个GHz，而动态RAM（DRAM）芯片的存取时间是时钟周期的数百倍。这意味着，当从RAM中取操作数或向RAM中存放结果这样的指令执行时，CPU可能等待很长时间。

为了缩小CPU和RAM之间的速度不匹配，引入了硬件高速缓存内存(hardware cache memory )。硬件高速缓存基于著名的 局部性原理(locality principle)，该原理既适用程序结构也适用数据结构。这表明由于程序的循环结构及相关数组可以组织成线性数组，最近最常用的相邻地址在最近的将来又被用到的可能性极大。因此，引入小而快的内存来存放最近最常使用的代码和数据变得很有意义。为此，x86体系结构中引入了一个叫 行(line)的新单位。行由几十个连续的字节组成，它们以脉冲突发模式(burst mode)在慢速DRAM和快速的用来实现高速缓存的片上静态RAM ( SRAM)之间传送，用来实现高速缓存。

高速缓存再被细分为行的子集。在一种极端的情况下，高速缓存可以是直接映射的(direct mapped)，这时主存中的一个行总是存放在高速缓存中完全相同的位置。在另一种极端情况下，高速缓存是充分关联的(fully  associative)，这意味着主存中的任意一个行可以存放在高速缓存中的任意位置。但是大多数高速缓存在某种程度上是N-路组关联的(N-way set associative)，意味着主存中的任意一个行可以存放在高速缓存N行中的任意一行中。例如，内存中的一个行可以存放到一个2路组关联高速缓存两个不同的行中。

![处理器硬件高速缓存.png](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E5%A4%84%E7%90%86%E5%99%A8%E7%A1%AC%E4%BB%B6%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98.png?raw=true)

高速缓存单元插在分页单元和主内存之间。它包含一个硬件高速缓存内存(hardware cache memory)和一个高速缓存控制器(cache controller) 。高速缓存内存存放内存中真正的行。高速缓存控制器存放一个表项数组，每个表项对应高速缓存内存中的一个行。每个表项有一个标签（tag）和描述高速缓存行状态的几个标志（flag）。这个标签由一些位组成，这些位让高速缓存控制器能够辨别由这个行当前所映射的内存单元。这种内存物理地址通常分为3组：最高几位对应标签，中间几位对应高速缓存控制器的子集索引，最低几位对应行内的偏移量。

当访问一个RAM存储单元时，CPU从物理地址中提取出子集的索引号并把子集中所有行的标签与物理地址的高几位相比较。如果发现某一个行的标签与这个物理地址的高位相同，则CPU命中一个高速缓存(cache hit);否则，高速缓存没有命中(cache miss )。

当命中一个高速缓存时，高速缓存控制器进行不同的操作，具体取决于存取类型。对于读操作，控制器从高速缓存行中选择数据并送到CPU寄存器;不需要访问RAM因而节约了CPU时间，因此，高速缓存系统起到了其应有的作用。对于写操作，控制器可能采用以下两个基本策略之一，分别称之为通写（write-through）和回写(write-back)。在通写中，控制器总是既写RAM也写高速缓存行，为了提高写操作的效率关闭高速缓存。回写方式只更新高速缓存行，不改变RAM的内容，提供了更快的功效。当然，回写结束以后，RAM最终必须被更新。只有当CPU执行一条要求刷新高速缓存表项的指令时，或者当一个FLUSH硬件信号产生时(通常在高速缓存不命中之后)，高速缓存控制器才把高速缓存行写回到RAM中。

当高速缓存没有命中时，高速缓存行被写回到内存中，如果有必要的话，把正确的行从RAM中取出放到高速缓存的表项中。

多处理器系统的每一个处理器都有个单独的硬件高速缓存，因此它们去要额外的硬件电路用于保持高速缓存内容的同步。每个CPU都有自己的本地硬件高速缓存。但是，现在更新变得更耗时：只要一个CPU修改了它的硬件高速缓存，它就必须检查同样的数据是否包含在其他的硬件高速缓存中;如果是，它必须通知其他CPU用适当的值对其更新。常把这种活动叫做高速缓存侦听（cache snooping）。所有这一切都在硬件级处理，内核无需关心。

处理器的cr0寄存器的CD标志位用来启用或禁用高速缓存电路。这个寄存器中的NW标志指明高速缓存是使用通写还是回写策略。

Pentium处理器高速缓存的另一个有趣的特点是，让操作系统把不同的高速缓存管理策略与每一个页框相关联。为此，每一个页目录项和每一个页表项都包含两个标志：PCD(Page Cache Disablt)标志指明当访问包含在这个页框中的数据时，高速缓存功能必须被启用还是禁用。PWT(page Write-Through)标志指明当把数据写到页框时，必须使用的策略是回写策略还是通写策略。Linux清除了所有页目录项和页表项中的PCD和PWT标志；结果是：对于所有的页框都启用高速缓存，对于写操作总是采用回写策略。

#### 转换后援缓冲器（TLB）

除了通用硬件高速缓存之外，x86处理器还包含了另一个称为转换后援缓冲器或TLB(Translation Lookaside Buffer)的高速缓存用于加快线性地址的转换。当一个线性地址被第一次使用时，通过慢速访问RAM中的页表计算出相应的物理地址。同时，物理地址被存放在一个TLB表项(TLB entry)中，以便以后对同一个线性地址的引用可以快速地得到转换。

在多处理系统中，每个CPU都有自己的TLB，这叫做该CPU的本地TLB。与硬件高速缓存相反，TLB中的对应项不必同步，这是因为运行在现有CPU上的进程可以使同一线线性地址与不同的物理地址发生联系。

当CPU的cr3控制寄存器被修改时，硬件自动使本地TLB中的所有项都无效，这是因为新的一组页表被启用而TLB指向的是旧数据。

### Linux中的分页

Linux采用了一种同时适用于32位和64位系统的普通分页模型。正像前面所解释的那样，两级页表对32位系统来说已经足够了，但64位系统需要更多数量的分页级别。直到2.6.10版本，Linux采用三级分页的模型。从2.6.11版本开始，采用了四级分页模型。下图中展示的4种页表分别被为：

- 页全局目录(Page Global Directory )
- 页上级目录(Page Upper Directory )
- 页中级目录(Page Middle Directory )
- 页表(Page Table)

![Linux分页模式.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/Linux%E5%88%86%E9%A1%B5%E6%A8%A1%E5%BC%8F.jpg?raw=true)

页全局目录包含若干页上级目录的地址，页上级目录又依次包含若干页中间目录的地址，而页中间目录又包含若干页表的地址。每一个页表项指向一个页框。线性地址因此被分成五个部分。每一部分的大小与具体的计算机体系结构有关。

对于没有启用物理地址扩展的32位系统，两级页表已经足够了。Linux通过使“页上级目录”位和“页中间目录”位全为0，从根本上取消了页上级目录和页中间目录字段。不过，页上级目录和页中间目录在指针序列中的位置被保留，以便同样的代码在32位系统和64位系统下都能使用。内核为页上级目录和页中间目录保留了一个位置，这是通过把它们的页目录项数设置为1，并把这两个目录项映射到页全局目录的一个适当的目录项而实现的。

启用了物理地址扩展（PAE）的32位系统使用了三级页表。Linux的页全局目录对应x86的页目录指针表(PDPT)，取消了页上级目录，页中间目录对应x86的页目录，Linux的页表对应x86的页表。

最后，64位系统使用二级还是四级分页取决于硬件对线性地址的位的划分。

Linux的进程处理很大程度上依赖于分页。事实上，线性地址到物理地址的自动转换使下面的设计目标变得可行：

- 给每一个进程分配一块不同的物理地址空间，这确保了可以有效地防止寻址错误。
- 区别页(即一组数据)和页框(即主存中的物理地址)之不同。这就允许存放在某个页框中的一个页，然后保存到磁盘上，以后重新装入这同一页时又可以被装在不同的页框中。这就是虚拟内存机制的基本要素。

每个进程有它自己的页全局目录和自己的页表集。当发生进程切换时，Linux把cr3控制寄存器的内存保存在前一个执行进程的描述符中，然后把下一个要执行进程的描述符的值装入cr3寄存器中。因此，当新进程重新开始在CPU上执行时，分页单元指向一组正确的页表。

把线性地址映射到物理地址虽然有点复杂，但现在已经成了一种机械式的任务。下面会列举一些函数和宏，它们检索内核为了查找地址和管理表格所需的信息；其中大多数函数只有一两行。

#### 线性地址字段

下列宏简化了页表处理。

| 宏名                                       | 描述                                       |
| ---------------------------------------- | ---------------------------------------- |
| PAGE_SHIFT                               | 指定Offset字段的位数；当用于x86处理器时，它产生的值为12。由于页内所有地址都必须能放到Offset字段中，因此x86系统的页的大小是2^12^=4096字节。 |
| PMD_SHIFT                                | 指定线性地址的Offset字段和Table字段的总位数。换句话说，是页中间目录项可以映射的区域大小的对数。PMD_SIZE宏用于计算由页中间目录的一个单独表项所映射的区域大小，也就是一个页表的大小。PMD_MASK宏用于屏蔽Offset字段与Table字段的所有位。当PAE被禁用时，PMD_SHIFT产生的值为22(来自Offset的12位加上来自Table的10位)，PMD_SIZE产生的值为2^22^或4 MB ， PMD_MASK产生的值为0xffc000000相反，当PAE被激活时，PMD_SHIFT产生的值为21(来自Offset的12位加上来自Table的9位)，PMD_SIZE产生的值为2^21^或2 MB，  PMD_MASK产生的值为0xffe00000。大型页不使用最后一级页表，所以产生大型页尺寸的LARGE_PAGE_SIZE宏等于PMD_SIZE(2PMD_SHIFT)，而在大型页地址中用于屏蔽Offset字段和Table字段的所有位的LARGE_PAGE_MASK宏，就等于PMD_MASK。 |
| PUD_SHIFT                                | 确定页上级目录项能映射的区域大小的对数。PUD_SIZE宏用于计算页全局目录中的一个单独表项所能映射的区域大小。PUD_ MASK宏用于屏蔽Offset字段、Table字段、Middle Air字段和Upper Air字段的所有位。在x86处理器上，PUD_SHIFT总是等价于PMD_SHIFT，而PUD_SIZE则等于4MB或2MB。 |
| PGDIR_SHIFT                              | 确定页全局目录项能映射的区域大小的对数。PGDIR_SIZE宏用于计算页全局目录中一个单独表项所能映射区域的大小。PGDIR_MASK宏用于屏蔽Offset， Table， Middle Air及Upper Air字段的所有位。当PAE被禁止时，PGDIR_SHIFT产生的值为22(与PMD-SHIFT和PUD_SHIFT产生的值相同)，PGDIR_SIZE产生的值为2^22^或4MB，以及PGDIR_MASK产生的值为0xffc00000。相反，当PAE被激活时，PGDIR_SHIFT产生的值为30(12位Offset加9位Table再加9位Middle Air)，PGD工R_S工ZE产生的值为2^30^或1 GB以及PGDIR_ MASK产生的值为0xc0000000。 |
| PTRS_PER_PTE  PTRS_PER_PMD PTRS_PER_PUD PTRS_PER_PGD | 用于计算页表、页中间目录、页上级目录和页全局目录表中表项的个数。当PAE被禁止时，它们产生的值分别为1024，1，1和1024。当PAE被激活时，产生的值分别为512，512，1和4。 |

#### 页表处理

pte_t， pmd_t， pud_t和pgd_t分别描述页表项、页中间目录项、页上级目录和页全局目录项的格式。当PAE被激活时它们都是64位的数据类型，否则都是32位数据类型。pgprot_t是另一个64位(PAE激活时)或32位(PAE禁用时)的数据类型，它表示与一个单独表项相关的保护标志。

五个类型转换宏(\_\_pte、\_\_pmd、\_\_pud、\_\_pgd和__pgprot)把一个无符号整数转换成所需的类型。另外的五个类型转换宏(pte_val， pmd_val， pud_val， pgd_val和pgprot_val)执行相反的转换，即把上面提到的四种特殊的类型转换成一个无符号整数。
内核还提供了许多宏和函数用于读或修改页表表项：

- 如果相应的表项值为0，那么，宏pte_none， pmd_none， pud_none和pgd_none产生的值为1，否则产生的值为0。
- 宏pte_clear， pmd_clear， pud_clear和pgd_clear清除相应页表的一个表项，由此禁止进程使用由该页表项映射的线性地址。ptep_get_and_clear()函数清除一个页表项并返回前一个值。
- set_pte， set_pmd， set_pud和 set_pgd向一个页表项中写入指定的值。set_pte_atomic与set_pte的作用相同，但是当PAE被激活时它同样能保证64位的值被原子地写入。
- 如果a和b两个页表项指向同一页并且指定相同的访问优先级，那么pte_same(a，b)返回1，否则返回0。
- 如果页中间目录项e指向一个大型页(2MB或4MB )，那么pmd_large(e)返回1，否贝返回0。

宏pmd_bad由函数使用并通过输入参数传递来检查页中间目录项。如果目录项指向一个不能使用的页表，也就是说，如果至少出现以下条件中的一个，则这个宏产生的值为1：

- 页不在主存中(Present标志被清除)。
- 页只允许读访问(Read/Write标志被清除)。
- Acessed或者Dirty位被清除(对于每个现有的页表，Linux总是强制设置这些 标志)。

宏pud_bad和pgd_bad总是产生0。没有定义pte_bad宏，因为页表项引用一个不在主存中的页、一个不可写的页或一个根本无法访问的页都是合法的。

如果一个页表项的Present标志或者Page Size标志等于1，则pte_present宏产生的值为1，否则为0。前面讲过页表项的Page Size标志对微处理器的分页单元来讲没有意义，然而，对于当前在主存中却又没有读、写或执行权限的页，内核将其Present和Page Size分别标记为0和1。这样，任何试图对此类页的访问都会引起一个缺页异常，因为页的Present标志被清0，而内核可以通过检查Page Size的值来检测到产生异常并不是因为缺页。

如果相应表项的Present标志等于1，也就是说，如果对应的页或页表被载入主存，pmd_present宏产生的值为1。pud_present宏和pgd_present宏产生的值总是1。

下表列出的函数用来查询页表项中任意一个标志的当前值；除了pte_file()外，其他函数只有在pte_present返回1的时候，才能正常返回页表项中任意一个标志。

| 函数名称                       | 说明                                       |
| -------------------------- | ---------------------------------------- |
| pte_user()                 | 读User/Supervisor标志                       |
| pte_read()                 | 读User/Supervisor标志（x86处理器上的页不受读的保护）      |
| pte_write()                | 读Read/Write标志                            |
| pte_exec()                 | 读User/Supervisor标志（x86处理器上的页不受代码执行的保护）   |
| pte_dirty()                | 读Dirty标志                                 |
| pte_young()                | 读Accessed标志                              |
| pte_file()                 | 读Dirty标志（当Present标志被清除而Dirty标志被设置时，页属于一个非线性磁盘文件映射） |
| mk_pte_huge()              | 设置页表项中的Page Size和Present标志               |
| pte_wrprotect()            | 清除Read/Write标志                           |
| pte_rdprotect()            | 清除User/Supervisor标志                      |
| pte_exprotect()            | 清除User/Supervisor标志                      |
| pte_mkwrite()              | 设置Read/Write标志                           |
| pte_mkread()               | 设置User/Supervisor标志                      |
| pte_mkexec()               | 设置User/Supervisor标志                      |
| pte_mkclean()              | 清除Dirty标志                                |
| pte_mkdirty()              | 设置Dirty标志                                |
| pte_mkold                  | 清除Accessed标志（此页标记为未访问）                   |
| pte_mkyoung                | 设置Accessed标志（此页标记为访问过）                   |
| pte_modify(p， v)           | 把页表项p的所有访问权限设置为指定的值v                     |
| ptep_set_wrprotect()       | 类似pre_wrprotect()，但作用于指向页表项的指针           |
| ptep_set_access_flags()    | 如果Dirty标志被设置为1，则将页的存取权限设置为指定的值，并调用flush_tlb_page()函数。 |
| ptep_mkdirty()             | 类似pte_mkdirty()，但作用于指向页表项的指针             |
| pte_test_and_clear_dirty() | 类似pte_mkclean()，但作用于指向页表项的指针并返回Dirty标志的旧值 |
| pte_test_and_clear_young() | 类似pte_mkold()，但作用于指向页表项的指针并返回Accessed标志的旧值 |

下表为一些宏，它们把一个页地址和一组保护标志组合成页表项，或者执行相反的操作，从一个页表项中提取出页地址。注意这其中一些宏对页的引用是通过“页描述符”的线性地址，而不是通过该页本身的线性地址。

| 宏名称                         | 说明                                       |
| --------------------------- | ---------------------------------------- |
| pgd_index(addr)             | 找到线性地址addr对应的目录项在页全局目录中的索引（相对位置）         |
| pgd_offset(mm，addr)         | 以内存描述符和线性地址作为参数。产生地址addr在页全局目录中相应表项的线性地址；通过内存描述符mm内的一个指针可以找到这个页全局目录 |
| pgd_offset_k(addr)          | 产生主内核页全局目录中的某个项的线性地址，该项                  |
| pgd_page(pgd)               | 通过页全局目录项pgd产生页上级目录所在页框的页描述符地址。在两级或三级分页系统中，该宏等价于pud_page()，后者应用于页上级目录项 |
| pud_offset(pgd，addr)        | 接收指向页全局目录项的指针pgd和线性地址addr作为参数。这个宏产生页上级目录中目录项addr对应的线性地址。在两级或三级分页系统中，该宏产生pgd，即一个页全局目录项的地址 |
| pud_page(pud)               | 通过页上级目录项pud产生相应的页中间目录的线性地址。在两级分页系统中，该宏等价于pmd_page() ，后者应用于页中间目录项 |
| pmd_index(addr)             | 产生线性地址addr在页中间目录中所对应目录项的索引(相对位置)         |
| pmd_offset(pud，addr)        | 接收指向页上级目录项的指针pud和线性地址addr作为参数。这个宏产生目录项addr在页中间目录中的偏移地址。在两级或三级分页系统中，它产生pud，即页全局目录项的地址 |
| pmd_page(pmd)               | 通过页中间目录项pmd产生相应页表的页描述符地址。在两级或三级分页系统中，pmd实际上是页全局目录中的一项 |
| mk_pte(p，prot)              | 接收页描述符地址P和一组存取权限prot作为参数，并创建相应的页表项       |
| pte_index(addr)             | 产生线性地址addr对应的表项在页表中的索引(相对位置)             |
| pte_offset_kernel(dir，addr) | 线性地址addr在页中间目录dir中有一个对应的项，该宏就产生这个对应项，即页表的线性地址。另外，该宏只在主内核页表上使用 |
| pte_offset_map(dir，addr)    | 接收指向一个页中间目录项的指针dir和线性地址addr作为参数，它产生与线性地址addr相对应的页表项的线性地址。如果页表被保存在高端内存中，那么内核建立一个临时内核映射，并用pte_unmap对它进行释放。pte_offset_map_nested宏和pte_unmap_nested宏是相同的，但它们使用不同的临时内核映射 |
| pte_page(x)                 | 返回页表项x所引用页的描述符地址                         |
| pte_to_pgoff(pte)           | 从一个页表项的pte字段内容中提取出文件偏移量，这个偏移量对应着一个非线性文件内存映射所在的页 |
| pgoff_to_pte(offset)        | 为非线性文件内存映射所在的页创建对应页表项的内容                 |

下表罗列最后一组函数来简化页表项的创建和撤消。

当使用两级页表时，创建或删除一个页中间目录项是不重要的。页中间目录仅含有一个指向下属页表的目录项。所以，页中间目录项只是页全局目录中的一项而已。然而当处理页表时，创建一个页表项可能很复杂，因为包含页表项的那个页表可能就不存在。在这样的情况下，有必要分配一个新页框，把它填写为0，并把这个表项加入。

如果PAE被激活，内核使用三级页表。当内核创建一个新的页全局目录时，同时也分配四个相应的页中间目录;只有当父页全局目录被释放时，这四个页中间目录才得以释放。

当使用两级或三级分页时，页上级目录项总是被映射为页全局目录中的一个单独项。

| 函数名称                            | 说明                                       |
| ------------------------------- | ---------------------------------------- |
| pgd_alloc(mm)                   | 分配一个新的页全局目录。如果PAE被激活，它还分配3个对应用户态线性地址的子页中间目录。mm在x86体系结构上被忽略 |
| pgd_free(pgd)                   | 释放页全局目录中地址为pgd的项。如果PAE被激活，它还将释放用户态线性地址对应的三个页中间目录 |
| pud_alloc(mm，pgd，addr)          | 在两级或三级分页系统下，这个函数什么也不做：它仅仅返回页全局目录项pgd的线性地址 |
| pud_free(x)                     | 在两级或三级分页系统下，这个宏什么也不做                     |
| pmd_alloc(mm，pud，addr)          | 定义这个函数以使普通三级分页系统可以为线性地址addr分配一个新的页中间目录。如果PAE未被激活，这个函数只是返回输入参数pud的值，也就是说，返回页全局目录中目录项的地址。如果PAE被激活，该函数返回线性地址addr对应的页 |
| pmd_free(x)                     | 该函数什么也不做，因为页中间目录的分配和释放是随同它们的父全局目录一同进行的   |
| pte_alloc_map(mm，pmd，addr)      | 接收页中间目录项的地址pmd和线性地址addr作为参数，并返回与addr对应的页表项的地址。如果页中间目录项为空，该函数通过调用函数pte_alloc_one()分配一个新页表。如果分配了一个新页表，addr对应的项就被创建，同时User/Supervisor标志被设置为1。如果页表被保存在高端内存，贝内核建立一个临时内核映射，并用pte_unmap对它进行释放 |
| pte_alloc_kernel(mm，pmd，addr)   | 如果与地址addr相关的页中间目录项pmd为空，该函数分配一个新页表。然后返回与addr相关的页表项的线性地址。该函数仅被主内核页表使用 |
| pte_free(pte)                   | 释放与页描述符指针pte相关的页表                        |
| pte_free_kernel(pte)            | 等价于pte_free()，但由主内核页表使用                  |
| clear_page_range(mmu，start，end) | 从线性地址start到end通过反复释放页表和清除页中间目录项来清除进程页表的内容 |

#### 物理内存布局

可参考   [地址空间布局](http：//www.cnblogs.com/chengxuyuancc/archive/2013/04/17/3026920.html)

在初始化阶段，内核必须建立一个物理地址映射来指定哪些物理地址范围对内核可用而哪些不可用。

内核将下列页框记为保留：

- 在不可用的物理地址范围内的页框。
- 含有内核代码和已初始化的数据结构的页框。

保留页框中的页绝不能被动态分配或交换到磁盘上。

一般来说，Linux内核安装在RAM中从物理地址0x00100000开始的地方，也就是说，从第二个MB开始。所需页框总数依赖于内核的配置方案：典型的配置所得到的内核可以被安装在小于3MB的RAM中。

为什么内核没有安装在RAM第一个MB开始的地方?因为PC体系结构有几个独特的地方必须考虑到。例如：

- 页框0由BIOS使用，存放加电自检(Power-On Self-Test， POST)期间检查到的系统硬件配置。因此，很多膝上型电脑的BIOS甚至在系统初始化后还将数据写到该页框。
- 物理地址从0x000a0000到0x000fffff的范围通常留给BIOS例程，并且映射ISA图形卡上的内部内存。这个区域就是所有IBM兼容PC上从640KB到1MB之间著名的洞：物理地址存在但被保留，对应的页框不能由操作系统使用。
- 第一个MB内的其他页框可能由特定计算机模型保留。例如，IBM Thinkpnd把0xa0页框映射到0x9f页框。

在启动过程的早期阶段，内核询问BIOS并了解物理内存的大小。在新近的计算机中，内核也调用BIOS过程建立一组物理地址范围和其对应的内存类型。

随后，内核执行machine_specific_memory_setup()函数，该函数建立物理地址映射。当然，如果这张表是可获取的，那是内核在BIOS列表的基础上构建的。否则，内核按保守的缺省设置构建这张表：从0x9f000(LOWMEMSIZE())到0x100000(HIGH_MEMORY)号的所有页框都标记为保留。

| 开始          | 结束           | 类型        |
| ----------- | ------------ | --------- |
| 0x0000 0000 | 0x0009 ffff  | Usable    |
| 0x000f 0000 | 0x000f ffff  | Reserved  |
| 0x0010 0000 | 0x07fe ffff  | Usable    |
| 0x07ff 0000 | 0x07ff 2ffff | ACPI data |
| 0x07ff 3000 | 0x07ff ffff  | ACPI NVS  |
| 0xffff 0000 | 0xffff ffff  | Reserved  |

上表显示了具有128MB(0x0800 0000) RAM计算机的典型配置。从0x07ff 0000到0x07ff 2fff 的物理地址范围中存有加电自检(POST)阶段由BIOS写入的系统硬件设备信息。在初始化阶段，内核把这些信息拷贝到一个合适的内核数据结构中，然后认为这些页框是可用的。相反，从0x07ff3000到0x07ff ffff的物理地址范围被映射到硬件设备的ROM芯片。从0xffff 0000开始的物理地址范围标记为保留，因为它由硬件映射到BIOS的ROM芯片。注意BIOS也许并不提供一些物理地址范围的信息(在上述表中，范围是0x000a 0000到0x000e ffff)。为安全可靠起见，Linux假定这样的范围是不可用的。

内核可能不会见到BIOS报告的所有物理内存：例如，如果未使用PAE支持来编译，即使有更大的物理内存可供使用，内核也只能寻址4GB大小的RAM。setup_memory()函数在machine_specific_memory_setup()执行后被调用：它分析物理内存区域表并初始化一些变量来描述内核的物理内存布局，这些变量如下表所示。

| 变量名称            | 说明                        |
| --------------- | ------------------------- |
| num_physpages   | 最高可用页框的页框号                |
| totalram_pages  | 可用页框的总数量                  |
| min_low_pfn     | RAM中在内核映像后第一个可用页框的页框号     |
| max_pfn         | 最后一个可用页框的页框号              |
| max_low_pfn     | 被内核直接映射的最后一个页框的页框号（低地址内存） |
| totalhigh_pages | 内核非直接映射的页框总数（高地址内存）       |
| highstart_pfn   | 内核非直接映射的第一个页框的页框号         |
| highend_pfn     | 内核非直接映射的最后一个页框的页框号        |

为了避免把内核装入一组不连续的页框里，Linux更愿跳过RAM的第一个MB。明确地说，Linux用PC体系结构未保留的页框来动态存放所分配的页。下图显示了Linux怎样填充前3MB的RAM：

![Linux2.6的前768个页框（3MB）.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/Linux2.6%E7%9A%84%E5%89%8D768%E4%B8%AA%E9%A1%B5%E6%A1%86%EF%BC%883MB%EF%BC%89.jpg?raw=true)

符号\_text对应于物理地址0x0010 0000，表示内核代码第一个字节的地址。内核代码的结束位代由另外一个类似的符号\_etext表示。内核数据分为两组：初始化过的数据的和没有初始化的数据。初始化过的数据在\_etext后开始，在\_edata处结束。紧接着是未初始化的数据并以\_end结束。

图中出现的符号并没有在Linux源代码中定义，它们是编译内核时产生的（可以在System.map文件中找到这些符号，System.map是编译内核以后所创建的）。

#### 进程页表

进程的线性地址空间分成两部分：

- 从0x0000 0000——0xbfff ffff的线性地址，无论进程运行在用户态还是内核态都可以寻址（0—3GB）。
- 从0xc000 0000——0xffff ffff的线性地址，只有内核的进程才能寻址。

进程运行在用户态时，所产生的线性地址小于0xc000 0000，而运行在内核态时，执行内核代码，所产生的地址大于等于0xc000 0000。但是，在某些情况下，内核为了检索或存放数据必须访问用户态线性地址空间。

宏PAGE_OFFSET产生的值是0xc000 0000，这就是进程在线性地址空间中的偏移量，也是内核生存空间的开始之处。

页全局目录的第一部分表项映射的线性地址小于0xc000 0000(在PAE未启用时是前768项，PAE启用时是前3项)，具体大小依赖于特定进程。相反，剩余的表项对所有进程来说都应该是相同的，它们等于主内核页全局目录的相应表项。

#### 内核页表

内核维持着一组自己使用的页表，驻留在所谓的主内核页全局目录(master kernel Page Global Directory)中。系统初始化后，这组页表还从未被任何进程或任何内核线程直接使用；更确切地说，主内核页全局目录的最高目录项部分作为参考模型，为系统中每个普通进程对应的页全局目录项提供参考模型。

内核初始化自己的页表，这个过程分为两个阶段。事实上，内核映像刚刚被装入内存后，CPU仍然运行于实模式，所以分页功能没有被启用。

第一个阶段，内核创建一个有限的地址空间，包括内核的代码段和数据段、初始页表和用于存放动态数据结构的共128KB大小的空间。这个最小限度的地址空间仅够将内核装入RAM和对其初始化的核心数据结构。

第二个阶段，内核充分利用剩余的RAM并适当地建立分页表。下一节解释这个方案是怎样实施的。

#### 临时内核页表

临时页全局目录是在内核编译过程中静态地初始化的，而临时页表是由startup_32()汇编语言函数(定义于arch/i386/kernel/head.S)初始化的。我们不再过多提及页上级目录和页中间目录，因为它们相当于页全局目录项。在这个阶段PAE支持并未激活。

临时页全局目录放在swapper_pg_dir变量中。临时页表在pg0变量处开始存放，紧接在内核未初始化的数据段(_end符号)后面。为简单起见，我们假设内核使用的段、临时页表和128KB的内存范围能容纳于RAM前8MB空间里。为了映射RAM前8MB的空间，需要用到两个页表。

分页第一个阶段的目标是允许在实模式下和保护模式下都能很容易地对这8MB寻址。因此，内核必须创建一个映射，把从0x0000 0000到0x007f ffff的线性地址和从0xc000 0000到0xc07f ffff的线性地址映射到从0x0000 0000到0x007f ffff的物理地址。换句话说，内核在初始化的第一阶段，可以通过与物理地址相同的线性地址或者通过从0xc000 0000开始的8MB线性地址对RAM的前8MB进行寻址。

内核通过把swapper_pg_dir所有项都填充为0来创建期望的映射，不过，0、1、0x300(十进制768)和0x301(十进制769)这四项除外。后两项包含了从0xc000 0000到0xc07f ffff间的所有线性地址。0、1、0x300和0x301按以下方式初始化：

- 0项和0x300项的地址字段置为pg0的物理地址，而1项和0x301项的地址字段 置为紧随pg0后的页框的物理地址。
- 把这四个项中的Present、Read/Write和User/Supervisor标志置位。
- 把这四个项中的Accessed、Dirty、PCD、PWD和Page Size标志清0。

汇编语言函数startup_32()也启用分页单元，通过向cr3控制寄存器装入swapper_pg_dir的地址及设置cr0控制寄存器的PG标志来达到这一目的。下面是等价的代码片段：

```c
movl $swapper_pg_dir-0xc0000000，%eax
movl %eax，%cr3		/*设置页表指针*/
movl %cr0，%eax
orl $0x80000000，%eax
movl %eax，%cr0		/*设置分页(PG)位“/
```

#### 当RAM小于896MB时的最终内核页表

由内核页表所提供的最终映射必须把从0xc0000000开始的线性地址转化为从0开始的物理地址。

宏\_\_pa用于把从PAGE_OFFSET开始的线性地址转换成相应的物理地址，而宏\_\_va做相反的转化。

主内核页全局目录仍然保存在swapper_pg_dir变量中。它由paging_init()函数初始化。该函数进行如下操作：

1. 调用pagetable_init()适当地建立页表项。
2. 把swapper_pg_dir的物理地址写入cr3控制寄存器中。
3. 如果CPU支持PAE并且如果内核编译时支持PAE，则将cr4控制寄存器的PAE标志置位。
4. 调用\_\_flush_tlb_all()使TLB的所有项无效。

pagetable_init()执行的操作既依赖于现有RAM的容量，也依赖于CPU模型。计算机有小于896MB(1024-128，128MB留给其他映射)的RAM， 32位物理地址足以对所有可用RAM进行寻址，因而没有必要激活PAE机制。
swapper_pg_dir页全局目录由如下等价的循环重新初始化：

```c
pgd = swapper_pg_dir + pgd_index(PAGE_OFFSET);	/*768*/
phys_addr = 0x00000000;
while(phys_addr < (max_low_pfn * PAGE_SIZE))
{
    pmd = one_md_table_init(pgd);	/*返回pgd*/
  	set_pmd(pmd， __pmd(phys_addr | pgprot_val(__pgprot(0x1e3))));
  /*0x1e3 == Present，Accessed，Dirty，Read/Write，Page Size，Global*/
  	phys_addr += PTRS_PER_PTE * PAGE_SIZE;	/*0X400000*/
  	++pgd;
}
```

我们假定CPU是支持4MB页和“全局(global)" TLB表项的最新x86微处理器。注意如果页全局目录项对应的是0xc0000000之上的线性地址，则把所有这些项的User/Supervisor标志清0，由此拒绝用户态进程访问内核地址空间。还要注意Page Size被置位使得内核可以通过使用大型页来对RAM进行寻址(“扩展分页”)。

由startup_32()函数创建的物理内存前8MB的恒等映射用来完成内核的初始化阶段。当这种映射不再必要时，内核调用zap_low_mappings()函数清除对应的页表项。

实际上，这种描述并未说明全部事实。我们将在后面“固定映射的线性地址”一节看到，内核也调整与“固定映射的线性地址”对应的页表项。

#### 当RAM大小在896MB—4096MB时的最终内核页表

在这种情况下，并不把RAM全部映射到内核地址空间。Linux在初始化阶段可以做的最好的事是把一个具有896MB的RAM窗口(window)映射到内核线性地址空间。如果一个程序需要对现有RAM的其余部分寻址，那就必须把某些其他的线性地址间隔映射到所需的RAM。这意味着修改某些页表项的值。将在以后（L-B：第八章 内存管理）讨论这种动态重映射是如何进行的。

内核使用与前一种情况相同的代码来初始化页全局目录。

#### 当RAM大于4096MB时的最终内核页表

现在考虑RAM大于4GB计算机的内核页表初始化;更确切地说，处理以下发生的情况：

- CPU模型支持物理地址扩展(PAE)
- RAM容量大于4GB
- 内核以PAE支持来编译

尽管PAE处理36位物理地址，但是线性地址依然是32位地址。如前所述，Linux映射一个896MB的RAM窗口到内核线性地址空间，剩余RAM留着不映射，并由动态重映射来处理，（L-B：第八章 内存管理）将对此进行描述。与前一种情况的主要差异是使用三级分页模型，因此页全局目录按以下循环代码来初始化：

```c
pgd_idx = pgd_index(PAGE_OFFSET);	/* 3 */
for(i = 0; i < pgd_idx; i++)
    set_pgd(swapper_pg_dir + i， __pgd(__pa(empty_zero_page) + 0x001));		/* 0x001 == Present */
pgd = swapper_pg_dir + pgd_idx;
phys_addr = 0x00000000;
for(; i < PTRS_PER_PGD; ++i， ++pgd)
{
    pmd = (pmd_t *)alloc_bootmem_low_pages(PAGE_SIZE);
  	set_pgd(pgd， __pgd(__pa(pmd) | 0x001));
  	if(phys_addr < max_low_pfn * PAGE_SIZE)
      	for(j = 0; j < PTRS_PER_PMD && phys_addr < max_low_pfn * PAGE_SIZE; ++j)
        {
            set_pmd(pmd， __pmd(phys_addr | pgprot_val(__pgprot(0x1e3))));
          	/*0x1e3 == Present，Accessed，Dirty，Read/Write，Page Size，Global*/
            phys_addr += PTRS_PER_PTE * PAGE_SIZE;	/* 0x200000 */
        }
}
swapper_pg_dir[0] = swapper_pg_dir(pgd_idx);
```

页全局目录中的前三项与用户线性地址空间相对应，内核用一个空页(empty_zeropage)的地址对这三项进行初始化。第四项用页中间目录(pmd)的地址初始化，该页中间目录是通过调用alloc_bootmem_low_pages()分配的。页中间目录中的前448项(有512项，但后64项留给非连续内存分配)用RAM前896MB的物理地址填充。

注意，支持PAE的所有CPU模型也支持大型2MB页和全局页。正如前一种情况一样，只要可能，Linux使用大型页来减少页表数。

然后页全局目录的第四项被拷贝到第一项中，这样好为线性地址空间的前896MB中的低物理内存映射作镜像。为了完成对SMP系统的初始化，这个映射是必需的：当这个映射不再必要时，内核通过调用zap_low_mappings()函数来清除对应的页表项，正如先前的情况一样。

#### 固定映射的线性地址

内核线性地址第四个GB的初始部分映射系统的物理内存。但是，至少128MB的线性地址总是留作他用，因为内核使用这些线性地址实现非连续性内存分配和固定映射的线性地址。

非连续内存分配仅仅是动态分配和释放内存页的一种特殊方式。

固定映射的线性地址（fix-mapped linear address）基本上是一种类似于0xffff c000这样的常量线性地址，其对应的物理地址不必等于线性地址减去0xc000 0000，而是可以以任意方式建立。因此，每个固定映射的线性地址都映射一个物理内存的页框。之后会看到，内核使用固定映射的线性地址来代替指针变量，因为这些指针变量的值从不改变。

固定映射的线性地址概念上类似于对RAM前896MB映射的线性地址。不过，固定映射的线性地址可以映射任何物理地址，而由第4GB初始部分的线性地址所建立的映射是线性的（线性地址X 映射物理地址X-PAGE_OFFSET）。
就指针变量而言，固定映射的线性地址更有效。事实上，间接引用一个指针变量比间接引用一个立即常量地址要多一次内存访问。此外，在间接引用一个指针变量之前对其值进行检查是一个良好的编程习惯；相反，对一个常量线性地址的检查则是没有必要的。

每个固定映射的线性地址都由定义于enum fixed_addresses的数据结构中的整型索引来表示：

```c
enum fixed_addresses {
#ifdef CONFIG_X86_32
	FIX_HOLE，
	FIX_VDSO，
#else
	VSYSCALL_LAST_PAGE，
	VSYSCALL_FIRST_PAGE = VSYSCALL_LAST_PAGE
			    + ((VSYSCALL_END-VSYSCALL_START) >> PAGE_SHIFT) - 1，
	VSYSCALL_HPET，
#endif
	FIX_DBGP_BASE，
	FIX_EARLYCON_MEM_BASE，
#ifdef CONFIG_PROVIDE_OHCI1394_DMA_INIT
	FIX_OHCI1394_BASE，
#endif
#ifdef CONFIG_X86_LOCAL_APIC
	FIX_APIC_BASE，	/* local (CPU) APIC) -- required for SMP or not */
#endif
#ifdef CONFIG_X86_IO_APIC
	FIX_IO_APIC_BASE_0，
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + MAX_IO_APICS - 1，
#endif
#ifdef CONFIG_X86_VISWS_APIC
	FIX_CO_CPU，	/* Cobalt timer */
	FIX_CO_APIC，	/* Cobalt APIC Redirection Table */
	FIX_LI_PCIA，	/* Lithium PCI Bridge A */
	FIX_LI_PCIB，	/* Lithium PCI Bridge B */
#endif
#ifdef CONFIG_X86_F00F_BUG
	FIX_F00F_IDT，	/* Virtual mapping for IDT */
#endif
#ifdef CONFIG_X86_CYCLONE_TIMER
	FIX_CYCLONE_TIMER， /*cyclone timer register*/
#endif
#ifdef CONFIG_X86_32
	FIX_KMAP_BEGIN，	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1，
#ifdef CONFIG_PCI_MMCONFIG
	FIX_PCIE_MCFG，
#endif
#endif
#ifdef CONFIG_PARAVIRT
	FIX_PARAVIRT_BOOTMAP，
#endif
	FIX_TEXT_POKE1，	/* reserve 2 pages for text_poke() */
	FIX_TEXT_POKE0， /* first page is last， because allocation is backward */
	__end_of_permanent_fixed_addresses，
	/*
	 * 256 temporary boot-time mappings， used by early_ioremap()，
	 * before ioremap() is functional.
	 *
	 * If necessary we round it up to the next 256 pages boundary so
	 * that we can have a single pgd entry and a single pte table：
	 */
#define NR_FIX_BTMAPS		64
#define FIX_BTMAPS_SLOTS	4
#define TOTAL_FIX_BTMAPS	(NR_FIX_BTMAPS * FIX_BTMAPS_SLOTS)
	FIX_BTMAP_END =
	 (__end_of_permanent_fixed_addresses ^
	  (__end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS - 1)) &
	 -PTRS_PER_PTE
	 ? __end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS -
	   (__end_of_permanent_fixed_addresses & (TOTAL_FIX_BTMAPS - 1))
	 ： __end_of_permanent_fixed_addresses，
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + TOTAL_FIX_BTMAPS - 1，
#ifdef CONFIG_X86_32
	FIX_WP_TEST，
#endif
#ifdef CONFIG_INTEL_TXT
	FIX_TBOOT_BASE，
#endif
	__end_of_fixed_addresses
};
```

每个固定映射的线性地址都存放在线性地址第四个GB的末端。fix_to_virt()函数计算从给定索引开始的常量线性地址：

```c
static __always_inline unsigned long fix_to_virt(const unsigned int idx)
{
	/*
	 * this branch gets completely eliminated after inlining，
	 * except when someone tries to use fixaddr indices in an
	 * illegal way. (such as mixing up address types or using
	 * out-of-range indices).
	 *
	 * If it doesn't get removed， the linker will complain
	 * loudly with a reasonably clear error message..
	 */
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();

	return __fix_to_virt(idx);
}
```

假定某个内核函数调用fix_to_virt(FIX_IO_APIC_BASE\_0)。因为该函数声明为"\_\_always_inline"，所以C编译程序不调用fix_to_virt()，而仅仅把它的代码插入到调用函数中。此外，运行时从不对这个索引值进行检查。事实上，FIX_IO_APIC_BASE_0是个等于3的常量，因此编译程序可以去掉if语句，因为它的条件在编译时为假。相反，如果条件为真，或者参数不是一个常量，则编译程序在连接阶段产生一个错误，因为符号__this_fixmap_does_not_exist在别处没有定义。最后，编译程序计算0xffff f000-(3 << PAGE_SHIFT)，并用常量线性地址0xfff c000代替fix_to_virt()函数调用。

为了把一个物理地址与固定映射的线性地址关联起来，内核使用set_fixmap(idx，phys)和set_fixmap_nocache(idx，phys)宏。这两个函数都把fix_to_virt(idx)线性地址对应的一个页表项初始化为物理地址phys；不过，第二个函数也把页表项的PCD标志置位，因此，当访问这个页框中的数据时禁用硬件高速缓存。反过来，clear_fixmap(idx)用来撤消固定映射线性地址idx和物理地址之间的连接。

#### 处理硬件高速缓存和TLB

硬件高速缓存是通过高速缓存行(cache line)寻址的。L1_CACHE_BYTES宏产生以字节为单位的高速缓存行的大小。在早于Pentium 4的Intel模型中，这个宏产生的值为32；在Pentium 4上，它产生的值为128。

- 为了使高速缓存的命中率达到最优化，内核在下列决策中考虑体系结构：
  一个数据结构中最常使用的字段放在该数据结构内的低偏移部分，以便它们能够处于高速缓存的同一行中。
- 当为一大组数据结构分配空间时，内核试图把它们都存放在内存中，以便所有高速缓存行按同一方式使用。

x86微处理器自动处理高速缓存的同步，所以应用于这种处理器的Linux内核并不处理任何硬件高速缓存的刷新。不过内核却为不能同步高速缓存的处理器提供了高速缓存刷新接口。

处理器不能自动同步它们自己的TLB高速缓存，因为决定线性地址和物理地址之间映射何时不再有效的是内核，而不是硬件。

Linux 2.6提供了几种在合适时机应当运用的TLB刷新方法，这取决于页表更换的类型。

| 方法名称                   | 说明                      | 典型的应用时机        |
| ---------------------- | ----------------------- | -------------- |
| flush_tlb_all          | 刷新所有TLB表项               | 改变内核页表项时       |
| flush_tlb_kernel_range | 刷新给定线性地址范围内的所有TLB表项     | 更换一个范围内的内核页表项时 |
| flush_tlb              | 刷新当前进程拥有的非全局页相关的所有TLB表项 | 执行进程切换时        |
| flush_tlb_mm           | 刷新指定进程拥有的非全局页相关的所有TLB表项 | 创建一个新的子进程时     |
| flush_tlb_range        | 刷新指定进程的线性地址间隔对应的TLB表项   | 释放某个进程的线性地址间隔时 |
| flush_tlb_pgtables     | 刷新指定进程中特定的相临页表集相关的TLB表项 | 释放进程的一些页表时     |
| flush_tlb_page         | 刷新指定进程中单个页表项相关的TLB表项    | 处理缺页异常时        |

尽管普通Linux内核提供了丰富的TLB方法，但通常每个微处理器都提供了更受限制的一组使TLB无效的汇编语言指令。在这个方面，一个更为灵活的硬件平台就是Sun的U1traSPARC。与之相比，Intel微处理器只提供了两种使TLB无效的技术：

- 在向cr3寄存器写入值时所有Pentium处理器自动刷新相对于非全局页的TLB表项。
- 在Pentium Pro及以后的处理器中，invlpg汇编语言指令使映射指定线性地址的单个TLB表项无效。

下表列出了采用这种硬件技术的Linux宏；这些宏是实现独立于系统的方法（上表）的基本要素。

| 宏名称                      | 描述                                       | 使用对象                                   |
| ------------------------ | ---------------------------------------- | -------------------------------------- |
| __flush_tlb()            | 将cr3寄存器的当前值重新写回cr3                       | flush_tlb，flush_tlb_mm，flush_tlb_range |
| __flush_tlb_global()     | 通过清除cr4的PGE禁用全局页，将cr3寄存器的当前值重新写回cr3，并在此设置PGE标志 | flush_tlb_all，flush_tlb_kernel_range   |
| __flush_tlb_single(addr) | 以addr为参数执行invlpg汇编语言指令                   | flush_tlb_page                         |

上表中没有flush tlb_pgtables方法：在x86系统中，当页表与父页表解除链接时什么也不需要做，所以实现这个方法的函数为空。

独立于体系结构的使TLB无效的方法非常简单地扩展到了多处理器系统上。在一个CPU上运行的函数发送一个处理器间中断给其他的CPU来强制它们执行适当的函数使TLB无效。

一般来说，任何进程切换都会暗示着更换活动页表集。相对于过期页表，本地TLB表项必须被刷新；这个过程在内核把新的页全局目录的地址写入cr3控制寄存器时会自动完成。不过内核在下列情况下将避免TLB被刷新：

- 当两个使用相同页表集的普通进程之间执行进程切换时。
- 当在一个普通进程和一个内核线程间执行进程切换时。（内核线程无自己的页表集，使用上一个普通进程的页表集）

除了进程切换以外，还有其他几种情况下内核需要刷新TLB中的一些表项。例如，当内核为某个用户态进程分配页框并将它的物理地址存入页表项时，它必须刷新与相应线性地址对应的任何本地TLB表项。在多处理器系统中，如果有多个CPU在使用相同的页表集，那么内核还必须刷新这些CPU上使用相同页表集的TLB表项。

为了避免多处理器系统上无用的TLB刷新，内核使用一种叫做懒惰TLB (lazy TLB)模式的技术。其基本思想是，如果几个CPU正在使用相同的页表，而且必须对这些CPU上的一个TLB表项刷新，那么，在某些情况下，正在运行内核线程的那些CPU上的刷新就可以延迟。

事实上，每个内核线程并不拥有自己的页表集，它使用一个普通进程的的页表集。不过，没有必要使一个用户态线性地址对应的TLB表项无效，因为内核线程不访问内核态地址空间。（flush_tlb_all方法不使用懒惰TLB模式机制）

当某个CPU开始运行一个内核线程时，内核把它置为懒惰TLB模式。当发出清除TLB表项的请求时，处于懒惰TLB模式的每个CPU都不刷新相应的表项。但是，CPU记住它的当前进程正运行在一组页表上，而这组页表的TLB表项对用户态地址是无效的。只要处于懒惰TLB模式的CPU用一个不同的页表集切换到一个普通进程，硬件就自动刷新TLB表项，同时内核把CPU设置为非懒惰TLB模式。然而，如果处于懒惰TLB模式的CPU切换到的进程与刚才运行的内核线程拥有相同的页表集，那么，任何使TLB无效的延迟操作必须由内核有效地实施；这种使TLB无效的“懒惰”操作可以通过刷新CPU的所有非全局TLB项来有效地获取。

为了实现懒惰TLB模式，需要一些额外的数据结构。cpu_tlbstate变量是一个具有NR_CPUS个结构的静态数组，这个结构有两个字段，一个是指向当前进程内存描述符的active_ mm字段，一个是具有两个状态值的state字段：TLBSTATE_ OK(非懒惰TLB模式)或TLBSTATE_LAZY(懒惰TLB模式)。此外，每个内存描述符中包含一个cpu_vm_mask字段，该字段存放的是CPU(这些CPU将要接收与TLB刷新相关的处理器间中断)下标；只有当内存描述符属于当前运行的一个进程时这个字段才有意义。

当一个CPU开始执行内核线程时、内核把该CPU的cpu_tlbstate元素的state字段置为TLBSTATE_LAZY。此外，活动(active)内存描述符的cpu_vm_mask字段存放系统中所有CPU(包括进入懒惰TLB模式的CPU)的下标。对于与给定页表集相关的所有CPU的TLB表项，当另外一个CPU想使这些表项无效时，该CPU就把一个处理器间中断发送给下标处于对应内存描述符的cpu_vm_mask字段中的那些CPU。

当CPU接受到一个与TLB刷新相关的处理器间中断，并验证它影响了其当前进程的页表集时，它就检查它的cpu_tlbstate元素的state字段是否等于TLBSTATE_LAZY；如果等于，内核就拒绝使TLB表项无效，并从内存描述符的cpu_vm_mask字段删除该CPU下标。这有两种结果：

- 只要CPU还处于懒惰TLB模式，它将不接受其他与TLB刷新相关的处理器间中断。
- 如果CPU切换到另一个进程，而这个进程与刚被替换的内核线程使用相同的页表集，那么内核调用\_\_flush_tlb()使该CPU的所有非全局TLB表项无效。

##  第3章 进程

进程是任何多道程序设计操作系统中的基本概念。

### 进程、轻量级进程和线程

进程是程序执行的一个实例，是充分描述程序已经执行到何种程度的数据结构的汇集。

从内核观点看，进程的目的就是担当分配系统资源（CPU时间、内存等）的实体。

拥有很多相对独立执行流的用户程序共享应用程序的大部分数据结构。一个进程由几个用户线程(或简单地说，线程)组成，每个线程都代表进程的一个执行流。大部分多线程应用程序都是用pthread(POSIX thread)库的标准库函数集编写的。

Linux内核的早期版本没有提供多线程应用的支持。从内核观点看，多线程应用程序仅仅是一个普通进程。多线程应用程序多个执行流的创建、处理、调度整个都是在用户态进行的(通常使用POSIX兼容的pthread库)。

但是，这种多线程应用程序的实现方式不那么令人满意。例如，假设一个象棋程序使用两个线程：其中一个控制图形化棋盘，等待人类选手的移动并显示计算机的移动，而另一个思考棋的下一步移动。尽管第一个线程等待选手移动时，第二个线程应当继续运行，以此利用选手的思考时间。但是，如果象棋程序仅是一个单独的进程，第一个线程就不能简单地发出等待用户行为的阻塞系统调用。否则，第二个线程也被阻塞。相反，第一个线程必须使用复杂的非阻塞技术来确保进程仍然是可运行的。

Linux使用轻量级进程(lightwetght process)对多线程应用程序提供更好的支持。两个轻量级进程基本上可以共享一些资源，诸如地址空间、打开的文件等等。只要其中一个修改共享资源，另一个就立即查看这种修改。当然，当两个线程访问共享资源时就必须同步它们自己。

实现多线程应用程序的一个简单方式就是把轻量级进程与每个线程关联起来。这样，线程之间就可以通过简单地共享同一内存地址空间、同一打开文件集等来访问相同的应用程序数据结构集。同时，每个线程都可以由内核独立调度，以便一个睡眠的同时另一个仍然是可运行的。POSIX兼容的pthread库使用Linux轻量级进程有3个例子，它们是LinuxThreads， Native Posix Thread Library(NPTL)和IBM的下一代Posix线程包NGPT(Next Generation Posix Threading Package)。

POSIX兼容的多线程应用程序由支持“线程组”的内核来处理。在Linux中，一个线程组基本上就是实现了多线程应用的一组轻量级进程，对于像getpid()，kill()，和_exit()这样的一些系统调用，它像一个组织，起整体的作用。

### 进程描述符

为了管理进程，内核必须对每个进程所做的事情进行清楚的描述。例如，内核必须知道进程的优先级，它是正在CPU上运行还是因某些事件而被阻塞，给它分配了什么样的地址空间，允许它访问哪个文件等等。这正是进程描述符(process descriptor)的作用——进程描述符都是task_struct类型结构，它的字段包含了与一个进程相关的所有信息。它不仅包含了很多进程属性的字段，而且一些字段还包括了指向其他数据结构的指针，依此类推。

![Linux进程描述符.png](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/Linux%E8%BF%9B%E7%A8%8B%E6%8F%8F%E8%BF%B0%E7%AC%A6.png?raw=true)

#### 进程状态

进程描述符中的state字段描述了进程当前所处的状态。它由一组标志组成，其中每个标志描述一种可能的进程状态，且状态是互斥的。

| 标志                              | 描述                                       |
| ------------------------------- | ---------------------------------------- |
| 可运行状态(TASK_RUNNING)             | 进程要么在CPU上执行，要么准备执行。                      |
| 可中断的等待状态(TASK_INTERRUPTIBLE)    | 进程被挂起(睡眠)，直到某个条件变为真。产生一个硬件中断，释放进程正等待的系统资源，或传递一个信号都是可以唤醒进程的条件(把进程的状态放回到TASK_RUNNING。 |
| 不可中断的等待状态(TASK_UNINTERRUPTIBLE) | 与可中断的等待状态类似，但是，把信号传递到睡眠进程不能改变它的状态。这种状态在一些特定的情况下(进程必须等待，直到一个不能被中断的事件发生)是很有用的。例如，当进程打开一个设备文件，其相应的设备驱动程序开始探测相应的硬件设备时会用到这种状态。探测完成以前，设备驱动程序不能被中断，否则，硬件设备会处于不可预知的状态。 |
| 暂停状态(TASK_STOPPED)              | 进程的执行被暂停。当进程接收到SIGSTOP， SIGTSTP，  SIGTTIN或SIGTTOU信号后，进入暂停状态。 |
| 跟踪状态(TASK_TRACED)               | 进程的执行已由debugger程序暂停。当一个进程被另一个进程监控时(例如    debugger执行ptrace()系统调用监控一个测试程序)，任何信号都可以把这个进程置于TASK_TRACED状态。 |
| 僵死状态(EXIT_ZOMBIE)               | 可存放在exit_state字段中。进程的执行被终止，但是，父进程还没有发布wait4()或waitpid()系统调用来返回有关死亡进程的信息。发布wait()类系统调用前，内核不能丢弃包含在死进程描述符中的数据，因为父进程可能还需要它。 |
| 僵死撤消状态(EXIT_DEAD)               | 可存放在exit_state字段中。最终状态：由于父进程刚发出wait4()或waitpid()系统调用，因而进程由系统删除。为了防止其他执行线程在同一个进程上也执行wait()类系统调用(这是一种竞争条件)，而把进程的状态由僵死(EXIT_ZOMBIE)状态改为僵死撤消状态 (EXIT_DEAD)。 |

内核使用set_task_state和set_current_state宏分别设置指定进程的状态和当前执行进程的状态。此外，这些宏确保编译程序或CPU控制单元不把赋值操作与其他指令混合。混合指令的顺序有时会导致灾难性的后果。

#### 标识一个进程

能被独立调度的每个执行上下文都必须拥有它自己的进程描述符。因此，即使共享内核大部分数据结构的轻量级进程，也有它们自己的task_struct结构。

进程和进程描述符之间有非常严格的一一对应关系，这使得用32位进程描述符地址标识进程成为一种方便的方式。进程描述符指针指向这些地址，内核对进程的大部分引用是通过进程描述符指针进行的。

另一方面，类Unix操作系统允许用户使用一个叫做进程标识符process ID(或PID)的数来标识进程，PID存放在进程描述符的pid字段中。PID被顺序编号，新创建进程的PID通常是前一个进程的PID加1。不过，PID的值有一个上限，当内核使用的PID达到这个上限值的时候就必须开始循环使用已闲置的小PID号。系统管理员可以通过往/proc/sys/kernel/pid_max这个文件中写入一个值来更改PID的上限值。

由于循环使用PID编号，内核必须通过管理一个pidmap-array位图来表示当前已分配的PID号和闲置的PID号。因为一个页框包含32768个位，所以在32位体系结构中pidmap-array位图存放在一个单独的页中。然而，在64位体系结构中，当内核分配了超过当前位图大小的PID号时，需要为PID位图增加更多的页。系统会一直保存这些页不被释放。

Linux把不同的PID与系统中每个进程或轻量级进程相关联(多处理器系统上稍有例外)。这种方式能提供最大的灵活性，因为系统中每个执行上下文都可以被唯一地识别。

另一方面，Unix程序员希望同一组中的线程有共同的PID。例如，把指定PID的信号发送给组中的所有线程。事实上，POSIX 1003.1c标准规定一个多线程应用程序中的所有线程都必须有相同的PID。

遵照这个标准，Linux引入线程组的表示。一个线程组中的所有线程使用和该线程组的领头线程(thread group leader)相同的PID，也就是该组中第一个轻量级进程的PID，它被存入进程描述符的tgid字段中。getpid()系统调用（sys_getpid()）返回当前进程的tgid值而不是pid的值，因此，一个多线程应用的所有线程共享相同的PID。绝大多数进程都属于一个线程组，包含单一的成员。线程组的领头线程其tgid的值与pid的值相同，因而getpid()系统调用对这类进程所起的作用和一般进程是一样的。

从进程的PID中有效地导出它的描述符指针，效率至关重要，因为像kill()这样的很多系统调用使用PID表示所操作的进程。

#### 进程描述符处理

进程是动态实体，其生命周期范围从几毫秒到几个月。因此，内核必须能够同时处理很多进程，并把进程描述符存放在动态内存中，而不是放在永久分配给内核的内存区(线性地址在3GB之上)。

对每个进程来说，Linux都把两个不同的数据结构紧凑地存放在一个单独为进程分配的存储区域内（union thread_union ）：一个是内核态的进程堆栈，另一个是紧挨进程描述符的小数据结构thread_info，叫做线程描述符。这块存储区域的大小通常为8192个字节(两个页框)。考虑到效率的因素，内核让这8K空间占据连续的两个页框并让第一个页框的起始地址是2^13^的倍数。当几乎没有可用的动态内存空间时，就会很难找到这样的两个连续页框，因为空闲空间可能存在大量碎片。因此，在80x86体系结构中，在编译时可以进行设置，以使内核栈和线程描述符跨越一个单独的页框(4096个字节)。

在第二章“Linux中的分段”一节中我们已经知道，内核态的进程访问处于内核数据段的栈，这个栈不同于用户态的进程所用的栈。因为内核控制路径使用很少的栈，因此只需要几千个字节的内核态堆栈。所以，对栈和thread_info结构来说，8KB足够了。不过，当使用一个页框存放内核态堆栈和thread_info结构时，内核要采用一些额外的栈以防止中断和异常的深度嵌套而引起的溢出。

下图显示了在2页(8KB)内存区中存放两种数据结构的方式。线程描述符驻留于这个内存区的开始，而栈从末端向下增长。该图还显示了分别通过task和thread_info字段使thread_info结构与task_struct结构互相关联。

![进程内核栈.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E8%BF%9B%E7%A8%8B%E5%86%85%E6%A0%B8%E6%A0%88.jpg?raw=true)


esp寄存器是CPU栈指针，用来存放栈顶单元的地址。在80x86系统中，栈起始于末端，并朝这个内存区开始的方向增长。从用户态刚切换到内核态以后，进程的内核栈总是空的，因此，esp寄存器指向这个栈的顶端。

一旦数据写入堆栈，esp的值就递减。因为thread_info结构是52个字节长，因此内核栈能扩展到8140个字节。

C语言使用下列的联合结构方便地表示一个进程的线程描述符和内核栈：

```c
union thread_union {
	struct thread_info thread_info;
	unsigned long stack[THREAD_SIZE/sizeof(long)];/*2048 or 1024*/
};
```

如上图所示，thread_info结构从0x015f 0000地址处开始存放，而栈从0x015f c000地址处开始存放。esp寄存器的值指向地址为0x012f a878的当前栈顶。

内核使用alloc_thread_info和free_thread_info宏分配和释放存储thread_info结构和内核栈的内存区。

#### 标识当前进程

从效率的观点来看，thread_info结构与内核态堆栈之间的紧密结合提供的主要好处是：内核很容易从esp寄存器的值获得当前在CPU上正在运行进程的thread_ info结构的地址。如果thread_union结构长度是8K(2^13^字节)，则内核屏蔽掉esp的低13位有效位就可以获得thread_info结构的基地址。而如果thread_union结构长度是4K，内核需要屏蔽掉esp的低12位有效位。这项工作由current_thread_ info()函数来完成，它产生如下一些汇编指令：

```c
movl $0xffffe000， %ecx	/*或者是用于4K堆栈的Oxfffff000*/
andl %esp， %ecx
movl %ecx， p
```

这三条指令执行以后，p就包含在执行指令的CPU上运行的进程的thread_info结构的指针。

进程最常用的是进程描述符的地址而不是thread_info结构的地址。为了获得当前在CPU上运行进程的描述符指针，内核要调用current宏，该宏本质上等价于current_thread_info()->task，它产生如下汇编语言指令：

```c
movl $0xffffe000， %ecx	/*或者是用于4K堆栈的Oxfffff000*/
andl %esp，%ecx
movl (%ecx)，p
```

因为task字段在thread_info结构中的偏移量为0（体系结构相关），所以执行完这三条指令之后，p就包含在CPU上运行进程的描述符指针。

current宏经常作为进程描述符字段的前缀出现在内核代码中，例如，current->pid返回在CPU上正在执行的进程的PID。

用栈存放进程描述符的另一个优点体现在多处理器系统上：如前所述，对于每个硬件处理器，仅通过检查栈就可以获得当前正确的进程。早先的Linux版本没有把内核栈与进程描述符存放在一起，而是强制引入全局静态变量current来标识正在运行进程的描述符。在多处理器系统上，有必要把current定义为一个数组，每一个元素对应一个可用CPU。

#### 进程链表

每个task_struct结构都包含一个list_head类型的tasks字段，这个类型的prev和next字段分别指向前面和后面的task_struct元素。

进程链表的头是init_task描述符，它是所谓的0进程(process 0)或swapper进程的进程描述符(参见本章“内核线程”一节)。init_task的tasks.prev字段指向链表中最后插入的进程描述符的tasks字段。

宏 for_each_process，扫描整个进程链表，其定义如下：
```c
define next_task(p)	list_entry((p)->tasks.next， struct task_struct， tasks)
define prev_task(p)	list_entry((p)->tasks.prev， struct task_struct， tasks)
define for_each_process(p) \
	for (p = &init_task ; (p = next_task(p)) != &init_task ; )
```
这个宏是循环控制语句，内核开发者利用它提供循环。注意init_task进程描述符是如何起到链表头作用的。这个宏从指向init_task的指针开始，把指针移到下一个任务，然后继续，直到又到init_task为止。在每一次循环时，传递给这个宏的参变量中存放的是当前被扫描进程描述符的地址，这与list_entry宏的返回值一样。

#### TASK_RUNNING状态的进程链表

当内核寻找一个新进程在CPU上运行时，必须只考虑可运行进程(即处在TASK_RUNNING状态的进程)。

早先的Linux版本把所有的可运行进程都放在同一个叫做运行队列(runqueue)的链表中，由于维持链表中的进程按优先级排序开销过大，因此，早期的调度程序不得不为选择“最佳”可运行进程而扫描整个队列。

Linux2.6实现的运行队列有所不同。其目的是让调度程序能在固定的时间内选出“最佳”可运行进程，与队列中可运行的进程数无关。第七章会详细描述这种新的运行队列。

提高调度程序运行速度的诀窍是建立多个可运行进程链表，每种进程优先权对应一个不同的链表。每个task_struct描述符包含一个list_head类型的字段run_list（struct sched_rt_entity rt）。如果进程的优先权等于k(其取值范围是0到139)，run_list字段把该进程链人优先权为k的可运行进程的链表中。此外，在多处理器系统中，每个CPU都有它自己的运行队列，即它自己的进程链表集。这是一个通过使数据结构更复杂来改善性能的典型例子：调度程序的操作效率的确更高了，但运行队列的链表却为此而被拆分成140个不同的队列!

正如我们将看到的，内核必须为系统中每个运行队列保存大量的数据，不过运行队列的主要数据结构还是组成运行队列的进程描述符链表，所有这些链表都由一个单独的prio_array_t数据结构来实现，其字段说明如下表所示：

| 类型                     | 字段        | 描述                               |
| ---------------------- | --------- | -------------------------------- |
| int                    | nr_active | 链表中进程描述符的数量                      |
| unsigned long [5]      | bitmap    | 优先权位图：当且仅当某个优先权的进程链表不为空时设置相应的位标志 |
| struct list_head [140] | queue     | 140个优先权队列的头结点                    |

enqueue_task(p,array)函数把进程描述符插入某个运行队列的链表，其代码本质上等同于：

```c
static void enqueue_task(struct task_struct *p, prio_array_t *array)
{
	sched_info_queued(p);
	list_add_tail(&p->run_list, array->queue + p->prio);
	__set_bit(p->prio, array->bitmap);
	array->nr_active++;
	p->array = array;
}
```

进程描述符的prio字段存放进程的动态优先权，而array字段是一个指针，指向当前运行队列的prio_array_t数据结构。类似地，dequeue_task(p,array)函数从运行队列的链表中删除一个进程的描述符。

#### 进程间的关系

程序创建的进程具有父/子关系。如果一个进程创建多个子进程时，则子进程之间具有兄弟关系。在进程描述符中引入几个字段来表示这些关系，表示给定进程P的这些字段列在下表中。进程0和进程1是由内核创建的。稍后我们将看到，进程1  (init)是所有进程的祖先。

| 字段名         | 说明                                       |
| ----------- | ---------------------------------------- |
| real_parent | 指向创建了P的进程的描述符，如果P的父进程不再存在，就指向进程1 (init)的描述符(因此，如果用户运行一个后台进程而且退出了shell，后台进程就会成为init的子进程) |
| parent      | 指向P的当前父进程(这种进程的子进程终止时，必须向父进程发信号)。它的值通常与real_parent一致，但偶尔也可以不同，例如，当另一个进程发出监控P的ptrace()系统调用请求时(参见第二十章中“执行跟踪”一节) |
| children    | 链表的头部，链表中的所有元素都是P创建的子进程                  |
| sibling     | 指向兄弟进程链表中的下一个元素或前一个元素的指针，这些兄弟进程的父进程都是P   |

![五个进程间的亲属关系.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E4%BA%94%E4%B8%AA%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E4%BA%B2%E5%B1%9E%E5%85%B3%E7%B3%BB.jpg?raw=true)

上图显示了一组进程间的亲属关系。进程P0接连创建了P1,P2和P3。进程P3又创建了P4。

特别要说明的是，进程之间还存在其他关系:一个进程可能是一个进程组或登录会话的领头进程(参见第一章“进程管理”一节)，也可能是一个线程组的领头进程(参见本章前面“标识一个进程”一节)，它还可能跟踪其他进程的执行(参见第二十二章“执行跟踪”一节)。下表列出了进程描述符中的一些字段，这些字段建立起了进程P和其他进程之间的关系。

| 字段名             | 说明                                   |
| --------------- | ------------------------------------ |
| group_leader    | P所在进程组的领头进程的描述符指针                    |
| signal->pgrp    | P所在进程组的领头进程的PID                      |
| tgid            | P所在线程组的领头进程的PID                      |
| signal->session | P的登录会话领头进程的PID                       |
| ptrace_children | 链表的头，该链表包含所有被debugge程序跟踪的P的子进程       |
| ptrace_list     | 指向所跟踪进程其实际父进程链表的前一个和下一个元素(用于P被跟踪的时候) |

##### pidhash表及链表

在几种情况下，内核必须能从进程的PID导出对应的进程描述符指针。例如，为kill()系统调用提供服务时就会发生这种情况:当进程P1希望向另一个进程P2发送一个信号时，P1调用kill()系统调用，其参数为P2的PID，内核从这个PID导出其对应的进程描述符，然后从P2的进程描述符中取出记录挂起信号的数据结构指针。

顺序扫描进程链表并检查进程描述符的pid字段是可行但相当低效的。为了加速查找，引入了4个散列表。需要4个散列表是因为进程描述符包含了表示不同类型PID的字段(见下表)，而且每种类型的PID需要它自己的散列表。

| Hash表的类型     | 字段名     | 说明          |
| ------------ | ------- | ----------- |
| PIDTYPE_PID  | pid     | 进程的PID      |
| PIDTYPE_TGID | tgid    | 线程组领头进程的PID |
| PIDTYPE_PGID | pgrp    | 进程组领头进程的PID |
| PIDTYPE_SID  | session | 会话领头进程的PID  |

内核初始化期间动态地为4个散列表分配空间，并把它们的地址存入pid_hash数组。一个散列表的长度依赖于可用RAM的容量，例如：一个系统拥有512MB的RAM，那么每个散列表就被存在4个页框中，可以拥有2048个表项。

用pid_hashfn宏把PID转化为表索引，pid_hashfn宏展开为:

```c
#define pid_hashfn(nr) hash_long((unsigned long)nr, pidhash_shift)
```

变量pidhash_shift用来存放表索引的长度(以位为单位的长度，在我们的例子里是11位)。很多散列函数都使用hash_long()，在32位体系结构中它基本等价于：

```c
#define GOLDEN_RATIO_PRIME 0x9e370001UL
#define BITS_PER_LONG 32
static inline unsigned long hash_long(unsigned long val, unsigned int bits)
{
	unsigned long hash = val;
	/* On some cpus multiply is faster, on others gcc will do shifts */
	hash *= GOLDEN_RATIO_PRIME;
	/* High bits are more random, so use them. */
	return hash >> (BITS_PER_LONG - bits);
}
```

因为在我们的例子中pidhash_shift等于11，所以pid_hashfn的取值范围是0到2^11^-1=2047。

散列(hash)函数并不总能确保PID与表的索引一一对应。两个不同的PID散列(hash)到相同的表索引称为冲突(colliding)。

```
魔数常量：常量0x9e370001究竟是怎么得出的？这种散列函数是基于表索引乘以一个适当的大数，于是结果溢出，就把留在32位变量中的值作为模数操作的结果。Knuth建议，要得到满意的结果，这个大乘数就应当是接近黄金比例的2^32^的一个素数(32位是80x86寄存器的大小)。这里，0x9e370001就是接近2^32^的一个素数。
```

Linux利用链表来处理冲突的PID：每一个表项是由冲突的进程描述符组成的双向链表。

具有链表的散列法比从PID到表索引的线性转换更优越，这是因为在任何给定的实例中，系统中的进程数总是远远小于32768(所允许的进程PID的最大数)。如果在任何给定的实例中大部分表项都不使用的话，那么把表定义为32768项会是一种存储浪费。

由于需要跟踪进程间的关系，PID散列表中使用的数据结构非常复杂。看一个例子：假设内核必须回收一个指定线程组中的所有进程，这意味着这些进程的tgid的值是相同的，都等于一个给定值。如果根据线程组号查找散列表，只能返回一个进程描述符，就是线程组领头进程的描述符。为了能快速返回组中其他所有进程，内核就必须为每个线程组保留一个进程链表。在查找给定登录会话或进程组的进程时也会有同样的情形。

PID散列表的数据结构解决了所有这些难题，因为它们可以为包含在一个散列表中的任何PID号定义进程链表。最主要的数据结构是四个pid结构的数组，它在进程描述符的pids字段中，下表显示pid结构的字段。

| 类型                | 名称        | 描述              |
| ----------------- | --------- | --------------- |
| int               | nr        | pid的数值          |
| struct hlist_node | pid_chain | 链接散列表的下一个和前一个元素 |
| struct list_head  | pid_list  | 每个pid的进程链表头     |

![PID散列表.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/PID%E6%95%A3%E5%88%97%E8%A1%A8.jpg?raw=true)

上图给出了PIDTYPE_TGID类型散列表的例子。pid_hash数组的第二个元素存放散列表的地址，也就是用hlist_head结构的数组表示链表的头。在散列表第71项为起点形成的链表中，有两个PID号为246和4351的进程描述符(双箭头线表示一对向前和向后的指针)。PID的值存放在pid结构的nr字段中，而pid结构在进程描述符中。(顺便提一下，由于线程组的号和它的首创者的PID相同，因此这些PID值也存在进程描述符的pid字段中。)我们考虑线程组4351的PID链表:散列表中的进程描述符的pid_list字段中存放链表的头，同时每个PID链表中指向前一个元素和后一个元素的指针也存放在每个链表元素的pid_list字段中。

下面是处理PID散列表的函数和宏：

| 名称                                       | 描述                                       |
| ---------------------------------------- | ---------------------------------------- |
| do_each_task_pid(who, type, task)          while_each_task_pid(who, type, task) | 标记do-while循环的开始和结束，循环作用在PID值等于nr的PID链表上，链表的类型由参数type给出，task参数指向当前被扫描的元素的进程描述符。 |
| find_task_by_pid_type(int type, int nr)  | 在type类型的散列表中查找PID等于nr的进程。该函数返回所匹配的进程描述符指针，若没有匹配的进程，函数返回NULL。 |
| #define find_task_by_pid(nr)             | 与find_task_by_pid_type(int type, int nr)相同。 |
| attach_pid(task_t *task, enum pid_type type, int nr) | 把task指向的PID等于nr的进程描述符插人type类型的散列表中。如果一个PID等于nr的进程描述符已经在散列表中，这个函数就只把task插入已有的PID进程链表中。 |
| detach_pid(task_t *task, enum pid_type type) | 从type类型的PID进程链表中删除task所指向的进程描述符。如果删除后PID进程链表没有变为空，则函数终止，否则，该函数还要从type类型的散列表中删除进程描述符。最后，如果PID的值没有出现在任何其他的散列表中，为了这个值能够被反复使用，该函数还必须清除PID位图中的相应位。 |
| next_thread(const task_t *p)             | 返回PIDTYPE_TGID类型的散列表链表中task指示的下一个轻量级进程的进程描述符。由于散列链表是循环的，若应用于传统的进程，那么该宏返回进程本身的描述符地址。 |

#### 如何组织进程

运行队列链表把处于TASK_RUNNING状态的所有进程组织在一起。没有为处于TASK_STOPPED、EXIT_ZOMBIE或EXIT_DEAD状态的进程建立专门的链表。由于对处于暂停、僵死、死亡状态进程的访问比较简单，或者通过PID,或者通过特定父进程的子进程链表，所以不必对这三种状态进程分组。

##### 等待队列

等待队列在内核中有很多用途，尤其用在中断处理、进程同步及定时。进程必须经常等待某些事件的发生，例如，等待一个磁盘操作的终止，等待释放系统资源，或等待时间经过固定的间隔。等待队列实现了在事件上的条件等待：希望等待特定事件的进程把自己放进合适的等待队列，并放弃控制权。因此，等待队列表示一组睡眠的进程，当某一条件变为真时，由内核唤醒它们。

等待队列由双向链表实现，其元素包括指向进程描述符的指针。每个等待队列都有一个等待队列头(wait queue head)，等待队列头是一个类型为wait_queue_head_t的数据结构:

```c
struct __wait_queue_head {
	spinlock_t lock;
	struct list_head task_list;
};
typedef struct __wait_queue_head wait_queue_head_t;
```

因为等待队列是由中断处理程序和主要内核函数修改的，因此必须对其双向链表进行保护以免对其进行同时访问而导致不可预测的后果。同步是通过等待队列头中的lock自旋锁达到的。task_list字段是等待进程链表的头。

等待队列链表中的元素类型为wait_queue_t：

```c
struct __wait_queue {
	unsigned int flags;
#define WQ_FLAG_EXCLUSIVE	0x01
	struct task_struct * task;
	wait_queue_func_t func;
	struct list_head task_list;
};
typedef struct __wait_queue wait_queue_t;
```

等待队列链表中的每个元素代表一个睡眠进程，该进程等待某一事件的发生；它的描述符地址存放在task字段中。task_list字段中包含的是指针，由这个指针把一个元素链接到等待相同事件的进程链表中。

然而，要唤醒等待队列中所有睡眠的进程有时并不方便。例如，如果两个或多个进程正在等待互斥访问某一要释放的资源，仅唤醒等待队列中的一个进程才有意义。这个进程占有资源，而其他进程继续睡眠。(这就避免了所谓“雷鸣般兽群”问题，即唤醒多个进程只为了竟争一个资源，而这个资源只能有一个进程访问，结果是其他进程必须再次回去睡眠。)

因此，有两种睡眠进程：互斥进程(等待队列元素的flags字段为1)由内核有选择地唤醒，而非互斥进程(falgs值为0)总是由内核在事件发生时唤醒。等待访问临界资源的进程就是互斥进程的典型例子。等待相关事件的进程是非互斥的。例如，我们考虑等待磁盘传输结束的一组进程：一但磁盘传输完成，所有等待的进程都会被唤醒。正如我们将在下面所看到的那样，等待队列元素的fun。字段用来表示等待队列中睡眠进程应该用什么方式唤醒。

##### 等待队列操作

DECLARE_WAIT_QUEUE_HEAD(name)宏静态地声明一个叫name的等待队列的头变量并对该变量的lock和task_list字段进行初始化。函数init_waitqueue_head()可以初始化动态分配的等待队列的头变量。

函数init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)初始化wait_queue_t结构的变量q：
```c
q->flags = 0;
q->task = p;
q->func = default_wake_function;
```

非互斥进程p将由default_wake_function()唤醒：

```c
int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
{
	task_t *p = curr->task;
	return try_to_wake_up(p, mode, sync);	/*第七章讨论*/
}
```

也可以选择DEFINE_WAIT宏声明一个wait_queue_t类型的新变量，并用CPU上运行的当前进程的描述符和唤醒函数autoremove_wake_function()的地址初始化这个新变量。这个函数调用default_wake_function()来唤醒睡眠进程，然后从等待队列的链表中删除对应的元素(每个等待队列链表中的一个元素其实就是指向睡眠进程描述符的指针)。最后，内核开发者可以通过init_waitqueue_func_entry()函数来自定义唤醒函数，该函数负责初始化等待队列的元素。

一旦定义了一个元素，必须把它插人等待队列。add_wait_queue()函数把一个非互斥进程插入等待队列链表的第一个位置。add_wait_queue_exclusive()函数把一个互斥进程插入等待队列链表的最后一个位置。remove_wait_queue()函数从等待队列链表中删除一个进程。waitqueue_active()函数检查一个给定的等待队列是否为空。

要等待特定条件的进程可以调用如下列表中的任何一个函数：

- sleep_on()对当前进程进行操作：

  ```c
  #define	SLEEP_ON_VAR					\
  	unsigned long flags;				\
  	wait_queue_t wait;				\
  	init_waitqueue_entry(&wait, current);

  #define SLEEP_ON_HEAD					\
  	spin_lock_irqsave(&q->lock,flags);		\
  	__add_wait_queue(q, &wait);			\
  	spin_unlock(&q->lock);

  #define	SLEEP_ON_TAIL					\
  	spin_lock_irq(&q->lock);			\
  	__remove_wait_queue(q, &wait);			\
  	spin_unlock_irqrestore(&q->lock, flags);

  void fastcall __sched sleep_on(wait_queue_head_t *q)
  {
  	SLEEP_ON_VAR

  	current->state = TASK_UNINTERRUPTIBLE;

  	SLEEP_ON_HEAD
  	schedule();
  	SLEEP_ON_TAIL
  }
  ```

  该函数把当前进程的状态设置为TASK_UNINTERRUPTIBLE，并把它插入到特定的等待队列。然后，它调用调度程序，而调度程序重新开始另一个程序的执行。当睡眠进程被唤醒时，调度程序重新开始执行sleep_on()函数，把该进程从等待队列中删除。

- interruptible_sleep_on()函数sleep_on()函数是一样的，但此函数把当前进程状态设置为TASK_INTERRUPTIBLE，因此，接受一个信号就可以唤醒当前进程。

  ```c
  void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
  {
  	SLEEP_ON_VAR

  	current->state = TASK_INTERRUPTIBLE;

  	SLEEP_ON_HEAD
  	schedule();
  	SLEEP_ON_TAIL
  }
  ```


- sleep_on_timeout()和interruptible_sleep_on timeout()与前面函数类似，但它们允许调用者定义一个时间间隔，过了这个间隔以后，进程将由内核唤醒。为了做到这点，它们调用schedule_timeout()函数而不是schedule()函数(参见第六章中“动态定时器的应用”一节)。


- 在Linux 2.6中引入的prepare_to_wait(), prepare_to_wait_exclusive()和
  finish_wait()函数提供了另外一种途径来使当前进程在一个等待队列中睡眠。它们的典型应用如下：

  ```c
  DEFINE_WAIT(wait);	   /*wq是等待队列的头*/
  prepare_to_wait(&wq, &wait，TASK_INTERRUPTIBLE);
  ...
  if(!condition)
  	schedule();
  finish_wait(&wq,&wait)
  ```
  函数prepare_to_wait()和prepare_to_wait_ exclusive()用传递的第三个参数设置进程的状态，然后把等待队列元素的互斥标志flag分别设置为0(非互斥)或1(互斥)，最后，把等待元素wait插人到以wq为头的等待队列的链表中。

  进程一但被唤醒就执行finish_wait()函数，它把进程的状态再次设置为TASK RUNNING(仅发生在调用schedule()之前，唤醒条件变为真的情况下)，并从等待队列中删除等待元素(除非这个工作已经由唤醒函数完成)。

- wait_event和wait_event_interruptible宏使它们的调用进程在等待队列上睡眠，一直到修改了给定条件为止。例如，宏wait_event(wq,condition)本质上实现下面的功能：

  ```c
  #define __wait_event(wq, condition) 					\
  do {									\
  	DEFINE_WAIT(__wait);						\
  									\
  	for (;;) {							\
  		prepare_to_wait(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
  		if (condition)						\
  			break;						\
  		schedule();						\
  	}								\
  	finish_wait(&wq, &__wait);					\
  } while (0)

  #define wait_event(wq, condition) 					\
  do {									\
  	if (condition)	 						\
  		break;							\
  	__wait_event(wq, condition);					\
  } while (0)
  ```

对上面列出的函数做一些说明：sleep_on()类函数在以下条件下不能使用，那就是必须测试条件并且当条件还没有得到验证时又紧接着让进程去睡眠；由于那些条件是众所周知的竞争条件产生的根源，所以不鼓励这样使用。此外，为了把一个互斥进程插人等待队列，内核必须使用prepare_to_wait_exclusive()函数[或者只是直接调用add_wait_queue_exclusive()]。所有其他的相关函数把进程当作非互斥进程来插人。最后，除非使用DEFINE_WAIT或finish_wait()，否则内核必须在唤醒等待进程后从等待队列中删除对应的等待队列元素。

内核通过下面的任何一个宏唤醒等待队列中的进程并把它们的状态置为TASK_RUNNING：
wake_up、wake_up_nr、wake_up_all、wake_up_interruptible、wake_up_interruptible_nr、wake_up_interruptible_all、
wake_up_interruptible_sync和wake_up_locked。从每个宏的名字我们可以明白其功能：

- 所有宏都考虑到处于TASK_INTERRUPTIBLE状态的睡眠进程；如果宏的名字中不含字符串”interruptible"，那么处于TASK_UNINTERRUPTIBLE状态的睡眠进程也被考虑到。
- 所有宏都唤醒具有请求状态的所有非互斥进程(参见上一项)。
- 名字中含有“nr”字符串的宏唤醒给定数的具有请求状态的互斥进程;这个数字是宏的一个参数。名字中含有“all”字符串的宏唤醒具有请求状态的所有互斥进程。最后，名字中不含“nr”或“all”字符串的宏只唤醒具有请求状态的一个互斥进程。
- 名字中不含有“sync”字符串的宏检查被唤醒进程的优先级是否高于系统中正在运行进程的优先级，并在必要时调用schedule()。这些检查并不是由名字中含有“sync”字符串的宏进行的，造成的结果是高优先级进程的执行稍有延迟。
- wake_up_locked宏和wake_up宏相类似，仅有的不同是当wait_queue_head_t中的自旋锁已经被持有时要调用wake_up_locked。

例如，wake_up宏等价于下列代码片段：

```c
void wake_up(wait_queue_head_t *q)
{
	struct list_head *tmp;
	wait_queue_t *curr;
	list_for_each(tmp, &q->task_list){
		curr=list_entry(tmp, wait_queue_t，task_list)
		if (curr->func(curr, TASK_INTERRUPTIBLE|TASK_UNINTERRUPTIBLE, 0, NULL) && curr->flags)
			break;
	}
}
```
list_for_each宏扫描双向链表q->task_list中的所有项，即等待队列中的所有进程。对每一项，list_entry宏都计算wait_queue_t变量对应的地址。这个变量的func字段存放唤醒函数的地址，它试图唤醒由等待队列元素的task字段标识的进程。如果一个进程已经被有效地唤醒(函数返回1)并且进程是互斥的(curr->flags等于1)，循环结束。

所有的非互斥进程总是在双向链表的开始位置，而所有的互斥进程在双向链表的尾部，所以函数总是先唤醒非互斥进程然后再唤醒互斥进程，如果有进程存在的话(一个队列同时包含互斥和非互斥进程的情况是非常罕见的)。

#### 进程资源限制

每个进程都有一组相关的资源限制(resource limit)，限制指定了进程能使用的系统资源数量。这些限制避免用户过分使用系统资源(CPU、磁盘空间等)。

对当前进程的资源限制存放在current->signal->rlim字段，即进程的信号描述符的一个字段(参见第十一章“与信号相关的数据结构”一节)。该字段是类型为rlimit结构的数组，每个资源限制对应一个元素：

```c
struct rlimit {
	unsigned long	rlim_cur;
	unsigned long	rlim_max;
};
```

| 字段名               | 说明                                       |
| ----------------- | ---------------------------------------- |
| RLIMIT_CPU (0)    | 进程使用CPU的最长时间(以秒为单位)。如果进程超过了这个限制，内核就向它发一个SIGXCPU信号，然后如果进程还不终止，再发一个SIGKILL信号(参见第十一章) |
| RLIMIT_FSIZE      | 文件大小的最大值(以字节为单位)。如果进程试图把一个文件的大小扩充到大于这个值，内核就给这个进程发SIGXFSZ信号 |
| RLIMIT_DATA       | 堆大小的最大值(以字节为单位)。在扩充进程的堆之前，内核检查这个值(参见第九章中“堆的管理”一节) |
| RLIMIT_STACK      | 栈大小的最大值(以字节为单位)。内核在扩充进程的用户态堆栈之前检查这个值(参见第九章“异常处理”一节) |
| RLIMIT_CORE       | 内存信息转储文件的大小(以字节为单位)。当一个进程异常终止时，内核在进程的当前目录下创建内存信息转储文件之前检查这个值(参见第十一章的“传递信号之前所执行的操作”一节)。如果这个限制为0，那么，内核就不创建这个文件 |
| RLIMIT_RSS (5)    | 进程所拥有的页框最大数(目前是非强制的)                     |
| RLIMIT_NPROC      | 用户能拥有的进程最大数(参见本章“clone(), fork()及vfork()系统调用”一节) |
| RLIMIT_NOFILE     | 打开文件描述符的最大数。当打开一个新文件或复制一个文件描述符时，内核检查这个值(参见第十二章) |
| RLIMIT_MEMLOCK    | 非交换内存的最大值(以字节为单位)。当进程试图通过mlock()或mlockall()系统调用锁住一个页框时，内核检查这个值(参见第九章“分配线性地址区间”一节) |
| RLIMIT_AS         | 进程地址空间的最大数(以字节为单位)。当进程使用malloc()或相关函数扩大它的地址空间时，内核检查这个值(参见第九章“进程的地址空间”一节) |
| RLIMIT_LOCKS (10) | 文件锁的最大值(目前是非强制的)                         |
| RLIMIT_SIGPENDING | 进程挂起信号的最大数(参见第十一章)                       |
| RLIMIT_MSGQUEUE   | POSIX消息队列中的最大字节数(参见第十九章“POSIX消息队列”一节)    |
| RLIMIT_NICE       |                                          |
| RLIMIT_RTPRIO     | 最大实时优先级                                  |

rlim_cur 表示资源的当前限制，例如 current->signal->rlim[RLIMIT_CPU]，rlim_cur表示正运行进程所占用CPU时间的当前限制。

rlim_max字段是资源限制所允许的最大值。利用getrlimit()和setrlimit()系统调用，用户总能把一些资源的rlim_cur限制增加到rlim_max。然而，只有超级用户(或更确切地说，具有CAP_SYS_RESOURCE权能的用户)才能改变rlim_max字段，或把rlim_cur字段设置成大于相应rlim_max字段的一个值。

大多数资源限制包含值RLIMIT_INFINITY(0xffffffff)，它意味着没有对相应的资源施加用户限制(当然，由于内核设计上的限制，可用RAM、可用磁盘空间等，实际的限制还是存在的)。然而，系统管理员可以给一些资源选择施加更强的限制。只要用户注册进系统，内核就创建一个由超级用户拥有的进程，超级用户能调用setrlimit()以减少一个资源rlim_max和rlim_cur字段的值。随后，同一进程执行一个login shell, 该进程就变为由用户拥有。由用户创建的每个新进程都继承其父进程rlim数组的内容，因此，用户不能忽略系统强加的限制。

### 进程切换

为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换(process switch)、任务切换(task switch)或上下文切换(context switch)。

#### 硬件上下文

尽管每个进程可以拥有属于自己的地址空间，但所有进程必须共享CPU寄存器。因此，在恢复一个进程的执行之前，内核必须确保每个寄存器装入了挂起进程时的值。

进程恢复执行前必须装入寄存器的一组数据称为硬件上下文(hardware context)。硬件上下文是进程可执行上下文的一个子集，因为可执行上下文包含进程执行时需要的所有信息。在Linux中，进程硬件上下文的一部分存放在TSS段，而剩余部分存放在内核态堆栈中。

在下面的描述中，我们假定用prev局部变量表示切换出的进程的描述符，next表示切换进的进程的描述符。因此，我们把进程切换定义为这样的行为：保存prev硬件上下文，用next硬件上下文代替prev。因为进程切换经常发生，因此减少保存和装入硬件上下文所花费的时间是非常重要的。

早期的Linux版本利用80x86体系结构所提供的硬件支持，并通过far jmp指令(far jmp 指令既修改cs寄存器，也修改eip寄存器，而简单的jmp指令只修改eip寄存器)跳到next进程TSS描述符的选择符来执行进程切换。当执行这条指令时，CPU通过自动保存原来的硬件上下文，装人新的硬件上下文来执行硬件上下文切换。但基于以下原因，Linux 2.6使用软件执行进程切换：

- 通过一组mov指令逐步执行切换，这样能较好地控制所装入数据的合法性。尤其是，这使检查ds和es段寄存器的值成为可能，这些值有可能被恶意伪造。当用单独的far jmp指令时，不可能进行这类检查。


- 旧方法和新方法所需时间大致相同。然而，尽管当前的切换代码还有改进的余地，却不能对硬件上下文切换进行优化。

进程切换只发生在内核态。在执行进程切换之前，用户态进程使用的所有寄存器内容都已保存在内核态堆栈上(参见第四章)，这也包括ss和esp这对寄存器的内容(存储用户态堆栈指针的地址)。

#### 任务状态段

x86体系结构包括了一个特殊的段类型，叫任务状态段(Task State Segment ,TSS)来存放硬件上下文。尽管Linux并不使用硬件上下文切换，但是强制它为系统中每个不同的CPU创建一个TSS。这样做的两个主要理由为：

- 当x86的一个CPU从用户态切换到内核态时，它就从TSS中获取内核态堆栈的地址(参见第四章“中断和异常的硬件处理”一节和第十章“通过sysenter指令发送系统调用”一节)。


- 当用户态进程试图通过in或out指令访问一个I/O端口时，CPU需要访问存放在TSS中的I/O许可权位图(Permission Bitmap)以检查该进程是否有访问端口的权力。

  更确切地说，当进程在用户态下执行in或out指令时，控制单元执行下列操作：

  1. 它检查eflags寄存器中的2位IOPL字段。如果该字段值为3，控制单元就执行I/O指令。否则，执行下一个检查。
  2. 访问tr寄存器以确定当前的TSS和相应的I/O许可权位图。
  3. 检查I/O指令中指定的I/O端口在I/O许可权位图中对应的位。如果该位清0，这条I/O指令就执行，否贝控制单元产生一个“General protection”异常。

tss_struct结构描述TSS的格式。正如第二章所提到的，init_tss数组为系统上每个不同的CPU存放一个TSS。在每次进程切换时，内核都更新TSS的某些字段以便相应的CPU控制单元可以安全地检索到它需要的信息。因此，TSS反映了CPU上的当前进程的特权级，但不必为没有在运行的进程保留TSS。

```c
Processor.h (include\asm-i386)
struct tss_struct {
	unsigned short	back_link,__blh;
	unsigned long	esp0;
	unsigned short	ss0,__ss0h;
	unsigned long	esp1;
	unsigned short	ss1,__ss1h;	/* ss1 is used to cache MSR_IA32_SYSENTER_CS */
	unsigned long	esp2;
	unsigned short	ss2,__ss2h;
	unsigned long	__cr3;
	unsigned long	eip;
	unsigned long	eflags;
	unsigned long	eax,ecx,edx,ebx;
	unsigned long	esp;
	unsigned long	ebp;
	unsigned long	esi;
	unsigned long	edi;
	unsigned short	es, __esh;
	unsigned short	cs, __csh;
	unsigned short	ss, __ssh;
	unsigned short	ds, __dsh;
	unsigned short	fs, __fsh;
	unsigned short	gs, __gsh;
	unsigned short	ldt, __ldth;
	unsigned short	trace, io_bitmap_base;
	/*
	 * The extra 1 is there because the CPU will access an
	 * additional byte beyond the end of the IO permission
	 * bitmap. The extra byte must be all 1 bits, and must
	 * be within the limit.
	 */
	unsigned long	io_bitmap[IO_BITMAP_LONGS + 1];
	/*
	 * Cache the current maximum and the last task that used the bitmap:
	 */
	unsigned long io_bitmap_max;
	struct thread_struct *io_bitmap_owner;
	/*
	 * pads the TSS to be cacheline-aligned (size is 0x100)
	 */
	unsigned long __cacheline_filler[35];
	/*
	 * .. and then another 0x100 bytes for emergency kernel stack
	 */
	unsigned long stack[64];
} __attribute__((packed));
```

每个TSS有它自己8字节的任务状态段描述符(Task State Segment Descriptor, TSSD )。这个描述符包括指向TSS起始地址的32位Base字段，20位Limit字段。TSSD的S标志位被清0，以表示相应的TSS是系统段的事实(参见第二章“段描述符”一节)。

Type字段置为11或9以表示这个段实际上是一个TSS。在Intel的原始设计中，系统中的每个进程都应当指向自己的TSS；Type字段的第二个有效位叫做Busy位，如果进程正由CPU执行，则该位置1，否则置0。在Linux的设计中，每个CPU只有一个TSS,因此，Busy位总置为1。

由Linux创建的TSSD存放在全局描述符表(GDT)中，GDT的基地址存放在每个CPU的gdtr寄存器中。每个CPU的tr寄存器包含相应TSS的TSSD选择符，也包含了两个隐藏的非编程字段:TSSD的Base字段和Limit字段。这样，处理器就能直接对TSS寻址而不用从GDT中检索TSS的地址。

##### thread字段

在每次进程切换时，被替换进程的硬件上下文必须保存在别处。不能像Intel原始设计那样把它保存在TSS中，因为Linux为每个处理器而不是为每个进程使用TSS。

因此，每个进程描述符包含一个类型为thread_struct的thread字段，只要进程被切换出去，内核就把其硬件上下文保存在这个结构中。随后我们会看到，这个数据结构包含的字段涉及大部分CPU寄存器，但不包括诸如eax,ebx等等这些通用寄存器，它们的值保留在内核堆栈中。

#### 执行进程切换

进程切换可能只发生在精心定义的点：schedule()函数(在第七章会用很长的篇幅来讨论)。这里，我们仅关注内核如何执行一个进程切换。

从本质上说，每个进程切换由两步组成：

1. 切换页全局目录以安装一个新的地址空间（在第九章描述这一步）。
2. 切换内核态堆栈和硬件上下文，因为硬件上下文提供了内核执行新进程所需要的所有信息，包含CPU寄存器。

##### switch_to宏

进程切换的第二步由switch_to宏执行。它是内核中与硬件关系最密切的例程之一，首先，该宏有三个参数，它们是prev，next和last。prev和next是局部变量prev和next的占位符，即它们是输入参数，分别表示被替换进程和新进程描述符的地址在内存中的位置。

那第三个参数last呢？在任何进程切换中，涉及到三个进程而不是两个。假设内核决定暂停进程A而激活进程B。在schedule()函数中，prev指向A的描述符而next指向B的描述符。switch_to宏一但使A暂停，A的执行流就冻结。

随后，当内核想再次此激活A，就必须暂停另一个进程C(这通常不同于B)，于是就要用prev指向C而next指向A来执行另一个switch_to宏。当A恢复它的执行流时，就会找到它原来的内核栈，于是prev局部变量还是指向A的描述符而next指向B的描述符。此时，代表进程A执行的内核就失去了对C的任何引用。但是，事实表明这个引用对于完成进程切换是很有用的(更多细节参见第七章)。

switch_to宏的最后一个参数是输出参数，它表示宏把进程C的描述符地址写在内存的什么位置了(这是在A恢复执行之后完成的)。在进程切换之前，宏把第一个输入参数prev(即在A的内核堆栈中分配的prev局部变量)表示的变量的内容存人CPU的eax寄存器。在完成进程切换，A已经恢复执行时，宏把CPU的eax寄存器的内容写入由第三个输出参数——last所指示的A在内存中的位置。因为CPU寄存器不会在切换点发生变化，所以C的描述符地址也存在内存的这个位置。在schedule()执行过程中，参数last指向A的局部变量prev，所以prev被C的地址覆盖。

下图显示了进程A,B,C内核堆栈的内容以及eax寄存器的内容。必须注意的是：图中显示的是在被eax寄存器的内容覆盖以前的prev局部变量的值。

![通过一个进程切换保留对进程C的引用.jpg](https://github.com/LiuChengqian90/Study-notes/blob/master/image/Linux/%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%88%87%E6%8D%A2%E4%BF%9D%E7%95%99%E5%AF%B9%E8%BF%9B%E7%A8%8BC%E7%9A%84%E5%BC%95%E7%94%A8.jpg?raw=true)

由于switch_to宏采用扩展的内联汇编语言编码，所以可读性比较差：实际上这段代码通过特殊位置记数法使用寄存器，而实际使用的通用寄存器由编译器自由选择。我们将采用标准汇编语言而不是麻烦的内联汇编语言来描述switch_to宏在x86微处理器上所完成的典型工作。

1. 在eax和edx寄存器中分别保存prev和next的值：

   ```c
   movl prev, %eax
   movl next, %edx
   ```

2. 把eflags和ebp寄存器的内容保存在prev内核栈中。必须保存它们的原因是编译器认为在switch_to结束之前它们的值应当保持不变。

   ```c
   pushfl
   pushl %ebp
   ```

3. 把esp的内容保存到prev->thread.esp中以使该字段指向prev内核栈的栈顶：

   ```c
   movl %esp, 484(%eax)
   ```
   484(%eax)操作数表示内存单元的地址为eax内容加上484。

4. 把next->thread.esp装人esp。此时，内核开始在next的内核栈上操作，因此这条指令实际上完成了从prev到next的切换。由于进程描述符的地址和内核栈的地址紧挨着，所以改变内核栈意味着改变当前进程。
    ```c
    movl 484(%edx)，%esp
    ```

5. 把标记为1的地址(本节后面所示)存入prev->thread.eip。当被替换的进程重新恢复执行时，进程执行被标记为1的那条指令：
  ```c
  movl $lf, 480(%eax)
  ```

6. 宏把next->thread.eip的值(绝大多数情况下是一个被标记为1的地址)压入next的内核栈：
  ```c
  pushl 480(%edx)
  ```

7. 跳到\_\_switch_to() C函数：
  ```c
  jmp __switch_to
  ```

8. 这里被进程B替换的进程A再次获得CPU:它执行一些保存eflags和ebp寄存器内容的指令，这两条指令的第一条指令被标记为1。
  ```c
  1:
  	popl %%ebp
  	popfl
  ```

  注意这些pop指令是怎样引用prev进程的内核栈的。当进程调度程序选择了prev作为新进程在CPU上运行时，将执行这些指令。于是，以prev作为第二个参数调用switch_to。因此，esp寄存器指向prev的内核栈。

9. 拷贝eax寄存器(上面步骤1中被装载)的内容到switch_to宏的第三个参数last标识的内存区域中 

  ```c
  movl %eax, last
  ```

  正如先前讨论的，eax寄存器指向刚被替换的进程的描述符(当前执行的schedule()函数重新使用了prev局部变量，于是汇编语言指令就是：movl %eax, prev)。


##### \_\_switch_to()函数

\_\_switch_to()函数执行大多数开始于switch_to()宏的进程切换。这个函数作用于prev_p和next_p参数，分别表示前一个进程和新进程。这个函数的调用不同于一般函数的调用，因为\_\_switch_to()从eax和edx取参数prev_p和next_p(fastcall，ecx、edx，我们在前面已看到这些参数就是保存在哪里)，而不像大多数函数一样从栈中取参数。为了强迫函数从寄存器取它的参数，内核利用\_\_attribute\_\_和regparm关键字，这两个关键字是C语言非标准的扩展名，由gcc编译程序实现。在include/asm-1386/system.h头文件中，\_\_switch_to()函数的声明如下：

	__switch_to(struct task_struct *prev, struct task_struct *next}
	__attribute__(regparm(3));

函数执行的步骤如下：

1. 执行由\_\_unlazy_fpu()宏产生的代码(参见本章稍后“保存和加载FPU, MMX及XMM寄存器”一节)，以有选择地保存prev_p进程的FPU, MMX及XMM寄存器的内容。

      ```c
      __unlazy_fpu(prev_p);
      ```

2. 执行smp_processor_id()宏获得本地(local)CPU的下标，即执行代码的CPU。该宏从当前进程的thread_info结构的cpu字段获得下标并将它保存到cpu局部变量。

3. 把next->thread.esp0装入对应于本地CPU的TSS的esp0字段；将在第十章的“通过sysenter指令发生系统调用”一节看到，以后任何由sysenter汇编指令产生的从用户态到内核态的特权级转换将把这个地址拷贝到esp寄存器中：

      ```c
      init_tss[cpu].esp0=next->thread.esp0;
      ```

4. 把next_p进程使用的线程局部存储(TLS)段装入本地CPU的全局描述符表；三个段选择符保存在进程描述符内的tls_array数组中(参见第二章的“Linu、中的分段”一节)。

  ```c
  cpu_gdt_table[cpu}[6] = next->thread.tls_array[0];
  cpu_gdt_table[cpu][7] = next->thread.tls_array[1];
  cpu_gdt_table[cpu][8] = next->thread.tls_array[2];
  ```

5. 把fs和gs段寄存器的内容分别存放在prev_p->thread.fs和prev_p->thread.gs中，对应的汇编语言指令是：

  ```C
  movl %fs，40(%esi)
  movl %gs，44(%esi)
  ```

  esi寄存器指向prev_p->thread结构。

6. 如果fs或gs段寄存器已经被prev_p或next_p进程中的任意一个使用(也就是说如果它们有一个非0的值)，则将next_p进程的thread_struct描述符中保存的值装入这些寄存器中。这一步在逻辑上补充了前一步中执行的操作。主要的汇编语言指令如下：

      ```c
       movl 40(%ebx)，%fs
       movl 44(%ebx)，%gs
      ```

       ebx寄存器指向next_p->thread结构。代码实际上更复杂，因为当它检测到一个无效的段寄存器值时，CPU可能产生一个异常。代码采用一种“修正(fix-up)”途径来考虑这种可能性(参见第十章“动态地址检查:修正代码”一节)。

7. 用next_p->thread.debugreg数组的内容装载dr0，…，dr7中的6个调试寄存器(x86调试器允许进程被硬件监控。最多可定义4个断点区域)。只有在next_p被挂起时正在使用调试寄存器(也就是说，next_p->thread.debugreg[7]字段不为0)，这种操作才能进行。这些寄存器不需要被保存，因为只有当一个调试器想要监控prev时prev_p->thread.debugreg才会被修改。

      ```c
      if  (next_p->thread.debugreg[7]){
      	loaddebug(&next_p->thread, 0);
        	loaddebug(&next_p->thread, 1);
        	loaddebug(&next_p->thread, 2);
        	loaddebug(&next_p->thread, 3);
        	/*没有4和5*/
        	loaddebug(&next_p->thread, 6);
        	loaddebug(&next_p->thread, 7);
      }
      ```

8. 如果必要，更新TSS中的I/O位图。当next_p或prev_p有其自己的定制I/O权限位图时必须这么做：

  ```c
  	if (prev}一>thread.io_bitmap_ptr || next_p->thread.io_ bitmap_ptr)
  	handle_io_bitmap(&next_p->thread, &init_tss[cpu]);
  ```

  因为进程很少修改I/O权限位图，所以该位图在“懒”模式中被处理：当且仅当一个进程在当前时间片内实际访问I/O端口时，真实位图才被拷贝到本地CPU的TSS中。进程的定制I/O权限位图被保存在thread_info结构的io_bitmap_ptr字段指向的缓冲区中。handle_io_bitmap()函数为next_p进程设置本地CPU使用的TSS的io_bitmap字段如下：

  - 如果next_p进程不拥有自己的I/O权限位图，则TSS的io_bitmap字段被设为0x8000。


  - 如果next_p进程拥有自己的I/O权限位图，则TSS的io_bitmap字段被设为0x9000。

    TSS的io_bitmap字段应当包含一个在TSS中的偏移量，其中存放实际位图。无论何时用户态进程试图访问一个1/O端口，0x8000和0x9000指向TSS界限之外并将因此引起“General protection”异常(参见第四章的“异常”一节)。

    do_general_protection()异常处理程序将检查保存在io_bitmap字段的值;如果是0x8000,函数发送一个SIGSEGV信号给用户态进程;如果是0x9000,函数把进程位图(由thread_info结构中的io_bitmap_ptr字段指示)拷贝到本地CPU的TSS中，把io_bitmap字段设为实际位图的偏移(104)，并强制再一次执行有缺陷的汇编语言指令。

9. 终止。\_\_switch_to() C函数通过使用下列声明结束：


```c
 return prev_p;
```

​	由编译器产生的相应汇编语言指令是：  	

```c
movl %edi, %eax
ret
```

​	prev_p参数(现在在edi中)被拷贝到eax，因为缺省情况下任何C函数的返回值被传递给eax寄存器。注意eax的值因此在调用\_\_switch_to()的过程中被保护起来;这非常重要，因为调用switch_to宏时会假定eax总是用来存放将被替换的进程描述符的地址。

​	汇编语言指令ret把栈顶保存的返回地址装人eip程序计数器。不过，通过简单地跳转到\_\_switch_to()函数来调用该函数。因此，ret汇编指令在栈中找到标号为1的指令的地址，其中标号为1的地址是由switch_to宏推入栈中的。如果因为next第一次执行而以前从未被挂起，\_\_switch_to()就找到ret_from_fork()函数的起始地址(参见本章后面“clone(),fork()和vfork()系统调用一节”)。

#### 保存和加载FPU、MMX和XMM寄存器

从Intel 80486DX开始，算术浮点单元(floating-point unit，FPU)已被集成到CPU中。数学协处理这个名词使人想起使用昂贵的专用芯片执行浮点计算的岁月。然而，为了维持与旧模式的兼容，浮点算术函数用ESCAPE指令来执行，这个指令的一些前缀字节在0xd8和0xdf之间。这些指令作用于包含在CPU中的浮点寄存器集。显然，如果一个进程正在使用ESCAPE指令，那么，浮点寄存器的内容就属于它的硬件上下文，并且应该被保存。

在最近的Pentium模型中，Intel在它的微处理器中引入一个新的汇编指令集，叫做MMX指令，用来加速多媒体应用程序的执行。MMX指令作用于FPU的浮点寄存器。选择这种体系结构的明显缺点是编程者不能把浮点指令与MMX指令混在一起使用。优点是操作系统设计者能忽视新指令集，因为保存浮点单元状态的任务切换代码可以不加修改地应用到保存MMX状态。

MMX指令加速了多媒体应用程序的执行，因为它们在处理器内部引入了单指令多数据(single-instruction multiple-data，SIMD )流水线。Pentium III模型扩展了这种SIMD能力：它引入SSE扩展(Streaming SIMD Extensions)，该扩展为处理包含在8个128位寄存器(叫做XMM寄存器)的浮点值增加了功能。这样的寄存器不与FPU和MMX寄存器重叠，因此SSE和FPU/MMX指令可以随意地混合。Pentium 4模型指令还引入另一种特点:SSE2扩展，该扩展基本上是SSE的一个扩展，支持高精度浮点值。SSE2与SSE使用同一XMM寄存器集。

80x86微处理器并不在TSS中自动保存FPU、MMX和XMM寄存器。不过，它们包含某种硬件支持，能在需要时保存这些寄存器的值。硬件支持由cr0寄存器中的一个TS(Task-Switching)标志组成，遵循以下规则：

- 每当执行硬件上下文切换时，设置TS标志。

- 每当TS标志被设置时执行ESCAPE,  MMX, SSE或SSE2指令，控制单元就产生一个“Device not available”异常(参见第四章)。

TS标志使得内核只有在真正需要时才保存和恢复FPU, MMX和XMM寄存器。为了说明它如何工作，假设进程A使用数学协处理器。当发生上下文切换时，内核置TS标志并把浮点寄存器保存在进程A的TSS中。如果新进程B不利用协处理器，内核就不必恢复浮点寄存器的内容。但是，只要B打算执行ESCAPE或MMX指令，CPU就产生一个“Device not available”异常，并且相应的异常处理程序用保存在进程B中的TSS的值装载浮点寄存器。

现在，让我们描述为处理FPU、MMX和XMM寄存器的选择性装入而引入的数据结构。它们存放在进程描述符的thread.i387子字段中，其格式由i387_union联合体描述：
```c
union i387_union{
	struct i387_fsave_struct fsave;
  	struct i387_fxsave_struct fxsave;
	struct i387_soft_struct soft;
};
```

正如看到的，这个字段只可以存放三种不同数据结构中的一种。i387\_soft_struct结构由无数学协处理器的CPU模型使用;Linux内核通过软件模拟协处理器来支持这些老式芯片。不过，不打算进一步讨论这种遗留问题。i387\_fsave_struct结构由具有数学协处理器、也可能有MMX单元的CPU模型使用。最后，i387_fxsave_struct结构由具有SSE和SSE2扩展功能的CPU模型使用。

进程描述符包含两个附加的标志:

- 包含在thread_info描述符的status字段中的TS_USEDFPU标志。它表示进程在当前执行的过程中是否使用过FPU、MMX和XMM寄存器。
- 包含在task_struct描述符的flags字段中的PF_USED_MATH标志。这个标志表示thread.i387子字段的内容是否有意义。该标志在两种情况下被清0(没有意义)，如下所示：
  - 当进程调用execve()系统调用(参见第二十章)开始执行一个新程序时。因为控制权将不再返回到前一个程序，所以当前存放在thread.i387中的数据也不再使用。
  - 当在用户态下执行一个程序的进程开始执行一个信号处理程序时(参见第十一章)。因为信号处理程序与程序的执行流是异步的，因此，浮点寄存器对信号处理程序来说可能是毫无意义的。不过，内核开始执行信号处理程序之前在thread.i387中保存浮点寄存器，处理程序结束以后恢复它们。因此，信号处理程序可以使用数学协处理器。

##### 保存FPU寄存器

如前所述，\_\_switch_to()函数把被替换进程prev的描述符作为参数传递给\_\_unlazy_fpu宏，并执行该宏。这个宏检查prev的TS_USEDFPU标志值。如果该标志被设置，说明prev在这次执行中使用了FPU,MMX,SSE或SSE2指令;因此内核必须保存相关的硬件上下文：

```c
#define __unlazy_fpu( tsk ) do { \
	if ((tsk)->thread_info->status & TS_USEDFPU) \
		save_init_fpu( tsk ); \
} while (0)
```

save_init_fpu()函数依次执行下列操作：

1. 把FPU寄存器的内容转储到prev进程描述符中，然后重新初始化FPU。如果CPU使用SSE/SSE2扩展，则还应该转储XMM寄存器的内容，井重新初始化SSE/SSE2单元。一对功能强大的嵌入式汇编语言指令处理每件事情，如果CPU使用SSE/SSE2扩展，则：

   ```c
   asm volatile( "fxsave %0 ; fnclex"
   			      : "=m" (tsk->thread.i387.fxsave) );
   ```

   否则：

   ```c
   asm volatile( "fnsave %0 ; fwait"
   			      : "=m" (tsk->thread.i387.fsave) );
   ```

2. 重置prev的TS_USEDFPU标志：

   ```c
   tsk->thread_info->status &= ~TS_USEDFPU;
   ```

3. 用stts()宏设置cr0的TS标志，实际上，该宏产生下面的汇编语言指令：

   ```c
   movl %cr0，%eax
   orl $8，%eax
   movl %eax，%cr0
   ```

   ​

##### 装载FPU寄存器

当next进程刚恢复执行时，浮点寄存器的内容还没有被恢复，不过，cr0的TS标志位已由\_\_unlazy_fpu()设置。因此，next进程第一次试图执行ESCAPE, MMX或SSE/SSE2指令时，控制单元产生一个“Device not available”异常，内核(更确切地说，由异常调用的异常处理程序)运行math_state_restore()函数。处理程序把next进程当作current进程。

```c
asmlinkage void math_state_restore(struct pt_regs regs)
{
	struct thread_info *thread = current_thread_info();
	struct task_struct *tsk = thread->task;

	__asm__ __volatile__ ("clts")		/* Allow maths ops (or we recurse) ;clear the TS flag of cr0 */
	if (!((tsk)->flags & PF_USED_MATH))
		init_fpu(tsk);
	restore_fpu(tsk);
	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
}
```

这个函数清cr0的TS标志，以便进程以后执行FPU, MMX或SSE/SSE2指令时不再触发“设备不可用”的异常。如果thread.i387子字段中的内容是无效的，也就是说，如果PF_USED_ MATH标志等于0，就调用init_fpu()重新设置thread.i387子字段，并把PF_USED_MATH标志的当前值置为1。 restore_fpu()函数把保存在thread.i387子字段中的适当值载入FPU寄存器。为此，根据CPU是否支持SSE/SSE2扩展来使用fxrstor或frstor汇编语言指令。最后，math_state_restore()设置TS_USEDFPU标志。

##### 在内核态使用FPU、MMX和SSE/SSE2单元

内核也可以使用FPU,MMX和SSE/SSE2单元。当然，这样做的时候，应该避免干扰用户态进程所进行的任何计算。因此：

- 在使用协处理器之前，如果用户态进程使用了FPU(TS_USEDFPU标志)，内核必须调用kernel_fpu_begin()，其本质就是调用save_init_fpu()来保存寄存器的内容，然后重新设置cr0寄存器的TS标志。
- 在使用完协处理器之后，内核必须调用kernel_fpu_end()设置cr0寄存器的TS标志。

稍后，当用户态进程执行协处理器指令时，math_state_restore()函数将恢复寄存器的内容(就像处理进程切换那样)。

但是，应该注意，当前用户态进程正在使用协处理器时，kernel_fpu_begin()的执行时间相当长，以至于无法通过使用FPU,MMX或SSE/SSE2单元达到加速的目的。实际上，内核只在有限的场合使用FPU, MMX或SSE/SSE2单元，典型的情况有：当移动或清除大内存区字段时，或者当计算校验和函数时。

### 创建进程

Unix操作系统紧紧依赖进程创建来满足用户的需求。例如，只要用户输入一条命令，shell进程就创建一个新进程，新进程执行shell的另一个拷贝。

传统的Unix操作系统以统一的方式对待所有的进程：子进程复制父进程所拥有的资源。这种方法使进程的创建非常慢且效率低，因为子进程需要拷贝父进程的整个地址空间。实际上，子进程几乎不必读或修改父进程拥有的所有资源，在很多情况下，子进程立即调用execve()，并清除父进程仔细拷贝过来的地址空间。

现代Unix内核通过引入三种不同的机制解决了这个问题：

- 写时复制技术允许父子进程读相同的物理页。只要两者中有一个试图写一个物理页，内核就把这个页的内容拷贝到一个新的物理页，并把这个新的物理页分配给正在写的进程。第九章将全面地解释这种技术在Linux中的实现。

- 轻量级进程允许父子进程共享每进程在内核的很多数据结构，如页表(也就是整个用户态地址空间)、打开文件表及信号处理。

- vfork()系统调用创建的进程能共享其父进程的内存地址空间。为了防止父进程重写子进程需要的数据，阻塞父进程的执行，一直到子进程退出或执行一个新的程序为止。

#### clone()、fork()及vfork()系统调用

在Linux中，轻量级进程是由名为clone()的函数创建的，这个函数使用下列参数：

| 参数          | 描述                                       |
| ----------- | ---------------------------------------- |
| fn          | 指定一个由新进程执行的函数。当这个函数返回时，子进程终止。函数返回一个整数，表示子进程的退出代码。 |
| arg         | 指向传递给fn()函数的数据。                          |
| flags       | 各种各样的信息。低字节指定子进程结束时发送到父进程的信号代码，通常选择SIGCHLD信号。剩余的3个字节给clone标志组用于编码。 |
| child_stack | 表示把用户态堆栈指针赋给子进程的esp寄存器。调用进程(指调用clone()的    父进程)应该总是为子进程分配新的堆栈。 |
| tls         | 表示线程局部存储段(TLS)数据结构的地址，该结构是为新轻量级进程定义的(参见第二章“Linux GDT”一节)。只有在CLONE_SETTLS标志被设置时才有意义。 |
| ptid        | 表示父进程的用户态变量地址，该父进程具有与新轻量级进程相同的PID。只有在CLONE_PARENT_SETTID标志被设置时才有意义。 |
| ctid        | 表示新轻量级进程的用户态变量地址，该进程具有这一类进程的PID。只有在CLONE_CHILD_ SETTID标志被设置时才有意义。 |

clone标志：

| 标志名称                 | 说明                                       |
| -------------------- | ---------------------------------------- |
| CLONE_VM             | 共享内存描述符和所有的页表(参见第九章)                     |
| CLONE_FS             | 共享根目录和当前工作目录所在的表，以及用于屏蔽新文件初始许可权的位掩码值(所谓文件的umask ) |
| CLONE_FILES          | 共享打开文件表(参见第十二章)                          |
| CLONE_SIGHAND        | 共享信号处理程序的表、阻塞信号表和挂起信号表(参见第十一章)。如果这个标志为true,就必须设置CLONE_VM标志 |
| CLONE_PTRACE         | 如果父进程被跟踪，那么，子进程也被跟踪。尤其是，debugger程序可能希望以自己作为父进程来跟踪子进程，在这种情况下，内核把该标志强置为1 |
| CLONE_VFORK          | 在发出vfork()系统调用时设置(参见本节后面)                |
| CLONE_PARENT         | 设置子进程的父进程(进程描述符中的parent和real_parent字段)为调用进程的父进程 |
| CLONE_THREAD         | 把子进程插入到父进程的同一线程组中，并迫使子进程共享父进程的信号描述符。因此也设置子进程的tgid字段和group_leader字段。如果这个标志位为true，就必须设置CLONE_SIGRAND标志 |
| CLONE_NEWNS          | 当clone需要自己的命名空间时(即它自己的已挂载文件系统视图)设置这个标志(参见第十二章)。不能同时设置CLONE_NEWNS和CLONE_FS |
| CLONE_SYSVSEM        | 共享System V IPC取消信号量的操作(参见第十九章"IPC信号量”一节) |
| CLONE_SETTLS         | 为轻量级进程创建新的线程局部存储段(TLS)，该段由参数tls所指向的结构进行描述 |
| CLONE_PARENT_SETTID  | 把子进程的PID写入由ptid参数所指向的父进程的用户态变量           |
| CLONE_CHILD_CLEARTID | 如果该标志被设置，则内核建立一种触发机制，用在子进程要退出或要开始执行新程序时。在这些情况下，内核将清除由参数ctid所指向的用户态变量，并唤醒等待这个事件的任何进程 |
| CLONE_DETACHED       | 遗留标志.内核会忽略它                              |
| CLONE_UNTRACED       | 内核设置这个标志以使CLONE_PTRACE标志失去作用(用来禁止内核线程跟踪进程，参见本章稍后的“内核线程”一节) |
| CLONE_CHILD_SETTID   | 把子进程的PID写入由ctid参数所指向的子进程的用户态变量中          |
| CLONE_STOPPED        | 强迫子进程开始于TASK_STOPPED状态                   |

实际上，clone()是在C语言库中定义的一个封装(wrapper)函数(参见第十章“POSIX API和系统调用”一节)，它负责建立新轻量级进程的堆栈并且调用对编程者隐藏的clone()系统调用。实现clone()系统调用的sys_clone()服务例程没有fn和arg参数。实际上，封装函数把fn指针存放在子进程堆栈的某个位置处，该位置就是该封装函数本身返回地址存放的位置。arg指针正好存放在子进程堆栈中fn的下面。当封装函数结束时，CPU从堆栈中取出返回地址，然后执行fn(arg)函数。

传统的fork()系统调用在Linux中是用clone()实现的，其中clone()的flags参数指定为SIGCHLD信号及所有清0的clone标志，而它的child_stack参数是父进程当前的堆栈指针。因此，父进程和子进程暂时共享同一个用户态堆栈。但是，要感谢写时复制机制，通常只要父子进程中有一个试图去改变栈，则立即各自得到用户态堆栈的一份拷贝。

前一节描述的vfork()系统调用在Linux中也是用clone()实现的，其中clone()的参数flags指定为SIGCHLD信号和CLONE_VM及CLONE_VFORK标志，clone()的参数child_ stack等于父进程当前的栈指针。

#### do_fork()函数

do_fork()函数负责处理clone(),fork()和vfork()系统调用，执行时使用下列参数：

| 参数                         | 描述                                       |
| -------------------------- | ---------------------------------------- |
| clone_flags                | 与clone()的参数flags相同                       |
| stack_start                | 与clone()的参数child_stack相同                 |
| regs                       | 指向通用寄存器值的指针，通用寄用器的值是在从用户态切换到内核态时被保存到内核态堆栈中的(参见第四章“do_IRQ()函数”一节) |
| stack_size                 | 未使用(总是被设置为O)                             |
| parent_tidptr child_tidptr | 与clone()中的对应参数ptid和ctid相同                |

do_fork()利用辅助函数copy_process()来创建进程描述符以及子进程执行所需要的所有其他内核数据结构。下面是do_fork()执行的主要步骤：

1. 通过查找pidmap_array位图，为子进程分配新的PID(参见本章前面“标识一个进程”一节)。

2. 检查父进程的ptrace字段(current->ptrace)：如果它的值不等于0，说明有另外一个进程正在跟踪父进程，因而，do_fork()检查debugger程序是否自己想跟踪子进程(独立于由父进程指定的CLONE_PTRACE标志的值)。在这种情况下，如果子进程不是内核线程(CLONE_UNTRACED标志被清0)，那么do_fork()函数设置CLONE_PTRACE标志。

3. 调用copy_process()复制进程描述符。如果所有必须的资源都是可用的，该函数返回刚创建的task_struct描述符的地址。这是创建过程的关键步骤，我们将在do_fork()之后描述它。

4. 如果设置了CLONE_STOPPED标志，或者必须跟踪子进程，即在p->ptrace中设置了PT_PTRACED标志，那么子进程的状态被设置成TASK_STOPPED，并为子进程增加挂起的SIGSTOP信号(参见第十一章“信号的作用一节)。在另外一个进程(不妨假设是跟踪进程或是父进程)把子进程的状态恢复为TASK_RUNNING之前(通常是通过发送SIGCONT信号)，子进程将一直保持TASK_STOPPED状态。

5. 如果没有设置CLONE_STOPPED标志，则调用wake_up_new_task()函数以执行下述操作：
   - 调整父进程和子进程的调度参数(参见第七章“调度算法”一节)
   - 如果子进程将和父进程运行在同一个CPU上(当内核创建新进程时，父进程可能被转移到另一个CPU上执行)，而且父进程和子进程不能共享同一组页表(CLONE_VM标志被清0)，那么，就把子进程插入父进程运行队列，插入时让子进程恰好在父进程前面，因此而迫使子进程先于父进程运行。如果子进程刷新其地址空间，并在创建之后执行新程序，那么这种简单的处理会产生较好的性能。而如果我们让父进程先运行，那么写时复制机制将会执行一系列不必要的页面复制。
   - 否则，如果子进程与父进程运行在不同的CPU上，或者父进程和子进程共享同一组页表(CLONE_VM标志被设置)，就把子进程插入父进程运行队列的队尾。
6. 如果CLONE_STOPPED标志被设置，则把子进程置为TASK_STOPPED状态。
7. 如果父进程被跟踪，则把子进程的PID存入current的ptrace_message字段并调用ptrace_notify()。ptrace_notify()使当前进程停止运行，并向当前进程的父进程发送SIGCHLD信号。子进程的祖父进程是跟踪父进程的debugger进程。SIGCHLD信号通知debugger进程:current已经创建了一个子进程，可以通过查找current->ptrace_message字段获得子进程的PID。
8. 如果设置了CLONE_VFORK标志，则把父进程插入等待队列，并挂起父进程直到子进程释放自己的内存地址空间(也就是说，直到子进程结束或执行新的程序)。
9. 结束并返回子进程的PID。

#### copy_process()函数

copy_process()创建进程描述符以及子进程执行所需要的所有其他数据结构。它的参数与do_fork()的参数相同，外加子进程的PID。下面描述copy_process()的最重要的步骤：

1. 检查参数clone_flags所传递标志的一致性。在下列情况下，它返回错误代号：
   - CLONE_NEWNS和CLONE_FS标志都被设置。
   - CLONE_THREAD标志被设置，但CLONE_SIGRAND标志被清0(同一线程组中的轻量级进程必须共享信号)。
   - CLONE_SIGHAND标志被设置，但CLONE_VM被清0(共享信号处理程序的轻量级进程也必须共享内存描述符)。
2. 通过调用security_task_create()以及稍后调用的security_task_alloc()执行所有附加的安全检查。Linux 2.6提供扩展安全性的钩子函数，与传统Unix相比，它具有更加强壮的安全模型。详情参见第二十章。
3. 调用dup_task_struct()为子进程获取进程描述符。该函数执行如下操作：
   - 如果需要，则在当前进程中调用\_\_unlazy_fpu()，把FPU,MMX和SSE/SSE2寄存器的内容保存到父进程的thread_info结构中。稍后，dup_task_struct()将把这些值复制到子进程的thread_info结构中。
   - 执行alloc_task_struct()宏，为新进程获取进程描述符(task_struct结构)，并将描述符地址保存在tsk局部变量中。
   - 执行alloc_thread_info宏以获取一块空闲内存区，用来存放新进程的thread_info结构和内核栈，并将这块内存区字段的地址存在局部变量ti中。正如在本章前面“标识一个进程”一节中所述：这块内存区字段的大小是8KB或4KB。
   - 将current进程描述符的内容复制到tsk所指向的task_struct结构中，然后把tsk->thread_ info置为ti。
   - 把current进程的thread_info描述符的内容复制到ti所指向的结构中，然后把ti->task置为tsk。
   - 把新进程描述符的使用计数器(tsk->usage)置为2，用来表示进程描述符正在被使用而且其相应的进程处于活动状态(进程状态即不是EXIT_ ZOMBIE,也不是EXIT_DEAD)。
   - 返回新进程的进程描述符指针(tsk)。
4. 检查存放在current->signal->rlim[RLIMIT_NPROC].rlim_cur变量中的值是否小于或等于用户所拥有的进程数。如果是，则返回错误码，除非进程没有root权限。该函数从每用户数据结构user_struct中获取用户所拥有的进程数。通过进程描述符user字段的指针可以找到这个数据结构。
5. 递增user_struct结构的使用计数器(tsk->user->\_\_count字段)和用户所拥有的进程的计数器(tsk->user->processes)。
6. 检查系统中的进程数量(存放在nr_threads变量中)是否超过max_threads变量的值。这个变量的缺省值取决于系统内存容量的大小。总的原则是：所有thread_info描述符和内核栈所占用的空间不能超过物理内存大小的1/8。不过，系统管理员可以通过写/proc/sys/kernel/threads-max文件来改变这个值。
7. 如果实现新进程的执行域和可执行格式的内核函数(参见第二十章)都包含在内核模块中，则递增它们的使用计数器(参见附录二)。
8. 设置与进程状态相关的几个关键字段：
   - 把大内核锁计数器tsk->lock_depth初始化为-1(参见第五章“大内核锁”一节)。
   - 把tsk->did_exec字段初始化为0；它记录了进程发出的execve()系统调用的次数。
   - 更新从父进程复制到tsk->flags字段中的一些标志：首先清除PF_SUPERPRIV标志，该标志表示进程是否使用了某种超级用户权限。然后设置PF_FORKNOEXEC标志，它表示子进程还没有发出execve()系统调用。
9. 把新进程的PID存人tsk->pid字段。
10. 如果clone_flags参数中的CLONE_PARENT_SETTID标志被设置，就把子进程的PID复制到参数parent_tidptr指向的用户态变量中。
11. 初始化子进程描述符中的list_head数据结构和自旋锁，并为与挂起信号、定时器及时间统计表相关的几个字段赋初值。
12. 调用copy_semundo()，copy_files()，copy_fs()，copy_sighand()，copy_signal() , copy_mm()和copy namespace()来创建新的数据结构，并把父进程相应数据结构的值复制到新数据结构中，除非clone_flags参数指出它们有不同的值。
13. 调用copy_thread()，用发出clone()系统调用时CPU寄存器的值(正如第十章所述，这些值已经被保存在父进程的内核栈中)来初始化子进程的内核栈。不过，copy_thread()把eax寄存器对应字段的值[这是fork()和clone()系统调用在子进程中的返回值]字段强行置为0。子进程描述符的thread.esp字段初始化为子进程内核栈的基地址，汇编语言函数(ret_from_fork())的地址存放在thread.eip字段中。如果父进程使用I/O权限位图，则子进程获取该位图的一个拷贝。最后，如果CLONE_SETTLS标志被设置，则子进程获取由clone()系统调用的参数tls指向的用户态数据结构所表示的TLS段(tls并不被传递给do_fork()和嵌套函数。在第十章会看到，通过拷贝系统调用的参数的值到某个CPU寄存器来把它们传递给内核；因此，这些值与其他寄存器一起被保存在内核态堆栈中。copy_thread()只查看esi的值在内核堆栈中对应的位置保存的地址)。
14. 如果clone_flags参数的值被置为CLONE_CHILD_SETTID或CLONE_CHILD_CLEARTID,就把child_tidptr参数的值分别复制到tsk->setchid_tid或tsk->clear_child_tid字段。这些标志说明:必须改变子进程用户态地址空间的child_tidptr所指向的变量的值，不过实际的写操作要稍后再执行。
15. 清除子进程thread_info结构的TIF_SYSCALL_TRACE标志，以使ret_from_fork()函数不会把系统调用结束的消息通知给调试进程(参见第十章“进入和退出系统调用”一节)。(因为对子进程的跟踪是由tsk->ptrace中的PTRACE_SYSCALL标志来控制的，所以子进程的系统调用跟踪不会被禁用。)
16. 用clone_flags参数低位的信号数字编码初始化tsk->exit_signal字段，如果CLONE_THREAD标志被置位，就把tsk->exit_sinal字段初始化为-1。正如我们将在本章稍后“进程终止”一节所看见的，只有当线程组的最后一个成员(通常是线程组的领头)“死亡”，才会产生一个信号，以通知线程组的领头进程的父进程。
17. 调用sched_fork()完成对新进程调度程序数据结构的初始化。该函数把新进程的状态设置为TASK_RUNNING，并把thread_info结构的preempt_count字段设置为1，从而禁止内核抢占(参见第五章“内核抢占”一节)。此外，为了保证公平的进程调度，该函数在父子进程之间共享父进程的时间片(参见第七章“scheduler_tick()函数”一节)。
18. 把新进程的thread_info结构的cpu字段设置为由smp_processor_id()所返回的本地CPU号。
19. 初始化表示亲子关系的字段。尤其是，如果CLONE_PARENT或CLONE_THREAD被设置，就用curent->real_parent的值初始化tsk->real_parent和tsk->parent,因此，子进程的父进程似乎是当前进程的父进程。否则，把tsk->real_parent和tsk->parent置为当前进程。
20. 如果不需要跟踪子进程(没有设置CLONE_PTRAC标志)，就把tsk->ptrace字段设置为O。tsk->ptrace字段会存放一些标志，而这些标志是在一个进程被另外一个进程跟踪时才会用到的。采用这种方式，即使当前进程被跟踪，子进程也不会被跟踪。
21. 执行SET_LINKS宏，把新进程描述符插人进程链表。
22. 如果子进程必须被跟踪(tsk->ptrace字段的PT_PTRACED标志被设置)，就把current->parent赋给tsk->parent，并将子进程插入调试程序的跟踪链表中。
23. 调用attach_pid()把新进程描述符的PID插入pidhash[PIDTYPE_PID]散列表。
24. 如果子进程是线程组的领头进程(CLONE_THREAD标志被清0)：
    - 把tsk->tgid的初值置为tsk->pid。
    - 把tsk->group_leader的初值置为tsk。
    - 调用三次attach_pid()，把子进程分别插入PIDTYPE_TGID, PIDTYPE_PGID和PIDTYPE_SID类型的PID散列表。
25. 否则，如果子进程属于它的父进程的线程组(CLONE_THREAD标志被设置)：
    - 把tsk->tgid的初值置为tsk->current->tgid。
    - 把tsk->group_leader的初值置为current->group_leader的值。
    - 调用attach_pid()，把子进程插入PIDTYPE_TGID类型的散列表中(更具体地说，插入current->group_leader进程的每个PID链表)。
26. 现在，新进程已经被加入进程集合:递增nr_threads变量的值。
27. 递增total_forks变量以记录被创建的进程的数量。
28. 终止并返回子进程描述符指针(tsk)。

现在，我们有了处于可运行状态的完整的子进程。但是，它还没有实际运行，调度程序要决定何时把CPU交给这个子进程。在以后的进程切换中，调度程序继续完善子进程:把子进程描述符thread字段的值装入几个CPU寄存器。特别是把thread.esp(即把子进程内核态堆栈的地址)装人esp寄存器，把函数ret_from_fork()的地址装人eip寄存器。这个汇编语言函数调用schedule_tail()函数(它依次调用finish_task_switch()来完成进程切换，参见第七章“schedule()函数”一节)，用存放在栈中的值再装载所有的寄存器，并强迫CPU返回到用户态。然后，在fork(),vfork()或clone()系统调用结束时，新进程将开始执行。系统调用的返回值放在eax寄存器中:返回给子进程的值是0，返回给父进程的值是子进程的PID。回顾copy_thread()对子进程的eax寄存器所执行的操作(copy_process()的第13步)，就能理解这是如何实现的。

除非fork系统调用返回0，否则，子进程将与父进程执行相同的代码(参见copy_process()的第13步)。应用程序的开发者可以按照Unix编程者熟悉的方式利用这一事实，在基于PID值的程序中插人一个条件语句使子进程与父进程有不同的行为。

#### 内核线程

传统的Unix系统把一些重要的任务委托给周期性执行的进程，这些任务包括刷新磁盘高速缓存，交换出不用的页框，维护网络连接等等。事实上，以严格线性的方式执行这些任务的确效率不高，如果把它们放在后台调度，不管是对它们的函数还是对终端用户进程都能得到较好的响应。因为一些系统进程只运行在内核态，所以现代操作系统把它们的函数委托给内核线程(kernel thread)，内核线程不受不必要的用户态上下文的拖累。在Linux中，内核线程在以下几方面不同于普通进程：

- 内核线程只运行在内核态，而普通进程既可以运行在内核态，也可以运行在用户态。
- 因为内核线程只运行在内核态，它们只使用大于PAGE_OFFSET的线性地址空间。另一方面，不管在用户态还是在内核态，普通进程可以用4GB的线性地址空间。

##### 创建一个内核线程

kernel_thread()函数创建一个新的内核线程，它接受的参数有：所要执行的内核函数的地址(fn)、要传递给函数的参数(arg)、一组clone标志(flags)。该函数本质上以下面的方式调用do_fork()：

```c
int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
{
	struct pt_regs regs;

	memset(&regs, 0, sizeof(regs));

	regs.ebx = (unsigned long) fn;
	regs.edx = (unsigned long) arg;

	regs.xds = __USER_DS;
	regs.xes = __USER_DS;
	regs.orig_eax = -1;
	regs.eip = (unsigned long) kernel_thread_helper;
	regs.xcs = __KERNEL_CS;
	regs.eflags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;

	/* Ok, create the new process.. */
	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
}
```

CLONE_VM标志避免复制调用进程的页表：由于新内核线程无论如何都不会访问用户态地址空间，所以这种复制无疑会造成时间和空间的浪费。CLONE_UNTRACED标志保证不会有任何进程跟踪新内核线程，即使调用进程被跟踪。

传递给do_fork()的参数regs表示内核栈的地址，copy_thread()函数将从这里找到为新线程初始化CPU寄存器的值。kernel_thread()函数在这个栈中保留寄存器值的目的是：

- 通过copy_thread()把ebx和edx设置为参数fn和arg的值。

- 把eip寄存器的值设置为下面汇编语言代码段的地址：

  ```c
  movl %edx, %eax
  pushl %edx
  call *%ebx
  pushl %eax
  call do_exit
  ```

因此，新的内核线程开始执行fn(arg)函数，如果该函数结束，内核线程执行系统调用_exit()，并把fn()的返回值传递给它(参见本章稍后“撤消进程”一节)。

##### 进程0

所有进程的祖先叫做进程0，idle进程或因为历史的原因叫做swapper进程，它是在Linux的初始化阶段从无到有创建的一个内核线程。这个祖先进程使用下列静态分配的数据结构(所有其他进程的数据结构都是动态分配的)：

- 存放在init_task变量中的进程描述符，由INIT_TASK宏完成对它的初始化。

- 存放在init_thread_union变量中的thread_info描述符和内核堆栈，由INiT_THREAD_INFO宏完成对它们的初始化。

- 由进程描述符指向的下列表：

  ——init_mm

  ——init_fs

  ——init_files

  ——init_signals

  ——init_sighand

  这些表分别由下列宏初始化：

  ——INIT_MM

  ——INIT_FS

  ——INIT_FILES

  ——INIT_SIGNALS

  ——INIT_SIGHAND

- 主内核页全局目录存放在swapper_pg_dir中(参见第二章“内核页表”一节)。

start_kernel()函数初始化内核需要的所有数据结构，激活中断，创建另一个叫进程1的内核线程(一般叫做init进程)：

```c
kernel_thread(init, NULL, CLONE_FS | CLONE_SIGHAND);
```

新创建内核线程的PID为1，并与进程0共享每进程所有的内核数据结构。此外，当调度程序选择到它时，init进程开始执行init()函数。

创建init进程后，进程0执行cpu_idle()函数，该函数本质上是在开中断的情况下重复执行hlt汇编语言指令(参见第四章)。只有当没有其他进程处于TASK_RUNNING状态时，调度程序才选择进程0。

在多处理器系统中，每个CPU都有一个进程0。只要打开机器电源，计算机的BIOS就启动某一个CPU，同时禁用其他CPU。运行在CPU 0上的swapper进程初始化内核数据结构，然后激活其他的CPU，并通过copy_process()函数创建另外的swapper进程，把0传递给新创建的swapper进程作为它们的新PID。此外，内核把适当的CPU索引赋给内核所创建的每个进程的thread_info描述符的cpu字段。

##### 进程1

由进程0创建的内核线程执行init()函数，init()依次完成内核初始化。init()调用execve()系统调用装入可执行程序init。结果，init内核线程变为一个普通进程，且拥有自己的每进程(per-process)内核数据结构(参见第二十章)。在系统关闭之前，init进程一直存活，因为它创建和监控在操作系统外层执行的所有进程的活动。

##### 其他内核线程

Linux使用很多其他内核线程。其中一些在初始化阶段创建，一直运行到系统关闭；而其他一些在内核必须执行一个任务时“按需”创建，这种任务在内核的执行上下文中得到很好的执行。

一些内核线程的例子(除了进程0和进程1)是：

| 线程              | 描述                                       |
| --------------- | ---------------------------------------- |
| keventd(也被称为事件) | 执行keventd_wq工作队列(参见第四章)中的函数。             |
| kapmd           | 处理与高级电源管理(APM)相关的事件。                     |
| kswapd          | 执行内存回收，在第十七章“周期回收”一节将进行描述。               |
| pdflush         | 刷新“脏”缓冲区中的内容到磁盘以回收内存，在第十五章“pdflush内核线程”一 |
| kblockd         | 执行kblockd_workqueue工作队列中的函数。实质上，它周期性地激活块设备驱动程序，将在第十四章“激活块设备驱动程序”一节给予描述。 |
| ksoftirqd       | 运行tasklet(参看第四章“软中断及tasklet”一节)。系统中每个CPU都有这样一个内核线程。 |

### 撤销进程

很多进程终止了它们本该执行的代码，从这种意义上说，这些进程“死”了。当这种情况发生时，必须通知内核以便内核释放进程所拥有的资源，包括内存、打开文件及其他我们在本书中讲到的零碎东西，如信号量。

进程终止的一般方式是调用exit()库函数，该函数释放c函数库所分配的资源，执行编程者所注册的每个函数，并结束从系统回收进程的那个系统调用。exit()函数可能由编程者显式地插入。另外，C编译程序总是把exit()函数插入到main()函数的最后一条语句之后。

内核可以有选择地强迫整个线程组死掉。这发生在以下两种典型情况下：

- 当进程接收到一个不能处理或忽视的信号时(参见十一章)
- 当内核正在代表进程运行时在内核态产生一个不可恢复的CPU异常时(参见第四章)。

#### 进程终止

在Linux 2.6中有两个终止用户态应用的系统调用：

- exit_grpup()系统调用，它终止整个线程组，即整个基于多线程的应用。do_group_exit()是实现这个系统调用的主要内核函数。这是C库函数exit()应该调用的系统调用。
- exit()系统调用，它终止某一个线程，而不管该线程所属线程组中的所有其他进程。do_exit()是实现这个系统调用的主要内核函数。这是被诸如pthread_exit()的Linux线程库的函数所调用的系统调用。

#### do_group_exit()函数

do_group_exit()函数杀死属于current线程组的所有进程。它接受进程终止代号作为参数，进程终止代号可能是系统调用exit_group()(正常结束)指定的一个值，也可能是内核提供的一个错误代号(异常结束)。该函数执行下述操作：

1. 检查退出进程的SIGNAL_GROUP_EXIT标志是否不为0，如果不为0，说明内核已经开始为线程组执行退出的过程。在这种情况下，就把存放在current->signal->group_exit_code中的值当作退出码，然后跳转到第4步。

2. 否则，设置进程的SIGNAL_GROUP_EXIT标志并把终止代号存放到current->signal->group_exit_code字段。

3. 调用zap_other_threads()函数杀死current线程组中的其他进程(如果有的话)。为了完成这个步骤，函数扫描与current->tgid对应的PIDTYPE_TGID类型的散列表中的每个PID链表，向表中所有不同于current的进程发送SIGKILL信号(参
   见第十一章)，结果，所有这样的进程都将执行do_exit()函数，从而被杀死。

4. 调用do_exit()函数，把进程的终止代号传递给它。do_exit()杀死进程而且不再返回。

#### do_exit()函数

所有进程的终止都是由do_exit()函数来处理的，这个函数从内核数据结构中删除对终止进程的大部分引用。do_exit()函数接受进程的终止代号作为参数并执行下列操作：

1. 把进程描述符的flag字段设置为PF_EXITING标志，以表示进程正在被删除。
2. 如果需要，通过函数del_timer_sync()(参见第六章)从动态定时器队列中删除进程描述符。
3. 分别调用exit_mm(),exit_sem(),\_\_exit_files(),\_\_exit_fs(),exit_namespace()和exit_thread()函数从进程描述符中分离出与分页、信号量、文件系统、打开文件描述符、命名空间以及I/O权限位图相关的数据结构。如果没有其他进程共享这些数据结构，那么这些函数还删除所有这些数据结构中。
4. 如果实现了被杀死进程的执行域和可执行格式(参见第二十章)的内核函数包含在内核模块中，则函数递减它们的使用计数器。
5. 把进程描述符的exit_code字段设置成进程的终止代号，这个值要么是_exit()或exit_group()系统调用参数(正常终止)，要么是由内核提供的一个错误代号(异常终止)。
6. 调用exit_notify()函数执行下面的操作：
   - 更新父进程和子进程的亲属关系。如果同一线程组中有正在运行的进程，就让终止进程所创建的所有子进程都变成同一线程组中另外一个进程的子进程，否则让它们成为init的子进程。
   - 检查被终止进程其进程描述符的exit_signal字段是否不等于-1，并检查进程是否是其所属进程组的最后一个成员(注意:正常进程都会具有这些条件，参见前面“clone(),fork()和vfork()系统调用”一节中对copy_process()的描述，第16步)。在这种情况下，函数通过给正被终止进程的父进程发送一个信号(通常是SIGCHLD)，以通知父进程子进程死亡。
   - 否则，也就是exit_signal字段等于-1，或者线程组中还有其他进程，那么只要进程正在被跟踪，就向父进程发送一个SIGCHLD信号(在这种情况下，父进程是调试程序，因而，向它报告轻量级进程死亡的信息)。
   - 如果进程描述符的exit_signal字段等于-1，而且进程没有被跟踪，就把进程描述符的exit_state字段置为EXIT_DEAD，然后调用release_task()回收进程的其他数据结构占用的内存，并递减进程描述符的使用计数器(见下一节)。使用记数器变为1(参见copy_process()函数的第3f步)，以使进程描述符本身正好不会被释放。
   - 否则，如果进程描述符的exit_signal字段不等于-1，或进程正在被跟踪，就把exit_state字段置为EXIT_ZOMBIE。在下一节我们将看到如何处理僵死进程。
   - 把进程描述符的flags字段设置为PF_DEAD标志(参见第七章“schedule()函数”一节)。
7. 调用schedule()函数(参见第七章)选择一个新进程运行。调度程序忽略处于EXIT_ZOMBIE状态的进程，所以这种进程正好在schedule()中的宏switch_to被调用之后停止执行。正如在第七章我们将看到的:调度程序将检查被替换的僵死进程描述符的PF_DEAD标志并递减使用计数器，从而说明进程不再存活的事实。


#### 进程删除

Unix允许进程查询内核以获得其父进程的PID,或者其任何子进程的执行状态。例如，进程可以创建一个子进程来执行特定的任务，然后调用诸如wait()这样的一些库函数检查子进程是否终止。如果子进程已经终止，那么，它的终止代号将告诉父进程这个任务是否已成功地完成。

为了遵循这些设计选择，不允许Unix内核在进程一终止后就丢弃包含在进程描述符字段中的数据。只有父进程发出了与被终止的进程相关的wait()类系统调用之后，才允许这样做。这就是引入僵死状态的原因:尽管从技术上来说进程已死，但必须保存它的描述符，直到父进程得到通知。

如果父进程在子进程结束之前结束会发生什么情况呢?在这种情况下，系统中会到处是僵死的进程，而且它们的进程描述符永久占据着RAM。如前所述，必须强迫所有的孤儿进程成为init进程的子进程来解决这个问题。这样，init进程在用wait()类系统调用检查其合法的子进程终止时，就会撤消僵死的进程。

release_task()函数从僵死进程的描述符中分离出最后的数据结构；对僵死进程的处理有两种可能的方式：

- 如果父进程不需要接收来自子进程的信号，就调用do_exit()。
- 如果已经给父进程发送了一个信号，就调用wait4()或waitpid()系统调用。

在后一种情况下，函数还将回收进程描述符所占用的内存空间，而在前一种情况下，内存的回收将由进程调度程序来完成(参见第七章)。该函数执行下述步骤：

1. 递减终止进程拥有者的进程个数。这个值存放在本章前面提到的user_struct结构中(参见copy_process()的第4步)。
2. 如果进程正在被跟踪，函数将它从调试程序的ptrace_children链表中删除，并让该进程重新属于初始的父进程。

3. 调用\_\_exit_signal()删除所有的挂起信号并释放进程的signal_struct描述符。如果该描述符不再被其他的轻量级进程使用，函数进一步删除这个数据结构。此外，函数调用exit_itimers()从进程中剥离掉所有的POSIX时间间隔定时器。

4. 调用\_\_exit_sighand()删除信号处理函数。

5. 调用\_\_unhash_process() ,该函数依次执行下面的操作：
   - 变量nr_threads减1。
   - 两次调用detach_pid()，分别从PIDTYPE_PID和PIDTYPE_TGID类型的PID散列表中删除进程描述符。
   - 如果进程是线程组的领头进程，那么再调用两次detach_pid()，从PIDTYPE_PGID和PIDTYPE_SID类型的散列表中删除进程描述符。
   - 用宏REMOVE_LINKS从进程链表中解除进程描述符的链接。
6. 如果进程不是线程组的领头进程，领头进程处于僵死状态，而且进程是线程组的最后一个成员，则该函数向领头进程的父进程发送一个信号，通知它进程已死亡。
7. 调用sched_exit()函数来调整父进程的时间片(这一步在逻辑上作为对copy_process()第17步的补充)。
8. 调用put_task_struct()递减进程描述符的使用计数器，如果计数器变为0，则函数终止所有残留的对进程的引用。
   - 递减进程所有者的user_struct数据结构的使用计数器(\_\_count字段)(参见copy_process()的第5步)，如果使用计数器变为0，就释放该数据结构。
   - 释放进程描述符以及thread_info描述符和内核态堆栈所占用的内存区域。

## 第4章 中断与异常

中断(interrupt)通常被定义为一个事件，该事件改变处理器执行的指令顺序。这样的事件与CPU芯片内外部硬件电路产生的电信号相对应。

中断通常分为同步(synchronous)中断和异步(asynchronous)中断：

- 同步中断是当指令执行时由CPU控制单元产生的，之所以称为同步，是因为只有在一条指令终止执行后CPU才会发出中断。
- 异步中断是由其他硬件设备依照CPU时钟信号随机产生的。

在Intel微处理器手册中，把同步和异步中断分别称为异常(exception)和中断(interrupt)。我们也采用这种分类，当然有时我们也用术语“中断信号”指这两种类型(同步及异步)。

中断是由间隔定时器和I/O设备产生的，例如，用户的一次按键会引起一个中断。

异常是由程序的错误产生的，或者是由内核必须处理的异常条件产生的。第一种情况下，内核通过发送一个每个Unix程序员都熟悉的信号来处理异常。第二种情况下，内核执行恢复异常需要的所有步骤，例如缺页，或对内核服务的一个请求（通过一条int或sysenter指令）。

我们在下一节描述引入信号的动机，以此开始进行学习。然后，说明由I/O设备产生的著名IRQ(Interrupt ReQuest)如何引起中断，我们将详细讨论x86微处理器如何在硬件级处理中断和异常。接下来，我们将在“初始化中断描述符表”一节阐明Linux如何初始化Intel中断结构必需的所有数据结构。剩余的3节描述Linux如何在软件级处理中断信号。

### 中断信号的作用

顾名思义，中断信号提供了一种特殊的方式，使处理器转而去运行正常控制流之外的代码。当一个中断信号到达时，CPU必须停止它当前正在做的事情，并且切换到一个新的活动。为了做到这一点，就要在内核态堆栈保存程序计数器的当前值（即eip和cs寄存器的内容），并把与中断类型相关的一个地址放进程序计数器。

在本章，有些事情会使你想起在前一章描述的上下文切换，这发生在内核用一个迸程替换另一个进程时。但是，中断处理与进程切换有一个明显的差异：由中断或异常处理程序执行的代码不是一个进程。更确切地说，它是一个内核控制路径，代表中断发生时正在运行的进程执行(参见本章“中断和异常处理程序的嵌套执行”一节)。作为一个内核控制路径，中断处理程序比一个进程要“轻”（light）(中断的上下文很少，建立或终止中断处理需要的时间很少)。

中断处理是由内核执行的最敏感的任务之一，因为它必须满足下列约束：

- 当内核正打算去完成一些别的事情时，中断随时会到来。因此，内核的目标就是让中断尽可能快地处理完，尽其所能把更多的处理向后推迟。例如，假设一个数据块已到达了网线，当硬件中断内核时，内核只简单地标志数据到来了，让处理器恢复到它以前运行的状态。其余的处理稍后再进行(如把数据移入一个缓冲区，它的接收进程可以在缓冲区找到数据并恢复这个进程的执行)。因此，内核响应中断后需要进行的操作分为两部分：关键而紧急的部分，内核立即执行；其余推迟的部分，内核随后执行

- 因为中断随时会到来，所以内核可能正在处理其中的一个中断时。另一个中断(不同类型)又发生了。应该尽可能多地允许这种情况发生。因为这能维持更多的I/O设备处于忙状态(参见“中断和异常处理程序的嵌套执行”一节)。因此，中断处理程序必须编写成使相应的内核控制路径能以嵌套的方式执行。当最后一个内核控制路径终止时，内核必须能恢复被中断进程的执行，或者。如果中断信号已导致了重新调度，内核能切换到另外的进程。

- 尽管内核在处理前一个中断时可以接受一个新的中断，但在内核代码中还是存在一些临界区，在临界区中，中断必须被禁止。必须尽可能地限制这样的临界区，因为根据以前的要求，内核，尤其是中断处理程序，应该在大部分时间内以开中断的方式运行。

### 中断和异常

Intel文档把中断和异常分为以下几类：

- 中断：

  可屏蔽中断（maskable interrupt）

  ​	根据中断允许标志的设置来判断CPU是否能响应中断请求。

  ​	I/O设备发出的所有中断请求(IRQ)都产生可屏蔽中断。可屏蔽中断可以处于两种状态：屏蔽的(masked)或非屏蔽的(unmasked)。一个屏蔽的中断只要还是屏蔽的，控制单元就忽略它。

  非屏蔽中断(nonmasked Interrupt)

  ​	不受中断允许标志的影响，不能用软件进行屏蔽。

  ​	只有几个危急事件(如硬件故障)才引起非屏蔽中浙。非屏蔽中断总是由CPU辨认。

- 异常：

  - 处理器探测异常(processor-detected exception)

    当CPU执行指令时探测到的一个反常条件所产生的异常。可以进一步分为三组，这取决于CPU控制单元产生异常时保存在内核态堆栈eip寄存器中的值。

    - 故障(fault)

      通常可以纠正；一旦纠正，程序就可以在不失连贯性的情况下重新开始。保存在eip中的值是引起故障的指令地址，因此，当异常处理程序终止时，那条指令会被重新执行。我们将在第九章的“缺页异常处理程序”一节中看到，只要处理程序能纠正引起异常的反常条件，重新执行同一指令就是必要的。

    - 陷阱(trap)

      在陷阱指令执行后立即报告；内核把控制权返回给程序后就可以继续它的执行而不失连贯性。保存在eip中的值是一个随后要执行的指令地址。只有当没有必要重新执行已终止的指令时，才触发陷阱。陷阱的主要用途就是为了调试程序。在这种情况下，中断信号的作用是通知调试程序一条特殊指令已被执行(例如到了一个程序内的断点)。一旦用户检查到调试程序所提供的数据，它就可能要求被调试程序从下一条指令重新开始执行。

    - 异常中止(abort)

      发生一个严重的错误，控制单元出了问题，不能在eip寄存器中保存引起异常的指令所在的确切位置。异常中止用于报告严重的错误，如硬件故璋或系统表中无效的值或不一致的值。由控制单元发送的这个中断信号是紧急信号，用来把控制权切换到相应的异常中止处理程序，这个异常中止处理程序除了强制受影响的进程终止外，没有别的选择。


  - 编程异常(programmed exception)

    在编程者发出请求时发生。是由int或int3指令触发的；当into(检查溢出)和bound(检查地址出界)指令检查的条件不为真时，也引起编程异常。控制单元把编程异常作为陷阱来处理。编程异常通常也叫做软中断(software interrupt)。这样的异常有两种常用的用途：执行系统调用及给调试程序通报一个特定的事件(参见第十章)。

  每个中断和异常是由0~255之间的一个数来标识。因为一些未知的原因，Intel把这个8位的无符号整数叫做一个向量(vector)。非屏蔽中断的向量和异常的向量是固定的，而可屏蔽中断的向量可以通过对中断控制器的编程来改变(参见下一节)。

#### IRQ和中断

每个能够发出中断请求的硬件设备控制器都有一条名为IRQ(Interrupt ReQuest)的输出线(复杂的设备有几条IRQ线，PCI卡可能使用多达4条IRQ线)所有现有的IRQ线都与一个名为可编程中断控制器(Programmable Interrupt Controuler, PIC)的硬件电路的输入引脚相连。可编程中断控制器执行下列动作：

1. 监视IRQ线，检查产生的信号(raised signal)。如果有条或两条以上的IRQ线上产生信号，就选择引脚编号较小的IRQ线。
2. 如果一个引发信号出现在IRQ线上：
   - 把接收到的引发信号转换成对应的向量。
   - 把这个向最存放在中断控制器的一个I/O端口，从而允许CPU通过数据总线读此向量。
   - 把引发信号发送到处理器的INTR引脚，即产生一个中断。 
   - 等待，直到CPU通过把这个中断信号写进可编程中断控制器的一个I/O端口来确认它；当这种情况发生时，清INTR线。
3. 返回到第1步。


IRQ线是从0开始顺序编号的，因此，第一条IRQ线通常表示成IRQ0。与IRQn关联的Intel的缺省向量是n+32。如前所述，通过向中断控制器端口发布合适的指令，就可以修改IRQ和向量之间的映射。

可以有选择地禁止每条IRQ线。因此，可以对PIC编程从而禁止IRQ，也就是说，可以告诉PIC停止对给定的IRQ线发布中断，或者激活它们。禁止的中断是丢失不了的，它们一旦被激活，PIC就又把它们发送到CPU。这个特点被大多数中断处理程序使用，因为这允许中断处理程序逐次地处理同一类型的IRQ。

有选择地激活/禁止IRQ线不同于可屏蔽中断的全局屏蔽/非屏蔽。当eflags寄存器的IF标志被清0时，由PIC发布的每个可屏蔽中断都由CPU暂时忽略。c1i和sti汇编指令分别清除和设置该标志。

传统的PIC是由两片8259A风格的外部芯片以“级联’的方式连接在一起的。每个芯片可以处理多达8个不同的IRQ输人线。因为从PIC的INT输出线连接到主P1C的IRQ2引脚，因此。可用iRQ线的个数限制为15。

#### 高级可编程中断控制器

以前的描述仅涉及为单处理器系统设计的PIC。如果系统只有一个单独的CPU，那么主PIC的输出线以直截了当的方式连接到CPU的INTR引脚。然而，如果系统中包含两个或多个CPU，那么这种方式不再有效，因而需要更复杂的PIC。

为了充分发挥SMP体系结构的并行性，能够把中断传递给系统中的每个CPU至关重要。基于此理由，Intel从Pentium IlI开始引人了一种名为I/O高级可编程控制器(I/O Advanced Programmable Interrupt Controller， I/O APIC)的新组件。用以代替老式的8259A可编程中断控制器。新近的主板为了支持以前的操作系统都包括两种芯片。此外，x86微处理器当前所有的CPU都含有一个本地APIC。每个本地APIC都有32位的寄存器、一个内部时钟、一个本地定时设备及为本地APIC中断保留的两条额外的IRQ线LINT0和LINT1。所有本地APIC都连接到一个外部I/O APIC，形成一个多APIC的系统。

下图以示意图的方式显示了一个多APIC系统的结构。一条APIC总线把“前端”I/O APIC连接到本地APIC。来自设备的IRQ线连接到I/O APIC，因此，相对于本地APIC，I/O APIC起路由器的作用。在Pentium III和早期处理器的母板上，APIC总线是一个串行三线总线;从Pentium 4开始，APIC总线通过系统总线来实现。不过，因为APIC总线及其信息对软件是不可见的，因此，我们不做进一步的详细讨论。

![img](https://raw.githubusercontent.com/LiuChengqian90/Study-notes/8a979b880f37434c7b830c57eebf765b41069aab/image/Linux/%E5%A4%9AAPIC%E7%B3%BB%E7%BB%9F.jpg)


I/O APIC的组成为：一组24条IRQ线、一张24项的中断重定向表(Interrupt Redirection Table)、可编程寄存器，以及通过APIC总线发送和接收APIC信息的一个信息单元。与8259A的IRQ引脚不同，中断优先级并不与引脚号相关联：中断重定向表中的每项都可以被单独编程以指明中断向量和优先级、目标处理器及选择处理器的方式。重定向表中的信息用于把每个外部IRQ信号转换为一条消息，然后，通过APIC总线把消息发送给一个或多个本地APIC单元。

来自外部硬件设备的中断请求以两种方式在可用CPU之间分发：

- 静态分发

  IPQ信号传递给重定向表相应项中所列出的本地APIC。中断立即传递给一个特定的CPU，或一组CPU，或所有CPU（广播方式）。

- 动态分发

  如果处理器正在执行最低优先级的进程，IRQ信号就传递给这种处理器的本地APIC。每个本地APIC都有一个可编程任务优先级寄存器（task priority register,TPR），TPR用来计算当前运行进程的优先级。Intel希望在操作系统内核中通过每次进程切换对这个寄存器进行修改。

  如果两个或多个CPU共享最低优先级，就利用仲裁（arbitration）技术在这些CPU之间分配负荷。在本地APIC的仲裁优先级寄存器中，给每个CPU都分配一个0（最低）~15（最高）范围内的值。

  每当中断传递给一个CPU时，其相应的仲裁优先级就主动置为0，而其他每个CPU的仲裁优先级都增加1。当仲裁优先级寄存器大于15时，就把它置为获胜CPU的前一个仲裁优先级加1。因此，中断以轮转方式在CPU之间分发，切具有相同的任务优先级（Pentium 4 本地APIC中没有仲裁优先级寄存器；仲裁机制隐藏在总线仲裁电路中。Intel手册中声明，如果操作系统内核不能有规律地更新任务优先级寄存器，那么性能可能就达不到最优，因为中断有可能由同一个CPU处理）。

除了在处理器之间分发中断外，多APIC系统还允许CPU产生处理器间中断(interprocessor interrupt)。当一个CPU希望把中断发给另一个CPU时，它就在白己本地APIC的中断指令寄存器(Interrupt Command Register，ICR)中存放这个中断向量和目标本地APIC的标识符。然后，通过APIC总线向目标本地APIC发送一条消息，从而向自己的CPU发出一个相应的中断。

处理器间中断(简称IPI)是SMP体系结构至关重要的组成部分，井由Linux有效地用来在CPu之间交换信息。

目前大部分单处理器系统都包含一个I/O APIC芯片，可以用以下两种方式对这种芯片进行配置：

- 作为一种标准8259A方式的外部PIC连接到CPU。本地APIC被禁止，两条LINT0和LINT1本地IRQ线分别配置为INTR和NMI引脚。
- 作为一种标淮外部I/O APIC。本地APIC被激活，且所有的外部中断都通过I/O APIC接收。

#### 异常

x86微处理器发布了大约20种不同的异常(具体数字依赖于处理器模型)。内核必须为每种异常提供一个专门的异常处理程序。对于某些异常，CPU控制单元在开始执行异常处理程序前会产生一个硬件出错码(hardware error code)。并且压入内核态堆栈。

下表给出了在x86处理器中可以找到的异常的向量、名字、类型及其简单描述。更多的信息可以在Intel的技术文档中找到。

| 编号   | 类型                                | 描述                                       |
| ---- | --------------------------------- | ---------------------------------------- |
| 0    | Divide error(故障)                  | 当一个程序试图执行整数被0除操作时产生。                     |
| 1    | Dehug(陷阱或故障)                      | 产生于：①设置eflags的TF标志时(对于实现调试程序的单步执行是相当有用的)，②一条指令或操作数的地址落在一个活动debug寄存器的范围之内(参见第三章的“硬件上下文”一节)。 |
| 2    | 未用                                | 为非屏蔽中断保留(利用NMI引脚的那些中断)。                  |
| 3    | Breakpoint(陷阱)                    | 由int3(断点)指令(通常由debugger插入)引起。            |
| 4    | Overflow(陷阱)                      | 当eflags的OF(overflow)标志被设置时，into(检查溢出)指令披执行。 |
| 5    | Bounds check(故障)                  | 对于有效地址范围之外的操作数，bound(检查地址边界)指令披执行。       |
| 6    | lnvalid opcode(故障)                | CPU执行单元检测到一个无效的操作码(决定执行操作的机器指令部分)。       |
| 7    | Deviee not available(故障)          | 随着cr0的ts标志披设置，ESCAPE、MMX或XMM指令披执行。       |
| 8    | Double fault(异常中止)                | 正常情况下，当CPU正试图为前一个异常调用处理程序时，同时又检测到一个异常，两个异常能被串行地处理。然而，在少数情况下，处理器不能串行地处理它们，因而产生这种异常。 |
| 9    | Coprocessor segment overrun(异常中止) | 因外部的数学协处理器引起的问题(仅用于80386微处理器)。           |
| 10   | Invalid TSS(故障)                   | CPU试图让一个上下文切换到有无效的TSS的进程。                |
| 11   | Segment not present(故障)           | 引用一个不存在的内存段(段描述符的Segment-Present标志被清0)。  |
| 12   | Stack segment fault(故障)           | 试图超过栈段界限的指令，或者由ss标识的段不在内存。               |
| 13   | General protection(故障)            | 违反了x86保护模式下的保护规则之一。                      |
| 14   | Page fault(故障)                    | 寻址的页不在内存，相应的页表项为空，或者违反了一种分页保护机制。         |
| 15   | 由Intel保留                          |                                          |
| 16   | Floating point error(故障)          | 集成到CPU芯片中的浮点单元用信号通知一个错误情形，如数字溢出，或被0除（x86微处理器也产生这个异常，发生在执行一个带符号的除法运算，而运算结果不能以带符号整数存放的时候）。 |
| 17   | Alignmenr check(故障)               | 操作数的地址没有被正确地对齐(例如，一个长整数的地址不是4的倍数)。       |
| 18   | Machine check(异常中止)               | 机器检查机制检测到一个CPU错误或总线错误。                   |
| 19   | SIMD floating point exception(故障) | 集成到CPU芯片中的SSE或SSE2单元对浮点操作用信号通知一个错误情形。    |

20~31这些值由Intel留作将来开发。如下表所示，每个异常都由专门的异常处理程序来处理，它们通常把一个Unix信号发送到引起异常的进程。

| 编号   | 异常                          | 异常处理程序                        | 信号      |
| ---- | --------------------------- | ----------------------------- | ------- |
| 0    | Divide error                | divide_error                  | SIGFPE  |
| 1    | Debug                       | debug()                       | SIGTRAP |
| 2    | NMI                         | nmi()                         | None    |
| 3    | Breakpoint                  | int3()                        | SIGTRAP |
| 4    | Overflow                    | overflow()                    | SIGSEGV |
| 5    | Bounds check                | bounds()                      | SIGSEGV |
| 6    | Invalid opcode              | invalid_op()                  | SIGILL  |
| 7    | Device not available        | device_not_available()        | None    |
| 8    | Double fault                | doublefault_fn()              | None    |
| 9    | Coprocessor segment overrun | coprocessor_segment_overrun() | SIGFPE  |
| 10   | Invalid TSS                 | invalid_tss()                 | SIGSEGV |
| 11   | Segment not present         | segment_not_present()         | SIGBUS  |
| 12   | Stack exception             | stack_segment()               | SIGBUS  |
| 13   | General protection          | general_protection()          | SIGSEGV |
| 14   | Page fault                  | page_fault()                  | SIGSEGV |
| 15   | Intel reserved              | None                          | None    |
| 16   | Floating point error        | coprocessor_error()           | SIGFPE  |
| 17   | Alignment check             | alignment_check()             | SIGSEGV |
| 18   | Machine check               | machine_check()               | None    |
| 19   | SIMD floating point         | simd_coprocessor_error()      | SIGFPE  |

#### 中断描述符表

中断描述符表（Interrupt Descriptor Table，IDT）是一个系统表，它与每一个中断或异常向量相联系，每一个向量在表中有相应的中断或异常处理程序的入口地址。内核在允许中断发生前，必须适当地初始化IDT。

之前介绍了GDT和LDT，IDT的格式与这两种表的格式非常相似，表中的每一项对应一个中断或异常向量，每个向量由8个字节组成。因此，最多需要256 * 8 = 2048 字节来存放IDT。

idtr CPU寄存器使IDT可以位于内存的任何地方，它指定IDT的线性基地址及其限制（最大长度）。在允许中断之前，必须用lidt汇编指令初始化 idtr。

IDT包含三种类型的描述符，下图显示了每种描述符中的64位的含义。尤其值得注意的是，在40~43位的Type字段的值表示描述符的类型。

![img](https://raw.githubusercontent.com/LiuChengqian90/Study-notes/07dfc3dc74adb4cd5d6ac6c6422c4697b7194b39/image/Linux/%E9%97%A8%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%A0%BC%E5%BC%8F.jpg)

这些描述符是：

- 任务门(task gate)

  当中断信号发生时，必须取代当前进程的那个进程的TSS选择符存放在任务门中。

- 中断门(interrupt gate)

  包含段选择符和中断或异常处理程序的段内偏移量。当控制权转移到一个适当的段时，处理器清IF标志，从而关闭将来会发生的可屏蔽中断。

- 陷阱门(Trap gate)

  与中断门相似，只是控制权传递到一个适当的段时处理器不修改IF标志。

之后将在“中断门、陷阱门及系统门”一节看到，Linux利用中断门处理中断，利用陷阱门处理异常（“Double fault”异常是唯一由任务门处理的异常，它表示一种内核错误）。

#### 中断和异常的硬件处理

我们现在描述CPU控制单元如何处理中断和异常。我们假定内核已披初始化，因此，CPU在保护模式下运行。

当执行了一条指令后，cs和eip这对寄存器包含下一条将要执行的指令的逻辑地址。在处理那条指令之前，控制单元会检查在运行前一条指令时是否己经发生了一个中断或异常。如果发生了一个中断或异常。那么控制单元执行下列操作：

1. 确定与中断或异常关联的向量i(0 <= i <= 5)。

2. 读由idtr寄存器指向的IDT表中的第i项(在下面的描述中，我们假定IDT表项中包含的是一个中断门或一个陷阱门)。

3. 从gdtr寄存器获得GDT的基地址，井在GDT中查找，以读取IDT表项中的选择符所标识的段描述符。这个描述符指定中断或异常处理程序所在段的基地址。

4. 确信中断是由授权的(中断)发生源发出的。首先将当前特权级CPL(存放在es寄存器的低两位)与段描述符(存放在GDT中)的描述符特权级DPL比较，如果CPL小于DPL，就产生一个“General protection”异常，因为中断处理程序的特权不能低于引起中断的程序的特权。对于编程异常，则做进一步的安全检查：比较CPL与处于IDT中的门描述符的DPL，如果DPL小于CPL，就产生一个“General protection"异常。这最后一个检查可以避免用户应用程序访问特殊的陷阱门或中断门。

5. 检查是否发生了特权级的变化。也就是说，CPL是否不同于所选择的段描述符的DPL。如果是，控制单元必须开始使用与新的特权级相关的栈。通过执行以下步骤来做到这点：
   - 读tr寄存器，以访问运行进程的TSS段。
   - 用与新特权级相关的栈段和栈指针的正确值装载ss和esp寄存器。这些值可以在T5S中找到。
   - 在新的栈中保存ss和esp以前的值，这些值定义了与旧特权级相关的栈的逻辑地址。
6. 如果故障已发生，用引起异常的指令地址装载cs和eip寄存器，从而使得这条指令能再次被执行。
7. 在栈中保存eflags， cs及eip的内容。
8. 如果异常产生了一个硬件出错码，则将它保存在栈中。
9. 装载cs和eip寄存器，其值分别是IDT表中第i项门描述符的段选择符和偏移量字段。这些值给出了中断或者异常处理程序的第一条指令的逻辑地址。

控制单元所执行的最后一步就是跳转到中断或者异常处理程序。换句话说，处理完中断信号后，控制单元所执行的指令就是被选中处理程序的第一条指令。

中断或异常被处理完后，相应的处理程序必须产生一条iret指令，把控制权转交给被中断的进程，这将迫使控制单元：

1. 用保存在栈中的值装载cs、eip或eflags寄存器。如果一个硬件出错码曾被压入栈中，并且在eip内容的上面，那么，执行iret指令前必须先弹出这个硬件出错码。
2. 检查处理程序的CPL是否等干cs中最低两位的值(这意味着被中断的进程与处理程序运行在同一特权级)。如果是，iret终止执行；否则，转人下一步。

3. 从栈中装载ss和esp寄存器，因此，返回到与旧特权级相关的栈。

4. 检查ds、es、fs及gs段寄存器的内容，如果其中一个寄存器包含的选择符是一个段描述符，并且其DPL值小于CPL，那么，清相应的段寄存器。控制单元这么做是为了禁止用户态的程序(CPL=3)利用内核以前所用的段寄存器(DPL=0)。如果不清这些寄存器，怀有恶意的用户态程序就可能利用它们来访问内核地址空间。

### 中断和异常处理程序的嵌套执行

每个中断或异常都会引起一个内核控制路径，或者说代表当前进程在内核态执行单独的指令序列。例如：当I/O设备发出一个中断时，相应的内核控制路径的第一部分指令就是那些把寄存器的内容保存在内核堆栈的指令，而最后一部分指令就是恢复寄存器内容并让CPU返回到用户态的那些指令。

内核控制路径可以任意嵌套。一个中断处理程序可以被另一个中断处理程序“中断”，因此引起内核控制路径的嵌套执行。其结果是，对中断进行处理的内核控制路径，其最后一部分指令并不总能使当前进程返回到用户态:如果嵌套深度大于1，这些指令将执行上次被打断的内核控制路径，此时的CPU依然运行在内核态。

允许内核控制路径嵌套执行必须付出代价，那就是中断处理程序必须永不阻塞，换句话说，中断处理程序运行期间不能发生进程切换。事实上，嵌套的内核控制路径恢复执行时需要的所有数据都存放在内核态堆栈中，这个栈毫无疑义的属于当前进程。

假定内核没有hug，那么大多数异常就只在CPU处于用户态时发生。事实上，异常要么是由编程错误引起。要么是由调试程序触发。然而，"Page Fautt(缺页)"异常发生在内核态。这发生在当进程试图对属于其地址空间的页进行寻址，而该页现在不在RAM中时。当处理这样的一个异常时。内核可以挂起当前进程，并用另一个进程代替它，直到请求的页可以使用为止。只要被挂起的进程又获得处理器，处理缺页异常的内核控制路径就恢复执行。

因为“Page Fault”异常处理程序从不进一步引起异常，所以与异常相关的至多两个内核控制路径(第一个由系统调用引起，第二个由缺页引起)会堆叠在一起，一个在另一个之上。

与异常形成对照的是，尽管处理中断的内核控制路径代表当前进程运行，但由I/O设备产生的中断并不引用当前进程的专有数据结构。事实上，当一个给定的中断发生时，要预测哪个进程将会运行是不可能的。

一个中断处理程序既可以抢占其他的中断处理程序，也可以抢占异常处理程序。相反，异常处理程序从不抢占中断处理程序。在核态能触发的唯一异常就是刚刚描述的缺页异常。但是，中断处理程序从不执行可以导致缺页(因此意味着进程切换)的操作。

基于以下两个主要原因。Linux交错执行内核控制路径：

- 为了提高可编程中断控制器和设备控制器的吞吐量。假定设备控制器在一条IRQ线上产生了一个信号，PIC把这个信号转换成一个外部中断，然后PIC和设备控制器保持阻塞，一直到PIC从CPU处接收到一条应答信息。由于内核控制路径的交错执行，内核即使正在处理前一个中断，也能发送应答。
- 为了实现一种没有优先级的中断模型。因为每个中断处理程序都可以被另一个中断处理程序延缓，因此，在硬件设备之间没必要建立预定义优先级。这就简化了内核代码，提高了内核的可移植性。

在多处理器系统上，几个内核控制路径可以并发执行。此外，与异常相关的内核控制路径可以开始在一个CPU上执行，并且由于进程切换而移往另一个CPU获执行。

### 初始化中断描述符表

内核启动中断之前，必须把IDT表的初始化地址装到idtr寄存器，并初始化表中的每一项。这项工作是在初始化系统时完成的(参见附录一)。

int指令允许用户态进程发出一个中断信号，其值可以是0~255的任意一个向量。因此，为了防止用户通过int指令模拟非法的中断和异常，IDT的初始化必须非常小心。这可以通过把中断或陷阱门描述符的DPL字段设置成0来实现。如果进程试图发出其中的一个中断信号，控制单元将检查出CPL的值与DPL字段有冲突，并且产生一个"General protection”异常。

然而，在少数情况下，用户态进程必须能发出一个编程异常。为此.只要把中断或陷阱门描述符的DPL字段设置成3，即特权级尽可能一样高就足够了。

现在，让我们来看一扩Linux是如何实现这种策略的。

#### 中断门、陷阱门及系统门

Intel提供了三种类型的中断描述符：任务门、中断门及陷阱门描述符。Linux使用与Intel稍有不同的细目分类和术语，把它们如下进行分类：

| 门描述符                         | 简介                                       |
| ---------------------------- | ---------------------------------------- |
| 中断门(interrupt gute)          | 用户态的进程不能访问的一个Intel中断门(门的DPL字段为0)。所有的Linux中断处理程序都通过中断门激活，并全部限制在内核态。 |
| 系统门(system gate)             | 用户态的进程可以访问的一个Intel陷阱门(门的DPL字段为3)。通过系统门来激活三个Linux异常处理程序，它们的向量是4、5及128,因此，在用户态下，可以发布into, bound及int $0x80三条汇编语言指令。 |
| 系统中断门(syrtem interrupt gate) | 能够被用户态进程访问的Intel中断门(门的DPL.字段为3)。与向量3相关的异常处理程序是由系统中断门激活的，因此，在用户态可以使用汇编语言指令int3。 |
| 陷阱门(trap gate)               | 用户态的进程不能访问的一个Intel陷阱门(门的DPL字段为0)。大部分Linux异常处理程序都通过陷阱门来激活。 |
| 任务门(task gate)               | 不能被用户态进程访问的Intel任务门(门的DPL字段为0), Linux对"Double fault"异常的处理程序是由任务门激活的。 |

下列体系结构相关的函数用来在IDT中插入门：

| 函数                           | 简介                                       |
| ---------------------------- | ---------------------------------------- |
| set_intr_gate(n,addr)        | 在IDT的第n个表项插入一个中断门。门中的段选择符设置成内核代码的段选择符，偏移量设置为中断处理程序的地址addr,  DPL字段设置为0。 |
| set_system_gate(n,addr)      | 在IDT的第n个表项插入一个陷阱门。门中的段选择符设置成内核代码的段选择符，偏移量设置为异常处理程序的地址addr, DPL字段设置为3。 |
| set_system_intr_gate(n,addr) | 在IDT的第n个表项插入一个中断门。门中的段选择符设置成内核代码的段选择符，偏移量设置为异常处理程序的地址addr, DPL字段设置为3。 |
| set_trap_gate(n,addr)        | 与前一个函数类似，只不过DPL的字段设置成0。                  |
| set_task_gate(n,addr)        | 在IDT的第n个表项插入一个中断门。门中的段选择符中存放一个TSS的全局描述符表的指针，该TSS中包含要被激活的函数。偏移量设置为0，而DPL字段设置为3。 |

#### IDT的初步初始化

当计算机还运行在实模式时，IDT被初始化并由BIOS例程使用。然而，一旦Linux接管，IDT就被移到RAM的另一个区域，并进行第二次初始化，因为Linux没有利用任何BIOS例程(参见附录一)。

IDT存放在idt_table表中，有256个表项。6字节的idt_descr变量指定了IDT的大小和它的地址，只有当内核用lidt汇编指令初始化idtr寄存器时才用到这个变量(一些旧的Pentium模式有声名狼藉的“f00f”bug，能让用户程序冻结系统。当Linux在这样的CPU上执行时，就使用工作区，而该工作区基于用指向实际IDT的只读固定映射线性地址初始化idtr寄存器)。

在内核初始化过程中，setup_idt()汇编语言函数用同一个中断门(即指向ignore_int()中断处理程序)来填充所有这256个idt_table表项：

```c
setup_idt:
	lea ignore_int, %edx
	movl $(__KERNEL_CS << 16), %eax
	movw %dx, %ax
	movw $0x8e00, %dx
	lea idt_table, %edi
	mov $256, %ecx
rp_sidt:
	movl %eax, (%edi)
    movl %edx, 4(%edi)
    addl $8, %edi
    dec %ecx
    jne rp_sidt
    ret
```

用汇编语言写成的ignore_int()中断处理程序，可以看作一个空的处理程序，它执行下列动作：

1. 在栈中保存一些寄存器的内容。

2. 调用printk()函数打印"Unknown interrupt"系统消息。

3. 从栈恢复寄存器的内容。

4. 执行iret指令以恢复被中断的程序。

ignore_int()处理程序应该从不被执行，在控制台或日志文件中出现的“Unknown interrupt"消息标志着要么是出现了一个硬件的问题(一个I/O设备正在产生没有预料到的中断)，要么就是出现了一个内核的问题(一个中断或异常未被适当地处理)。

紧接着这个预初始化，内核将在IDT中进行第二遍初始化，用有意义的陷阱和中断处理程序替换这个空处理程序。一旦这个过程完成，对控制单元产生的每个不同的异常，IDT都有一个专门的陷阱或系统门，而对于可编程中断控制器确认的每一个IRQ, IDT都将包含一个专门的中断门。

在接下来的两节中，将分别针对异常和中断来详细地说明这个工作是如何完成的。

### 异常处理

CPU产生的大部分异常都由Linux解释为出错条件。当其中一个异常发生时，内核就向引起异常的进程发送一个信号向它通知一个反常条件.例如，如果进程执行了一个被0除的操作，CPU就产生一个“Divide error”异常，并由相应的异常处理程序向当前进程发送一个SIGFPE信号，这个进程将采取若干必要的步骤来(从出错中)恢复或者中止运行(如果没有为这个信号设置处理程序的话)。

但是，在两种情况下，Linux利用CPU异常更有效地管理硬件资源。第一种情况已经在第三章“保存和加载FPU、MMX及XMM寄存器”一节描述过，"Device not available"异常与cr0寄存器的TS标志一起用来把新值装入浮点寄存器。第二种情况指的是“Page Fault”异常，该异常推迟给进程分配新的页框，直到不能再推迟为止。相应的处理程序比较复杂，因为异常可能表示一个错误条件，也可能不表示一个错误条件(参见第九章“缺页异常处理程序”一节)。

异常处理程序有一个标准的结构，由以下三部分组成：

1. 在内核堆栈中保存大多数寄存器的内容(这部分用汇编语言实现)。

2. 用高级的C函数处理异常。

3. 通过ret_from_exception()函数从异常处理程序退出。


为了利用异常，必须对IDT进行适当的初始化，使得每个被确认的异常都有一个异常处理程序。trap_init()函数的工作是将一些最终值(即处理异常的函数)插入到IDT的非屏蔽中断及异常表项中。这是由函数set_trap_gate()、set_intr_gate()、set_system_gate()、set_system_intr_gate()和set_task_gate()来完成的。

```c
set_trap_gate(0, &divide_error);
set_trap_gate(1, &debug);
set_intr_gate(2, &nmi);
set_system_intr_gate(3, &int3);
set_system_gate(4, &overflow);
set_system_gate(5, &bounds);
set_trap_gate(6, &invalid_op);
set_trap_gate(7, &device_not_available);
set_task_gate(8, 31);
set_trap_gate(9, &coprocessor_segment_overrun);
set_trap_gate(10, &invalid_TSS);
set_trap_gate(11, &segment_not_present);
set_trap_gate(12, &stack_segment);
set_trap_gate(13, &general_protection);
set_intr_gate(14, &page_fault);
set_trap_gate(16, &coprocessor_error);
set_trap_gate(17, alignment_check);
set_trap_gate(18, &machine_check);
set_trap_gate(19, &simd_coprocessor_error);
set_system_gate(128, &system_call);
```

由于“Double fault”异常表示内核有严重的非法操作，其处理是通过任务门而不是陷阱门或系统门来完成的，因而，试图显示寄存器值的异常处理程序井不确定esp寄存器
的值是否正确。产生这种异常的时候，CPU取出存放在IDT第8项中的任务门描述符，该描述符指向存放在GDT表第32项中的TSS段描述符。然后，CPU用TSS段中的相关值装载eip和esp寄存器，结果是：处理器在白己的私有栈上执行doublefault_fn()异常处理函数。

现在我们要考察一旦一个典型的异常处理程序被调用，它会做些什么。由于篇幅所限，我们对异常处理仅做粗略的描述，尤其是我们不涉及下面的内容：

1. 由一些处理函数发送给用户态进程的信号码(见第十一章中的表)

2. 内核运行在MS-DOS虚拟模式(VM86模式)时产生的异常，它们的处理是不同的。

3. “Debug”异常。

#### 为异常处理程序保存寄存器的值

让我们用handler_name来表示一个通用的异常处理程序的名字。(所有异常处理程序的实际名字都出现在前一部分的宏列表中)每一个异常处理程序都以下列的汇编指令开始：

```c
handler_name:
	pushl $0 /* only for some exceptions */
	pushl Sdo_handler_name
	jmp error_code
```


当异常发生时，如果控制单元没有自动地把一个硬件出错代码插人到栈中，相应的汇编语言片段会包含一条pushl $0指令，在栈中垫上一个空值。然后，把高级C函数的地址压进栈中。它的名字由异常处理程序名与do_前缀组成。

标号为error_code的汇编语言片段对所有的异常处理程序都是相同的，除了“Device not available”这一个异常(参见第三章的“保存和加载FPU、MMX及XMM寄存器”一节)。这段代码执行以下步骤：

1. 把高级C函数可能用到的寄存器保存在栈中。

2. 产生一条cld指令来清eflags的方向标志DF，以确保调用字符串指令(一条诸如rep;mmovsb这样的汇编语言“字符串指令”能够作用于整个（字符串）块)时会自动增加edi和esi寄存器的值。

3. 把栈中位于esp + 36处的硬件出错码拷贝到edx中，给栈中这一位置存上值-1，正如我们将在第十一章的“系统调用的重新执行”一节中所看到的那样，这个值用来把0x80异常与其他异常隔离开。

4. 把保存在栈中esp+32位置的do_handler_name()高级C函数的地址装入edi寄存器中，然后，在栈的这个位置写入es的值。

5. 把内核栈的当前栈顶拷贝到eax寄存器。这个地址表示内存单元的地址，在这个单元中存放的是第1步所保存的最后一个寄存器的值。

6. 把用户数据段的选择符拷贝到ds和es寄存器中。

7. 调用地址在edi中的高级C函数。


被调用的函数从eax和edx寄存器而不是从栈中接收参数。我们已经遇见过一个从CPU寄存器获取参数的函数\_\_switch_to()，在第三章“执行进程切换”一节我们讨论过这个函数。

#### 进入和离开异常处理程序

如前所述，执行异常处理程序的C函数名总是由do_前缀和处理程序名组成。其中的大部分函数把硬件出错码和异常向量保存在当前进程的描述符中，然后，向当前进程发送一个适当的信号。用代码描述如下：

```c
current->thread.error_code = error_code;
current->thread.trap_no = vector;
force_sig(sig_number, curent);
```

异常处理程序刚一终止，当前进程就关注这个信号。该信号要么在用户态由进程自己的信号处理程序〔如果存在的话)来处理，要么由内核来处理。在后面这种情况下，内核一般会杀死这个进程(参见第十章)。异常处理程序发送的信号已在之前列出。

异常处理程序总是检查异常是发生在用户态还是在内核态，在后一种情况下，还要检查是否由系统调用的无效参数引起。将在第十章“动态地址检查：修正代码”一节描述内核如何防御自己受无效的系统调用参数攻击。出现在内核态的任何其他异常都是由于内核的bug引起的。在这种情况下，异常处理程序认为是内核行为失常了。为了避免硬盘上的数据崩溃，处理程序调用die()函数，该函数在控制台上打印出所有CPU寄存器的内容（这种转储叫做kernel oops），并调用do_exit()来终止当前进程（参见第三章“进程终止”一节）。

当执行异常处理的C函数终止时，程序执行一条jmp指令以跳转到ret_from_exceptionf()函数。这个函数将在后面的“从中断和异常返回”一节中进行描述。

### 中断处理

正如前面解释的那样，内核只要给引起异常的进程发送一个Unix信号就能处理大多数异常。因此，要采取的行动被延迟，直到进程接收到这个信号。所以，内核能很快地处理异常。

这种方法并不适合中断，因为经常会出现一个进程(例如，一个请求数据传输的进程)被挂起好久后中断才到达的情况，因此，一个完全无关的进程可能正在运行。所以，给当前进程发送一个Unix信号是毫无意义的。

中断处理依赖于中断类型。就我们的目的而言，我们将讨论三种主要的中断类型：

- I/O中断

  某些I/O设备需要关注；相应的中断处理程序必须查询设备以确定适当的操作过程。我们在后面“I/O中断处理”一节将描述这种中断。

- 时钟中断

  某种时钟(或者是一个本地APIC时钟，或者是一个外部时钟)产生一个中断；这种中断告诉内核一个固定的时间间隔已经过去。这些中断大部分是作为I/O中断来处理的；我们将在第六章讨论时钟中断的具体特征。

- 处理器间中断

  多处理器系统中一个CPU对另一个CPU发出一个中断。我们在后面“处理器间中断处理”一节将讨论这种中断。

#### I/O中断处理

一般而言，I/O中断处理程序必须足够灵活以给多个设备同时提供服务。例如在PCl总线的体系结构中，几个设备可以共享同一个IRQ线。这就意味着仅仅中断向量不能说明所有问题。在之前表所示的例子中，同一个向量43既分配给USB端口，也分配给声卡。然而，在老式PC体系结构(像ISA)中发现的一些硬件设备，当它们的IRQ与其他设备共享时，就不能可靠地运转。

中断处理程序的灵活性是以两种不同的方式实现的，讨论如下：

- IRQ共享

  中断处理程序执行多个中断服务例程(intrerrupr service routine, ISR)。每个ISR是一个与单独设备(共享IRQ线)相关的函数。因为不可能预先知道哪个特定的设备产生IRQ，因此，每个ISR都被执行，以验证它的设备是否需要关注；如果是，当设备产生中断时，就执行需要执行的所有操作。

- IRQ动态分配

  一条IRQ线在可能的最后时刻才与一个设备驱动程序相关联。例如，软盘设备的IRQ线只有在用户访问软盘设备时才被分配。这样，即使几个硬件设备并不共享IRQ线，同一个IRQ向量也可以由这几个设备在不同时刻使用(见本节最后一部分的讨论)。

当一个中断发生时，并不是所有的操作都具有相同的急迫性。事实上，把所有的操作都放进中断处理程序本身并不合适。需要时间长的、非重要的操作应该推后，因为当一个中断处理程序正在运行时，相应的IRQ线上发出的信号就披暂时忽路。更重要的是，中断处理程序是代表进程执行的，它所代表的进程必须总处于TASK_RUNNING状态，否则，就可能出现系统僵死情形。因此，中断处理程序不能执行任何阻塞过程，如磁盘I/O操作。因此，Linux把紧随中断要执行的操作分为三类：

- 紧急的(Critical)

  这样的操作诸如：对PIC应答中断，对PIC或设备控制器重编程，或者修改由设备和处理器同时访问的数据结构。这些都能披很快地执行，而之所以说它们是紧急的是因为它们必须披尽快地执行。紧急操作要在一个中断处理程序内立即执行，而且是在禁止可屏蔽中断的情况下。

- 非紧急的(Noncritical)

  这样的操作诸如：修改那些只有处理器才会访问的数据结构(例如，按下一个键后读扫描码)。这些操作也要很快地完成，因此，它们由中断处理程序立即执行，但必须是在开中断的情况下。

- 非紧急可延迟的(Noncritical deferrable)

  这样的操作诸如：把缓冲区的内容拷贝到某个进程的地址空间(例如，把键盘行缓冲区的内容发送到终端处理程序进程)。这些操作可能被延迟较长的时间间隔而不影响内核操作，有兴趣的进程将会等待数据。非紧急可延迟的操作由独立的函数来执行，将在“软中断及tasklet”一节讨论。

不管引起中断的电路种类如何，所有的I/O中断处理程序都执行四个相同的基本操作：

1. 在内核态堆栈中保存IRQ的值和寄存器的内容。
2. 为正在给IRQ线服务的PIC发送一个应答，这将允许PIC进一步发出中断。
3. 执行共享这个IRQ的所有设备的中断服务例程(ISR)。
4. 调到ret_from_intr()的地址后终止。

当中断发生时，需要用几个描述符来表示IRQ线的状态和需要执行的函数。下图以示意图的方式展示了处理一个中断的硬件电路和软件函数。下面几节讨论这些函数。![img](https://raw.githubusercontent.com/LiuChengqian90/Study-notes/989d5e9354ca4aeaab3e7d3bc95e0832fc4b2895/image/Linux/I-O%20%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86.jpg)

##### 中断向量

如下表所示，物理IRQ可以分配给32~238范围内的任何向量。不过，Linux使用向量128实现系统调用。

IBM PC兼容的体系结构要求，一些设备必须被静态地连接到指定的IRQ线，尤其是：

- 间隔定时设备必须连到IRQ0线(参见第六章)。

- 从8259A PIC须与IRQ2线相连(尽管现在有了更高级的PIC, Linux还是支持8259A 风格的PIC)。

- 必须把外部数学协处理器连接到IRQ13线(尽管最近的x86处理器不再使用这样的设备，但Linux仍然支持历史悠久的80386模型)。

- 一般而言，一个I/O设备可以连接到有限个IRQ线。(事实上，老式PC中，IRQ的共享是不可能的，由于IRQ与其他已经存在的硬件设备冲突，因此你不可能成功地安装一个新卡。)

| 向量范围                    | 用途                              |
| ----------------------- | ------------------------------- |
| 0 ~ 19 (0x0 ~ 0x13)     | 非屏蔽中断和异常                        |
| 20 ~ 31 (0x14 ~ 0x1f)   | Intel保留                         |
| 32 ~ 127 (0x20 ~ 0x7f)  | 外部中断（IRQ）                       |
| 128 (0x80)              | 用于系统调用的可编程异常（参见第十章）             |
| 129 ~ 238 (0x81 ~ 0xee) | 外部中断（IRQ）                       |
| 239 (0xef)              | 本地APIC时钟中断（参见第六章）               |
| 240 (0xf0)              | 本地APIC高温中断（在Pentium 4模型中引入）     |
| 241 ~ 250 (0xf0 ~ 0xfa) | 由Linux留作将来使用                    |
| 251 ~ 253 (0xfb ~ 0xff) | 处理器间中断（参见之后“处理器间中断处理”一节）        |
| 254 (0xfe)              | 本地APIC错误中断（当本地APIC检测到一个错误条件时产生） |
| 255 (0xff)              | 本地APIC伪中断（CPU屏蔽某个中断时产生）         |

为IRQ可配置设备选择一条线有三种方式：

- 设置一些硬件跳接器(仅适用于旧式设备卡)。
- 安装设备时执行一个实用程序。这样的程序可以让用户选择一个可用的IRQ号，或者探测系统自身以确定一个可用的IRQ号。
- 在系统启动时执行一个硬件协议。外设宣布它们准备使用哪些中断线，然后协商一个最终的值以尽可能减少冲突。该过程一旦完成，每个中断处理程序都通过访问设备某个I/O端口的函数，来读取所分配的IRQ。例如，遵循外设部件互连(Peripheral Component Interconnect, PCI)标准的设备的驱动程序利用一组函数，如pci_read_config_byte()访问设备的配置空间。

下表（把IRQ分配给I/O设备的一个例子）显示了设备和IRQ之间一种相当随意的安排，你或许能在某个PC中找到同样的排列。

| IRQ  | INT  | 硬件设备            |
| ---- | ---- | --------------- |
| 0    | 32   | 时钟              |
| 1    | 33   | 键盘              |
| 2    | 34   | PIC级联           |
| 3    | 35   | 第二串口            |
| 4    | 36   | 第一串口            |
| 6    | 38   | 软盘              |
| 8    | 40   | 系统时钟            |
| 10   | 42   | 网络接口            |
| 11   | 43   | USB端口、声卡        |
| 12   | 44   | PS/2鼠标          |
| 13   | 45   | 数学协处理器          |
| 14   | 46   | EIDE 磁盘控制器的一级链接 |
| 15   | 47   | EIDE 磁盘控制器的二级链接 |

内核必须在启用中断前发现IRQ号与I/O设备之间的对应，否则，内核在不知道哪个向量对应哪个设备(如SCSI硬盘)的情况下，怎么能处理来自这个设备的信号呢?IRQ号与I/Q设备之间的对应是在初始化每个设备驱动程序时建立的(参见第十三章)。

##### IRQ数据结构

当讨论到涉及状态转换的复杂操作时，首先了解关键数据存放在什么地方总是有益的。因此，本节将解释支持中断处理的数据结构以及怎样把它们放在各种描述符中。下图示意性地显示了几个主要描述符之间的关系，这些描述符表示IRQ线的状态(该图没有显示处理软中断及tasklet所需的数据结构，后面将对它们进行讨论。)

每个中断向量都有它自己的irq_desc_t描述符，其字段在下表中列出。所有的这些描述符组织在一起形成irq_desc数组。

![img](https://raw.githubusercontent.com/LiuChengqian90/Study-notes/922f994c8edbca0c00231ce45987c593a74e5d4e/image/Linux/IRQ%E6%8F%8F%E8%BF%B0%E7%AC%A6.jpg)

| 字段             | 说明                                       |
| -------------- | ---------------------------------------- |
| handler        | 指向PIC对象（hw_irq_controller描述符），它服务于IRQ线   |
| handler_data   | 指向PIC方法所使用的数据                            |
| action         | 标识当出现IRQ时要调用的中断服务例程。该字段指向IRQ的irqaction描述符链表的当一个元素。在本章后面将描述irqaction描述符 |
| status         | 描述IRQ线状态的一组标志（见下表）                       |
| depth          | 如果IRQ线被激活，则显示0；如果IRQ线被禁止了不止一次，则显示一个正数    |
| irq_count      | 中断计数器，统计IRQ线上发生中断的次数（仅在诊断时使用）            |
| irqs_unhandled | 对在IRQ线上发生的无法处理的中断进行计数（仅在诊断时使用）           |
| lock           | 用于串行访问IRQ描述符和PIC的自旋锁                     |

如果一个中断内核没有处理，那么这个中断就是意外中断。也就是说，与某个IRQ线相关的中断处理例程(ISR)不存在，或者与某个中断线相关的所有例程都识别不出是否是自己的硬件设备发出的中断。通常，内核检查从IRQ线接收的意外中断的数量，当这条IRQ线连接的有故障设备没完没了地发中断时，就禁用这条IRQ线。由于几个设备可能共享IRQ线，内核不会在每检测到一个意外中断时就立刻禁用IRQ线。更合适的办法是：内核把中断和意外中断的总次数分别存放在irq_desc_t描述符的irq_count和irqs_unhandled字段中，当第100000次中断产生时，如果意外中断的次数超过99900，内核才禁用这条IRQ线(即来自共享IRQ线的硬件设备的意外中断，比最近接收的100000次正常中断少101次。)

描述IRQ线状态的标志列在下表中。

| 标志名            | 描述                                       |
| -------------- | ---------------------------------------- |
| IRQ_INPROGRESS | IRQ的一个处理程序正在执行                           |
| IRQ_DISABLED   | 由一个设备驱动程序故意地禁用IRQ线                       |
| IRQ_PENDING    | 一个IRQ已经出现在线上，它的出现也已对PIC做出应答，但是内核还没有为它提供服务 |
| IRQ_REPLAY     | IRQ线已被禁用，但是前一个出现的IRQ还没有对PIC做出应答          |
| IRQ_AUTODETECT | 内核在执行硬件设备探测时使用IRQ线                       |
| IRQ_WAITING    | 内核在执行硬件设备探测时使用IRQ线；此外，相应的中断还没有产生         |
| IRQ_LEVEL      | 在x86架构上没有使用                              |
| IRQ_MASKED     | 未使用                                      |
| IRQ_PER_CPU    | 在x86架构上没有使用                              |

irq_desc_t描述符的depth字段和IRQ_DISABLED标志表示IRQ线是否被禁用。每次调用disable_irq()或disable_irq_nosync()函数，depth字段的值增加，如果depth等于0，函数禁用IRQ线并设置它的IRQ_DISABLED标志(disable_irq(n)一直等待，直到在其他CPU上为IRQn运行的所有中断处理程序都完成返回)，相反，每当调用enable_irq()函数，depth字段的值减少，如果果depth变为0，函数激活IRQ线并清除IRQ_DISABLED标志。

在系统初始化期间，init_IRQ()函数把每个IRQ主描述符的status字段设置成IRQ_DISABLED。此外，init_IRQ()通过替换由setup_idt()所建立的中断门来更新IDT。这是通过下列语句实现的：

```c
for(i = 0; i < NR_IRQS; i++)
	if (i + 32 != 128)
		set_intr_gate(i+32, interrupt[i]);
```

这段代码在interrupt数组中找到用于建立中断门的中断处理程序地址。interrupt数组中的第n项中存放IRQn的中断处理程序的地址(见后面“为中断处理程序保存寄存器的值”一节)。注意：这里不包括与128号中断向量相关的中断门，因为它用于系统调用的编程异常。

Linux除了支持本章前面已提到的8259A芯片外，也支持其他的几个PIC电路，如SMP IO-APIC、Intel PIIX4的内部8259 PIC及5GI的Visual Workstatio Cobalt(IO-)APIC。为了以统一的方式处理所有这样的设备，Linux用了一个“PIC对象”，由PIC名字和7个PIC标淮方法组成。这种面向对象方法的优点是，驱动程序不必关注安装在系统中的PIC种类。每个驱动程序可见的中断源透明地连接到适当的控制器。定义PIC对象的数据结构叫做hw_interrupt_type(也叫做hw_irq_controller)。

为了简单起见，让我们假定我们的计算机是有两片8259A PIC的单处理机，它提供16个标准的IRQ。在这种情况下，有16个irq_desc_t描述符，其中每个描述符的handler字段指向描述8259A PIC的i8259A_irq_type变量。这个变量被初始化为：

```c
struct hw_interrupt_type i8259A_irq_type = {
	.typename = "XT-PIC",
	.startup = startup_8259A_irq,
	.shutdown = shutdown_8259A_irq,
	.enable = enable_8259A_irq,
	.disable = disable_8259A_irq,
	.ack = mask_and_ack_8259A,
	.end = end_8259A_irq,
	.set_affinity = NULL
};
```

这个结构中的第一个字段“XT-PIC”是PIC的名字。接下来就是用于对PIC编程的六个不同的函数指针。前两个函数分别启动和关闭芯片的IRQ线。但是，在使用8259A芯片的情祝下，这两个函数的作用与第三、四个函数是一样的，第三、四个函数是启用和禁用IRQ线。mask_and_ack\_8259A()函数通过把适当的字节发往8259A I/O端口来应答所接收的IRQ。end_8259A_irq()函数在IRQ的中断处理程序终止时被调用。最后一个set_affinity()方法置为空：它用在多处理器系统中以声明特定IRQ所在CPU的“亲和力”——也就是说，那些CPU被启用来处理特定的IRQ。

如前所述，多个设备能共享一个单独的IRQ。因此，内核要维护多个irqaction描述符，其中的每个描述符涉及一个特定的硬件设备和一个特定的中断。包含在这个描述符中的字段如下表所示，标志如第二个表所示。

| 字段名     | 说明                                       |
| ------- | ---------------------------------------- |
| handler | 指向一个I/O设备的中断服务例程。这是允许多个设备共享同一IRQ的关键字段    |
| flags   | 描述IRQ与I/O设备之间的关系                         |
| mask    | 未使用                                      |
| name    | I/O设备名（通过读/proc/interrupts文件，在列出所服务的IRQ时也显示设备名） |
| dev_id  | I/O设备的私有字段。典型情况下，它标识I/O设备本身（例如，它可能等于其主设备号和次设备号）或者它指向设备驱动程序的数据 |
| next    | 指向irqaction描述符链表的下一个元素。链表中的元素指向共享同一IRQ的硬件设备 |
| irq     | IRQ线                                     |
| dir     | 指向与IRQn相关的/proc/irq/n目录的描述符              |

| 标志名              | 说明                                       |
| ---------------- | ---------------------------------------- |
| SA_INTERRUPT     | 处理程序必须以禁止中断执行                            |
| SA_SHIRQ         | 设备允许它的IRQ线与其他设备共享                        |
| SA_SAMPLE_RANDOM | 设备可以被看做是事件随机的发生源，因此，内核可以用它做随机数产生器（用户可以从/dev/random和/dev/urandom设备文件中取得随机数而访问这种特征） |

最后，irq_stat数组包含NR_CPUS个元素，系统中的每个CPU对应一个元素。每个元素的类型为irq_cpustat_t，该类型包含几个计数器和内核记录CPU正在做什么的标志（下表）。

| 字段                | 描述                           |
| ----------------- | ---------------------------- |
| __softirq_pending | 表示挂起的软中断                     |
| idle_timestamp    | CPU变为空闲的时间（只是在CPU正空闲的时候才有意义） |
| __nmi_count       | NMI中断发生的次数                   |
| apic_timer_irqs   | 本地APIC时钟中断发生的次数              |

##### IRQ在多处理器系统上的分发

Linux遵循对称多处理模型（SMP）。因而，内核试图以轮转的方式把来自硬件设备的IRQ信号在所有CPU之间分发。因此，所有CPU服务干I/O中断的执行时间片几乎相同。

在前面“高级可编程中断控制器”一节已提到，多APIC系统有复杂的机制在CPU之间动态分发IRQ信号。

在系统启动的过程中，引导CPU执行setup_IO_APIC_irqs()函数来初始化I/O APIC芯片。芯片的中断重定向表的24项披填充，以便根据“最低优先级”模式把来自I/O硬件设备的所有信号都传递给系统中的每个CPU。此外，在系统启动期间，所有的CPU都执行setup_local_APIC()函数，该函数处理本地APIC的初始化。特别是，每个芯片的任务优先级寄存器(TPR)都初始化为一个固定的值，这就意味着CPU愿意处理任何类型的IRQ信号，而不管其优先级。Linux内核启动以后再也不修改这个值。

因为所有的任务优先级寄存器都包含相同的值，因此，所有CPU总是具有相同的优先级。为了突破这种约束，正如前面所解释的那样，多APIC系统使用本地APIC仲裁优先级寄存器中的值。因为这样的值在每次中断后都自动改变，因此，IRQ信号就公平地在所有CPU之间分发。

简而言之，当硬件设备产生了一个中断信号时，多APIC系统就选择其中的一个CPU，并把该信号传递给相应的本地APIC，本地APIC又依次中断它的CPU。这个事件不通报给其他所有的CPU。

所有这些都由硬件神奇地完成，因此，多APIC系统初始化后无需内核费心。遗憾的是在有些情况下，硬件不能以公平的方式在微处理器之间成功地分配中断。因此，在必要的时候，Linux2.6利用叫做kirqd的特殊内核线程来纠正对CPU进行的IRQ的自动分配。

内核线程为多APIC系统开发了一种优良特性，叫做CPU的IRQ亲和力：通过修改I/O APIC的中断重定向表表项，可以把中断信号发送到某个特定的CPU上。set_ioapic_affinity_irq()函数用来实现这一功能，该函数有两个参数：被重定向的IRQ向量和一个32位掩码(表示可以接收这个IRQ的CPU)。系统管理员通过向文件/Iproc/irq/n/smp_affinity(n是中断向量)中写入新的CPU位图掩码也可以改变指定
中断IRQ的亲和力。

kirqd内核线程周期性地执行do_irqbalance()函数，该函数跟踪在最近时间间隔内每个CPU接收的中断次数。如果该函数发现负荷最重的CPU和负荷最轻的CPU之间IRQ负载不平衡的问题太严重，它要么把IRQ从一个CPU转移到另一个CPU,要么让所有的IRQ在所有CPU之间“轮转”。

##### 多种类型的内核栈

就像在第三章“标识一个进程”一节所提到的，每个进程的:thread_info描述符与
thread_union结构中的内核栈紧邻，而根据内核编译时的选项不同，thread_union结构可能占一个页框或两个页框。如果thread_union结构的大小为8KB，那么当前进程的内核栈被用于所有类型的内核控制路径：异常、中断和可延迟的函数（见后面“软中断及tasklet”一节）。相反，如果thread_union结构的大小为4KB，内核就使用三种类型的内核栈：

- 异常栈，用于处理异常(包括系统调用)。这个栈包含在每个进程的thread_union数据结构中，因此对系统中的每个进程，内核使用不同的异常栈。

- 硬中断请求栈，用于处理中断。系统中的每个CPU都有一个硬中断请求栈，而且每个栈占用一个单独的页框。
- 软中断请求栈，用于处理可延迟的函数(软中断或tasklet)。系统中的每个CPU都有一个软中断清求栈，而且每个栈占用一个单独的页框。

所有的硬中断请求存放在hardirq_stack数组中，而所有的软中断请求存放在softirq_stack数组中，每个数组元素都是跨越一个单独页框的irq_ctx类型的联合体。thread_info结构存放在这个页的底部，栈使用其余的内存空间，注意每个栈向低地址方向增长。所以，硬中断请求栈和软中断请求栈都与第三章“标识一个进程”一节所描述的异常栈很相似，唯一的区别是与每个栈相连的thread_info结构不是与进程而是与CPU相关联的。

hardirq_ctx和softirq_ctx数组使内核能快速确定指定CPU的硬中断请求栈和软中断请求栈，它们包含的指针分别指向相应的irq_ctx元素。

##### 为中断处理程序保存寄存器的值

当CPU接收一个中断时，就开始执行相应的中断处理程序代码，该代码的地址存放在IDT的相应门中(参见前面“中断和异常的硬件处理”一节)。

与其他上下文切换一样，需要保存寄存器这一点给内核开发者留下有点杂乱的编码工作，因为寄存器的保存和恢复必须用汇编语言代码，但是，在这些操作内部，又期望处理器从C函数调用和返回。在这一节，我们将描述处理寄存器的汇编语言任务，而下一节，我们将讨论在随后调用的C函数中所需的一些技巧。

保存寄存器是中断处理程序做的第一件事情。如前所述，IRQn中断处理程序的地址开始存在interrupt[n]中，然后复制到IDT相应表项的中断门中。

通过文件arch/i386/kernel/entry.S中的几条汇编语言指令建立interrupt数组，数组包括NR_IRQS个元素，这里NR_IRQS宏产生的数为224或16，当内核支持新近的I/O APIC芯片时(x86体系结构限制只能使用256个向量。其中32个留给CPU，因此可用向量为224个)，NR_IRQS宏产生的数为224；而当内核支持旧的8259A可编程控制器芯片时，NR_IRQS宏产生数16。数组中素引为n的元素中存放下面两条汇编语言指令的地址：

```c
pushl $n-256
jmp common_interrupt
```

结果是把中断号减256的结果保存在栈中。内核用负数表示所有的中断，因为正数用来表示系统调用(见第十章)。当引用这个数时，可以对所有的中断处理程序都执行相同的代码。这段通用代码开始于标签common_interrupt处，包括下面的汇编语言宏和指令。

```c
common_interrupt:
	SAVE_ALL
	movl %esp, %eax
	call do_IRQ
	jmp ret_from_intr
```

SAVE_ALL宏依次展开成下列片段：

```c
cld
push %es
push %ds
pushl %eax
pushl %ebp
pushl %edi
pushl %esi
pushl %edx
pushl %ecx
pushl %ebx
movl $__USER_DS, %edx
movl %edx, %ds
movl %edx, %es
```

SAVE_ALL可以在栈中保存中断处理程序可能会使用的所有CPU寄存器，但eflags、cs、eip、ss及esp除外，因为这几个寄存器已经由控制单元自动保存了(参见前面“中断和异常的硬件处理”一节)。然后，这个宏把用户数据段的选择符装到ds和es寄存器。

保存寄存器的值以后，栈顶的地址被存放到eax寄存器中，然后中断处理程序调用do_IRQ()函数。执行do_IRQ()的ret指令时(即函数结束时)，控制转到ret_from intr()(见后面“从中断和异常返回”一节)。

##### do_IRQ()函数

调用do_IRQ()函数执行与一个中断相关的所有中断服务例程。该函数声明为：

```c
__attribute__ ((regparm(3))) unsigned int __irq_entry do_IRQ(struct pt_regs *regs)
```

关键字regparm表示函数到eax寄存器中去找到参数regs的值。如上所见，eax指向被SAVE_ALL最后压入栈的那个寄存器在栈中的位置。

do_IRQ()函数执行下面的操作：

1. 执行irq_enter()宏，它使表示中断处理程序嵌套数量的计数器递增。计数器保存在当前进程thread_info结构的preempt_count字段中。

2. 如果thread_union结构的大小为4KB，函数切换到硬中断请求栈，并执行下面这些特殊的步骤：
   - 执行current_thread_info()函数以获取与内核栈(地址在esp中)相连的thread_info描述符的地址。
   - 把上一步获取的thread_info描述符的地址与存放在hardirq_ctx[smp_processor_id()]中的地址(与本地CPU相关的thread_info描述符的地址)相比较。如果两个地址相等，说明内核已经在使用硬中断请求栈，因此跳转到第3步，这种情况发生在内核处理另外一个中断时又产生了中断请求的时候。
   - 这一步必须切换内核栈。保存当前进程描述符指针，该指针在本地CPU的irq_ctx联合体中的thread_info描述符的task字段中。完成这一步操作就能在内核使用硬中断请求栈时使当前宏按预先的期望工作(参见第三章“标识一个进程”一节)。
   - 把esp栈指针寄存器的当前值存入本地CPU的irq_ctx联合体的thread_info描述符的previous_esp字段中(仅当为内核oop准备函数调用跟踪时使用该字段)。
   - 把本地CPU硬中断请求栈的栈顶(其值等于hardirq_ctx[smp_processor_id()]加上4095)装入esp寄存器；以前esp的值存入ebx寄存器。
3. 调用\_\_do_IRQ()函数，把指针regs和regs->orig_eax字段中的中断号传递给该函数。
4. 如果在上面的第2步已经成功地切换到硬中断请求栈，函数把ebx寄存器中的原始栈指针拷贝到esp寄存器，从而回到以前在用的异常栈或软中断请求栈。
5. 执行宏irq_exit()，该宏递减中断计数器并检查是否有可延迟函数正等待执行(见本章稍后“软中断及tasklet”一节)。
6. 结束：控制转向ret_fram_intr()的数(见后面“从中断和异常返回“一节)。

##### \_\_do_IRQ()函数

\_\_do_IRQ()函数接受IRQ号(通过eax寄存器)和指向pt_regs结构的指针(通过edx寄存器，用户态寄存器的值已经存在其中)作为它的参数。

函数相当于下面的代码段：

```c
spin_lock(&(irq_desc[irq].lock));
irq_desc[irq].handler->ack(irq);
irq_desc[irq].status &= ~(IRQ_REPLAY | IRQ_WAITING);
irq_desc[irq].status |= IRQ_PENDING;
if (!(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS)) && irq_desc[irq].action)
{
    irq_desc[irq].status |= IRQ_INPROGRESS;
    do
    {
        irq_desc[irq].status &= ~IRQ_PENDING;
        spin_unlock(&(irq_desc[irq].lock));
        handle_IRQ_event(irq, regs, irq_desc[irq].action);
        spin_lock(&(irq_desc[irq].lock));
    }while (irq_desc[irq].status & IRQ_PENDING)
    irq_desc[irq].status &= ~IRQ_INPROGRESS;
}
irq_desc[irq].handler->end(irq);
spin_unlock(&(irq_desc[irq].lock));
```

在访问主IRQ描述符之前，内核获得相应的自旋锁。在第五章我们会看到，自旋锁保护不同CPU的并发访问。在多处理器系统上，这个锁是必要的，因为同种类型的其他中断可能产生，其他CPU可能关注新中断的出现。没有自旋锁，主IRQ描述符会被几个CPU同时访间。正如我们会看到的那样，这种情况必须绝对避免。

获得自旋锁后，函数就调用主IRQ描述符的ack方法。如果使用旧的8259A PIC ，相应的mask_and_ack_8259A()函数应答PIC的中断，并禁用这条IRQ线。屏蔽IRQ线是为了确保在这个中断处理程序结束前，CPU不进一步接受这种中断的出现。请记住，\_\_do_IRQ()函数是以禁止本地中断运行的；事实上，CPU控制单元自动清eflags寄存器的IF标志，因为中断处理程序是通过IDT中断门调用的。不过，我们立即会看到，内核在执行这个中断的中断服务例程之前可能会重新激活本地中断。

然而，在使用I/O高级可编程中断控制器(APIC)时，事情更为复杂。应答中断依赖于中断类型，可能是由ack方法做，也可能延迟到中断处理程序结束(也就是应答由end方法去做)。在任何种情况下，我们都认为中断处理程序结束前，本地APIC不进一步接收这种中断，尽管这种中断的进一步出现可能被其他的CPU接受。

然后，\_\_do_IRQ()函数初始化主IRQ描述符的几个标志。设置IRQ_PENDING标志，是因为中断已被应答(在一定程度上)，但是还没有真正地处理；也清除IRQ_WAITING和IRQ_REPLAY标志(但我们现在不必关注它们)。

现在，\_\_do_IRQ()检查是否必须真正地处理中断。在三种情况下什么也不做，这在下面给予讨论：

- IRQ_DISABLED被设置

  即使相应的IRQ线被禁止，CPU也可能执行\_\_do_IRQ()函数；在后面“挽救丢失的中断”一节会找到对这种非直觉情况的解释。此外，即使PIC上的IRQ线被禁用，有问题的主板也可能产生伪中断。

- IRQ_INPROGRESS被没置

  在多处理器系统中，另一个CPU可能处理同一个中断的前一次出现。为什么不把这次出现的中断推迟到那个CPU(处理前一次中断)上去处理呢?这正是Linux所做的事情。这就导致了较简单的内核结构，因为设备驱动程序的中断服务例程不必是可重入的(它们的执行是串行的)。此外，释放的CPU很快又返回到它正在做的事上而没有弄脏它的硬件高速缓存；这对系统性能是有益的。只要一个CPU用来 执行中断的中断服务例程，IRQ_INPROGRESS标志就被设置。因此，\_\_do_IRQ()函数在开始真正工作之前对这个标志进行检查。

- irq_desc[irq].action为NULL

  当中断没有相关的中断服务例程时出现这种情况。通常情况下，只有在内核正在探测一个硬件设备时这才会发生。

让我们假定三种情况没有一种成立，因此中断必须被处理。\_\_do_IRQ()设置IRQ_INPROGRESS标志并开始一个循环。在每次循环中，函数清IRQ_PENDING标志，释放中断自旋锁，并调用handle_IRQ_event()执行中断服务例程(在后面“中断服务例程”一节中描述)。当handle_IRQ_event()终止时。\_\_do_IRQ()再次获得自旋锁，并检查IRQ_PENDING标志的值。如果该标志清0，那么，中断的进一步出现不传递给另一个CPU，因此，循环结束。相反，如果IRQ_PENDING被没置，当这个CPU正在执行handle_IRQ_event()时，另一个CPU已经在为这种中断执行do_IRQ()函数。因此，do_IRQ()执行循环的另一次反复，为新出现的中断提供服务(IRQ_PENDING是一个标志而不是计数器，因此只有第二次出现的中断才能被识别，而且do_IRQ()在每次循环中都只是丢弃再次出现的中断)。

\_\_do_IRQ()函数现在准备终止，或者是因为已经执行了中断服务例程，或者是因为无事可做。函数调用主IRQ描述符的end方法。当使用旧的8259A P1C时，相应的end_8259A_irq()重新激活IRQ线(除非出现伪中断)。当使用I/O AP1C时，end方法应答中断(如果ack方法还没有去做)。

最后，\_\_do_IRQ()释放白旋锁。

##### 挽救丢失的中断

\_\_do_IRQ()函数小而简单，但在大多数情况下它都能正常工作。IRQ_PENDINGIRQ_INPROGRESS和IRQ_DISABLED标志确保中断能被正确地处理，即使硬件失常也不例外。然而，在多处理器系统上事情可能不会这么顺利。

假定CPU有一条激活的IRQ线。一个硬件设备出现在这条IRQ线程上，且多APIC系统选择我们的CPU处理中断。在CPU应答中断前，这条IRQ线披另一个CPU屏蔽掉；结果，IRQ_DISABLED标志被设置。随后。我们的CPU开始处理挂起的中断；因此，do_IRQ()函数应答这个中断，然后返回，但没有执行中断服务例程，因为它发现IRQ_DISABLED标志披设置了。因此，在IRQ线禁用之前出现的中断丢失了。

为了应付这种局面，内核用来激活IRQ线的enable_irq()函数先检查是否发生了中断丢失，如果是，该函数就强迫硬件让丢失的中断再产生一次：

```c
spin_lock_irqsave(&(irq_desc[irq].lock), flags);
if (--irq_desc[irq].depth == 0)
{
    irq_desc[irq].status &= ~IRQ_DISABLED;
    if (irq_desc[irq].status & (IRQ_PENDING | IRQ_REPLAY) == IRQ_PENDING)
    {
        irq_desc[irq].status |= IRQ_REPLAY;
        hw_resend_irq(irq_desc[irq].handler.irq);
    }
    irq_desc[irq].handler->enable(irq);
}
spin_lock_irqrestore(&(irq_desc[irq].lock), flags);
```

函数通过检查IRQ_PENDING标志的值检测到一个中断被丢失了。当离开中断处理程序时，这个标志总置为0；因此，如果IRQ线被禁止且该标志被设置，那么，中断的一个出现已经被应答但还没有处理。在这种情况下，hw_resend_irq()函数产生一个新中断。这可以通过强制本地AP1C产生一个自我中断(self-interrupt)来达到(参看后面“处理器间中断处理”一节)。IRQ_REPLAY标志的作用是确保只产生一个自我中断。请记住，\_\_do_IRQ()函数在开始处理中断时清除那个标志。

##### 中断服务例程

如前所述，一个中断服务例程(ISR)实现一种特定设备的操作。当中断处理程序必须执行I5R时，它就调用handle_IRQ_event()函数。这个函数本质上执行如下步骤：

1. 如果SA_INTERRUPT标志清0，就用sti汇编语言指令激活本地中断。

2. 通过下列代码执行每个中断的中断服务例程：

   ```c
   retval = 0;
   do
   {
       retval != action->handler(irq, acton->dev_id, regs);
       aciton = action->next;
   }while (action)
   ```

   在循环的开始，action指向irqactian数据结构链表的开始，而irqactian表示接受中断后要采取的操作。

3. 用cli汇编语言指令禁止本地中断。

4. 通过返回局部变量retval的值而终止，也就是说，如果没有与中断对应的中断服务例程，返回0，否则返回1(见下面)。

所有的中断服务例程都作用于相同的参数(它们分别又一次通过eax、edx和ecx寄存器来传递)：

- irq

  IRQ号

- dev_id

  设备标识符

- regs

  指向内核(异常)栈的pt_regs结构的指针，栈中含有中断发生后随即保存的寄存器。pt_regs结构包括15个字段：开始的9个字段是披SAVE_ALL压入栈中的寄存器的值。第10个字段为IRQ号编码，通过orig_eax字段被引用。其余的字段对应由控制单元自动压入栈中的寄存器的值。

第一个参数允许一个单独的ISR处理几条IRQ线，第二个参数允许一个单独的ISR照顾几个同类型的设备，第三个参数允许ISR访问被中断的内核控制路径的执行上下文。实际上，大多数ISR不使用这些参数。

每个中断服务例程在成功处理完中断后都返回1，也就是说，当中断服务例程所处理的硬件设备(而不是共享相同IRQ的其他设备)发出信号时；否则返回0。这个返回码使内核可以更新在本章前面“IRQ数据结构”一节描述过的伪中断计数器。

当do_IRQ()函数调用一个ISR时，主IRQ描述符的5A_INTERRUPT标志决定是开中断还是关中断。通过中断调用的ISR可以由一种状态转换成相反的状态。在单处理器系统上，这是通过cli(关中断)和sti(开中断)汇编语言指令实现的。

ISR的结构依赖于所处理设备的特点。我们将在第六章和第十三章给出几个ISR的例子。

##### IRQ线的动态分配

在前面“中断向量”一节已经看到，有几个向量留给特定的设备，而其余的向量都被动态地处理。因此有一种方式，在该方式下同一条IRQ线可以让几个硬件设备使用，即使这些设备不允许IRQ共享。技巧就在于使这些硬件设备的活动串行化，以便一次只能有一个设备拥有这个IRQ线。

在激活一个准备利用IRQ线的设备之前，其相应的驱动程序调用request_irq()。这个函数建立一个新的irqaction描述符，并用参数值初始化它。然后调用setup_irq()函数把这个描述符插入到合适的IRQ链表。如果setup_irq()返回一个出错码，设备驱动程序中止操作，这意味着IRQ线己由另一个设备所使用，而这个设备不允许中断共享。当设备操作结束时，驱动程序调用free_irq()函数从IRQ链表中删除这个描述符，并释放相应的内存区。

让我们用一个简单的例予看一下这种方案是怎么工作的。假定一个程序想访问/dev/fd0设备文件对应于系统中的第一个软盘(通常不允许IRQ共享)。程序要做到这点，可以通过直接访问/dev/fd0，也可以通过在系统上安装一个文件系统。通常将IRQ6分配给软盘控制器，给定这个号，软盘驱动程序发出下列请求：

```c
request_irq(6, floppy_interrupt, SA_INTERRUPT | SA_SAMPLE_RANDOM, "floppy", NULL);
```

我们可以观察到，floppy_interrupt()中断服务例程必须以关中断(设置SA_INTERRUPT)的方式来执行，井且不共享这个IRQ(清5A_SHIRQ标志)。设置SA-SAMPLE-RANDOM标志意味着对软盘的访问是内核用于产生随机数的一个较好的随机事件源。当软盘的操作被终止时(要么终止对/dev/fd0的I/O操作，要么卸载这个文件系统)，驱动程序就释放IRQ6：

```c
free(6, NULL);
```

为了把一个irqactian描述符插入到适当的链表中，内核调用setup_irq()函数，传
递给这个函数的参数为irq_nr(即IRQ号)和new(即刚分配的irqaction描述符的地址)。这个函数将：

1. 检查另一个设备是否已经在用irq_nr这个IRQ，如果是，检查两个设备的irqaction描述符中的SA_SHIRQ标志是否都指定了IRQ线能披共享。如果不能使用这个IRQ线，则返回一个出错码。

2. 把*new(由new指向的新irqaction描述符)加到由irq_desc[irq_nr]->action指向的链表的末尾。

3. 如果没有其他设备共享同一个IRQ，清*new的flags字段的IRQ_DISABLED、IRQ_AUTODETECT、IRQ_WAITING和IRQ_INPROGRESS标志，井调用irq_desc[irq_nr]->handler PIC对象的startup方法以确保IRQ信号被激活。

举一个如何使用setup_irq()的例子，它是从系统初始化的代码中抽出的。内核通过执行time_init()函数中的下列指令，初始化间隔定时器设备的irq0描述符(参见第六章)。

```c
struct irqaction irq0 = (timer_interrupt, SA_INTERRUPT, 0, "timer", NULL, NULL);
setup_irq(0, &irq0);
```

首先，类型irqaction的irq0变量被初始化：把handler字段设置成timer_interrupt()函数的地址，flags字段设置成SA_INTERRUPT，name字段设置成"timer"，最后一个字段设置成NULL以表示没有用dev_id值。接下来，内核调用setup_irq()把irq0插人到与IRQ0相关的irqaction描述符链表中。

#### 处理器间中断处理

处理器间中断允许一个CPU向系统中的其他CPU发送中断信号。如本章前面“高级可编程中断控制器”一节所述，处理器间中断(IPI)不是通过IRQ线传输的，而是作为信号直接放在连接所有CPU本地APIC的总线上(在较老的主板上是一条专门的总线，而在基于Pentium 4的主板上就是系统总线)。

在多处理器系统上，Linux定义了下列三种处理器间中断：

- CALL_FUNCTION_VFCTOR(向量0xfb)

  发往所有的CPU(不包括发送者)，强制这些CPU运行发送者传递过来的函数。相应的中断处理程序叫做call_function_interrupt()。例如，地址存放在全局变量call_data中来传递的函数，可能强制其他所有CPU都停止，也可能强制它们设置内存类型范围寄存器(Memory Type Range Register, MTRR)(Intel微处理器包含这些附加的寄存器以易于定制高速缓存的操作)的内容。通常，这种中断发往所有的CPU，但通过smp_call_function()执行调用函数的CPU除外。

- RESCHEDULE_VECTOR(向最0xfc)

  当一个CPU接收这种类型的中断时，相应的处理程序(叫做reschedule_interrupt())限定自己来应答中断。当从中断返回时，所有的重新调度都自动进行(参见本章后面“从中断和异常返回”一节)。

- INVALIDATE_TLB_VECTOR(向量0xfd)

  发往所有的CPU〔不包括发送者)，强制它们的转换后援缓冲器(TLB)变为无效。相应的处理程序(叫做invalidate_interrupt())刷新处理器的某些TLB表项，正如在第二章“处理硬件高速缓存和TLB”一节所描述的那样。

处理器间中断处理程序的汇编语言代码是由BUILD_INTERRUPT宏产生的：它保存寄存器，从栈顶压入向量号减256的值，然后调用高级C函数。其名字就是低级处理程序的名字加前缀smp_，例如，CALL_FUNCTION_VECTOR类型的处理器间中断的低级处理程序是call_function_interrupt()，它调用名为smp_call_function_interrupt()的高级处理程序。每个高级处理程序应答本地AP1C上的处理器间中断，然后执行由中断触发的特定操作。

由于下列的一组函数，使得产生处理器间中断(IPI)变为一件容易的事：

- send_IPI_all()

  发送一个IPI到所有的CPU

- send_IPI_allbutself()

  发送一个IPI到所有的CPU(包括发送者)

- send_IPI_self()

  发送一个IPI到发送者的CPU

- send_IPI_mask()

  发送一个[PI到位掩码指定的一组CPU

### 软中断及tasklet

我们前面在“中断处理”一节提到，在由内核执行的几个任务之间有些不是紧急的，在必要情况下它们可以延迟一段时间。回忆一下，一个中断处理程序的几个中断服务例程之间是串行执行的，并且通常在一个中断的处理程序结束前，不应该再次出现这个中断。相反，可延迟中断可以在开中断的情况下执行。把可延迟中断从中断处理程序中抽出来有助于使内核保持较短的响应时间。这对于那些期望它们的中断能在几毫秒内得到处理的“急迫”应用来说是非常重要的。

Linux 2.6通过两种非紧迫、可中断内核函数：可延迟函数(也成为软中断，为避免与编程异常相混淆称为可延迟函数。Intel手册中，编程异常被称为软中断)(包括软中断与tasklet)和工作队列来执行的函数。

tasklet是在软中断之上实现。事实上，出现在内核代码中的术语“软中断(softirq)”常常表示可延迟函数的所有种类。另外一种披广泛使用的术语是“中断上下文”：表示内核当前正在执行一个中断处理程序或一个可延迟的函数。

软中断的分配是静态的(即在编译时定义)，而tasklet的分配和初始化可以动态进行(例如:安装一个内核模块时)。软中断(即便是同一种类型的软中断)可以并发地运行在多个CPU上。因此，软中断是可重入函数而且必须明确地使用自旋锁保护其数据结构。tasklet不必担心这些问题：因为内核对tasklet的执行进行了更加严格的控制。相同类型的tasklet总是被串行地执行，换句话说就是：不能在两个CPU上同时运行相同类型的tasklet。但是，类型不同的tasklet可以在几个CPU上并发执行。tasklet的串行化使tasklet函数不必是可重人的，因此简化了设备驱动程序开发者的工作。

一般而言，在可延迟函数上可以执行四种操作：

- 初始化(initialization)定义一个新的可延迟函数。这个操作通常在内核自身初始化或加载模块时进行。
- 激活(activation)标记一个可延迟函数为“挂起”(在可延迟函数的下一轮调度中执行)。激活可以在任何时候进行(即使正在处理中断)。
- 屏蔽(masking)有选择地屏蔽一个可延迟函数，这样，即使它披激活，内核也不执行它。
- 执行(execution)执行一个挂起的可延迟函数和同类型的其他所有挂起的可延迟函数；执行是在特定的时间进行的。

激活和执行不知何故总是捆绑在一起：由给定CPU激活的一个可延迟函数必须在同一个CPU上执行。没有什么明显的理由说明这条规则对系统性能是有益的。把可延迟函数绑定在激活CPU上从理论上说可以更好地利用CPU的硬件高速缓存。毕竟，可以想象，激活的内核线程访问的一些数据结构，可延迟函数也可能会使用。然而，当可延迟函数运行时，因为它的执行可以延迟一段时间，因此相关高速缓存行很可能就不再在高速缓存中了。此外，把一个函数绑定在一个CPU上总是一种有潜在“危险的”操作，因为一个CPU可能忙死而其他CPU又无所事事。

#### 软中断

Linux 2.6使用有限个软中断。在很多场合，tasklet是足够用的，且更容易编写，因为tasklet不必是可重入的。事实上，如下表所示。目前只定义了六种软中断。

| 软中断             | 下标   | 说明              |
| --------------- | ---- | --------------- |
| HI_SOFTIRQ      | 0    | 处理高优先级的tasklet  |
| TIMER_SOFTIRQ   | 1    | 和时钟中断相关的tasklet |
| NET_TX_SOFTIRQ  | 2    | 数据包传送到网卡        |
| NET_RX_SOFTIRQ  | 3    | 从网卡接受数据包        |
| SCSI_SOFTIRQ    | 4    | SCSI命令的后台中断处理   |
| TASKLET_SOFTIRQ | 5    | 常规tasklet       |

一个软中断的下标决定了它的优先级：低下标意味着高优先级，因为软中断函数将从下标0开始执行。

##### 软中断所使用的数据结构

表示软中断的主要数据结构是softirq_vec数组，该数组包含类型为softirq_action的32个元素。一个软中断的优先级是相应的softirq_action元素在数组内的下标。softirq_action数据结构包括两个字段：指向软中断函数的一个action指针和指向软中断函数需要的通用数据结构的data指针。

另外一个关键的字段是32位的preempt_count字段，用它来跟踪内核抢占和内核控制路径的嵌套，该字段存放在每个进程描述符的thread_info字段中。如下表所示，preempt_count字段的编码表示三个不同的计数器和一个标志。

| 位     | 描述                       |
| ----- | ------------------------ |
| 0~7   | 抢占计数器（max value = 255）   |
| 8~15  | 软中断计数器（max value = 255）  |
| 16~27 | 硬中断计数器（max value = 4096） |
| 28    | PREEMPT_ACTIVE标志         |

第一个计数器记录显式禁用本地CPU内核枪占的次数，值等于0表示允许内核抢占。第二个计数器表示可延迟函数被禁用的程度(值为0表示可延迟函数处于激活状态)。第三个计数器表示在本地CPU上中断处理程序的嵌套数(irq_enter()宏递增，irq_exit()宏递减它的值)。

给preempt_count字段起这个名字的理由是很充分的：当内核代码明确不允许发生抢占(抢占计数器不等于0)或当内核正在中断上下文中运行时，必须禁用内核的抢占功能。因此，为了确定是否能够抢占当前进程，内核快速检查preempt_count字段中的相应值是否等于0。

宏in_interrupt()检查current_thread_info()->preempt_count字段的硬中断计数器和软中断计数器，只要这两个计数器中的一个值为正数，该宏就产生一个非零值，否则产生一个零值。如果内核不使用多内核栈，该宏只检查当前进程的thread_info描述符的preempt_count字段。但是，如果内核使用多内核栈，则该宏可能还要检查本地CPU的irq_ctx联合体中thread_info描述符的preempt_count字段。在这种情况下，由于该字段总是正数值，所以宏返回非零值。

实现软中断的最后一个关键的数据结构是每个CPU都有的32位掩码(描述挂起的软中断)，它存放在irq_cpustat_t数据结构(回忆一下，在系统中每个CPU有一个这样的数据结构)的\_\_softirq_pending字段中。为了获取或设置位掩码的值，内
核使用宏local_softirq_pending()，它选择本地CPU的软中断位掩码。

##### 处理软中断

open_softirq()函数处理软中断的初始化。它使用三个参数：软中断下标、指向要执行的软中断函数的指针及指向可能由软中断函数使用的数据结构的指针。open_softirq()限制自己初始化softirq_vec数组中适当的元素。

raise_softirq()函数用来激活软中断，它接受软中断下标nr作为参数，执行下面的操作：

1. 执行local_irq_save宏以保存eflags寄存器IF标志的状态值并禁用本地CPU上的中断。

2. 把软中断标记为挂起状态，这是通过设置本地CPU的软中断掩码中与下标nr相关的位来实现的。

3. 如果in_interrupt()产生为1的值，则跳转到第5步。这种情况说明：要么已经在中断上下文中调用raise_softirq()，要么当前禁用了软中断。

4. 否则，就在需要的时候去调用wakeup_softirq()以唤醒本地CPU的ksoftirqd内核线程。

5. 执行local_irq_restore宏，恢复在第1步保存的IF标志的状态值。


应该周期性地(但又不能太频繁地)检查活动(挂起)的软中断，检查是在内核代码的几个点上进行的。这在下列几种情况下进行(注意，检查点的个数和位置随内核版本和所支持的硬件结构而变化)：

- 内核调用local_bh_enable()函数激活本地CPU的软中断时。
- 当do_IRQ()完成了I/O中断的处理时或调用irq_exit()宏时。
- 如果系统使用I/O APIC，则当smp_api_timer_interrupt()函数处理完本地定时器中断时。    在多处理器系统中，当CPU处理完被CALL_FUNCTION_VECTOR处理器间中断所触发的函数时。
- 当一个特殊的ksoftirqd/n内核线程被唤醒时。

##### do_softirq()数

如果在这样的一个检查点(local_softirq_pending()不为0)检测到挂起的软中断，内核就调用do_softirq()来处理它们。这个函数执行下面的操作：

1. 如果in_interrupt()产生值1，函数返回。这种情况说明要么在中断上下文中调用了do_softirq()函数，要么当前禁用软中断。

2. 执行local_irq_save宏以保存eflags寄存器IF标志的状态值并禁用本地CPU上的中断。

3. 如果thread_union的结构大小为4KB，那么在需要的情况下，它切换到软中断请求栈。
4. 调用\_\_do_softirq()函数。
5. 如果在上面第3步成功切换到软中断请求栈，则把最初的栈指针恢复到esp寄存器中，这样就切换回到以前使用的异常栈。

6. 执行local_irq_restore以恢复在第2步保存的IF标志(表示本地是关中断还是开中断)的状态值并返回。

##### \_\_do_softirq()函数

\_\_do_softirq()函数读取本地CPU的软中断掩码并执行与每个设置位相关的可延迟函数。由于正在执行一个软中断函数时可能出现新挂起的软中断，所以为了保证可延迟函数的低延迟性，\_\_do_softirq()一直运行到执行完所有挂起的软中断。但是，这种机制可能迫使\_\_do_softirq()运行很长一段时间，因而大大延迟用户态进程的执行。因此，\_\_do_softirq()只做固定次数的循环，然后就返回。如果还有其余挂起的软中断，内核线程ksoftirqd将会在预期的时间内处理它们。下面简单描述\_\_do_softirq()函数执行的操作：

1. 把循环计数器的值初始化为10。

2. 把本地CPU(被local_softirq_pending()选中的)软中断的位掩码复制到局部变量pending中。

3. 调用local_bh_disable()增加软中断计数器的值。在可延迟函数开始执行之前应该禁用它们，这似乎有点违反直觉，但确实极有意义。因为在绝大多数情况下可延迟函数是在开中断的状态下运行的，所以在执行\_\_do_softirq()的过程中可能会产生新的中断。当do_IRQ()执行irq_exit()宏时，可能有另外一个\_\_do_softirq()函数的实例开始执行。这种情况是应该避免的，因为可延迟函数必须以串行的方式在CPU上运行。因此，\_\_do_softirq()函数的第一个实例禁用可延迟函数，以使每个新的函数实例将会在do_softirq()函数的第1步就退出。

4. 清除本地CPU的软中断位图，以便可以激活新的软中断(在第2步，已经把位图保存在pending局部变量中)。

5. 执行local_bh_enable()来激活本地中断。

6. 根据局部变量pending每一位的设置，执行对应的软中断处理函数。下标为n的软中断函数的地址存放在softirq_vec[n]->action变量中。

7. 执行local_bh_disable()以禁用本地中断。

8. 把本地CPU的软中断位掩码复制到局部变量pending中，并且再次递减循环计数器。
9. 如果pending不为0，那么从最后一次循环开始，至少有一个软中断被激活，而且循环计数器仍然是正数，跳转回到第4步。
10. 如果还有更多的挂起软中断，则调用wakeup_softirq()唤醒内核线程来处理本地CPU的软中断。

11. 软中断计数器减1，因而重新激活可延迟函数。

##### ksoftirqd内核线程

每个CPU都有自己的ksoftirqd/n内核线程(n为CPU的逻辑号)。每个ksoftirqd/n内核线程都运行ksoftirqd()函数，该函数实际上执行下列的循环：

```c
for(;;)
{
    set_current_state(TASK_INTERRUPTIBLE);
    schedule();
    /*now in TASK_RUNNING state*/
    while(local_softirq_pending())
    {
        preempt_disable();
        do_softirq();
        preempt_enable();
        cond_resched();
    }
}
```

当内核线程被唤醒时，就检查local_softirq_pending()中的软中断位掩码并在必要时调用do_spftirq()。如果没有挂起的软中断，函数把当前进程状态置为TASK_INTERRUPTIBLE，随后，如果当前进程需要(当前thread_info的TIF_NEED_RESCHED标志被设置)就调用cond_resched()函数来实现进程切换。

ksoftirgd/n内核线程为重要而难以平衡的问题提供了解决方案。

软中断函数可以重新激活自己；实际上，网络软中断和tasklet软中断都可以这么做。此外，像网卡上数据包泛滥这样的外部事件可能以高频率激活软中断。

软中断的连续高流量可能会产生问题，该问题就是由引入的内核线程来解决的。没有内核线程，开发者实际上就面临两种选择策略。

第一种策略就是忽路do_softirq()运行时新出现的软中断。换句话说，do_softirq()函数开始执行时，确定哪些软中断是挂起的，然后执行这些软中断的函数。接下来，do_softirq()不再重新检查挂起的软中断就终止。这种解决方法不是很好。假设一个软中断函数在do_softirq()执行期间被重新激活。在最坏的情况下，即使机器空闲，也只有在下一次时钟中断到来时，该软中断才被再执行。结果，对网络开发者来说，软中断的等待时间是不可接受的。

第二种策略在于不断地重新检查挂起的软中断。do_softirq()函数一直检查挂起的软中断，只有在没有挂起的软中断时才终止。尽管这种解决方法可能满足了网络开发者的愿望，但是，它肯定会使系统中的普通用户感到恼怒：如果网卡接收高频率的数据包流，或者如果一个软中断函数总是激活自己，那么，do_softirq()函数就会永不返回，用户态程序实际上就停止执行。

ksoftirqd/n内核线程试图解决这种很难平衡的问题。do_softirq()函数确定哪些软中断是挂起的，井执行它们的函数。如果已经执行的软中断又被激活，do_softirq()则唤醒内核线程并终止(\_\_do_softirq()的第10步)。内核线程有较低的优先级，因此用户程序就有机会运行；但是，如果机器空闲，挂起的软中断就很快被执行。

#### tasklet

tasklet是I/O驱动程序中实现可延迟函数的首选方法。如前所述，tasklet建立在两个叫做HI_SOFTIRQ和TASKLET_SOFTIRQ的软中断之上。几个tasklet可以与同一个软中断相关联，每个taskLet执行自己的函数。两个软中断之间没有真正的区别，只不过do_softirq()先执行HI_SOFTIRQ的tasklet，后执行TASKLET_SOFTIRQ的tasklet。

tasklet和高优先级的tasklet分别存放在tasklet_vec和tasklet_hi_vec数组中。二者都包含类型为tasklet_head的NR_CPUS个元素，每个元素都由一个指向tasklet描述符链表的指针组成。taskLet描述符是一个TaskLet_struct类型的数据结构，其字段如下表所示。

| 字段名   | 描述             |
| ----- | -------------- |
| next  | 指向链表中下一个描述符的指针 |
| state | tasklet的状态     |
| count | 锁计数器           |
| func  | 指向tasklet函数的指针 |
| data  | 无符号长整数         |

tasklet描述符的state字段含有两个标志：

- TASKLET_STATE_SCHED

  该标志被设置时，表示tasklet是挂起的(曾被调度执行)，也意味着tasklet描述符被插入到tasklet_vec和tasklet_hi_vec数组的其中一个链表中。

- TASKLET_STATE_RUN

  该标志被设置时，表示tasklet正在被执行；在单处理器系统上不使用这个标志，因为没有必要检查特定的tasklet是否在运行。

写一个设备驱动程序且想使用tasklet，应该做些什么呢?首先，应该分配一个新的tasklet_struct数据结构，并调用tasklet_init()初始化它；该函数接收的参数为tasklet描述符的地址、tasklet函数的地址和它的可选整型参数。

调用tasklet_disable_nosync()或tasklet_disable()可以选择性地禁止tasklet。这两个函数都增加tasklet描述符的count字段，但是后一个函数只有在tasklet函数已经运行的实例结束后才返回。重新激活tasklet，调用tasklet_enable()。

激活tasklet，应该根据tasklet需要的优先级，调用tasklet_schedule()函数或tasklet_hi_schedule()函数。这两个函数非常类似，其中每个都执行下列操作：

1. 检查TASKLET_STATE_SCHED标志。如果设置则返回(tasklet已经被调度)。

2. 调用local_irq_save保存IF标志的状态并禁用本地中断。

3. 在tasklet_vec[n]或tasklet_hi_vec[n]指向的链表的起始处增加tasklet描述符(表示本地CPU的逻辑号)。

4. 调用raise_softirq_irqoff()激活TASKLET_SOFTIRQ或HI_SOFTIRQ类型的软中断。(这个函数与raise_softirq()函数类似，只是raise_softirq_irqoff()函数假设己经禁用了本地中断。)

5. 调用local_irq_restore恢复IF标志的状态。

最后，看一下tasklet如何被执行。从前一节知道，软中断函数一旦被激活，就由do_softirq()函数执行。与HI_SOFTIRQ软中断相关的软中断函数叫做tasklet_hi_action()，而与TASKLET_SOFTIRQ相关的函数叫做tasklet_action()。这两个函数非常相似，它们都执行下列操作：

1. 禁用本地中断。

2. 获得本地CPU的逻辑号n。

3. 把tasklet_vec[n]或tasklet_hi_vec[n]指向的链表的地址存入局部变盈list。

4. 把tasklet_vec[n]或tasklet_hi_vec[n]的值赋为NULL，因此，已调度的tasklet描述符的链表被清空。

5. 打开本地中断。

6. 对于list指向的链表中的每个tasklet描述符：

   - 在多处理器系统上，检查tasklet的TASKLET_STATE_RUN标志。
     - 如果该标志披设置，说明同类型的一个tasklet正在另一个CPU上运行，因此，就把任务描述符重新插入到由tasklet_vec[n]或tasklet_hi_vec[n]指向的链表中，并再次激活TASKLET_SOFTIRQ或HI_SOFTIRQ软中断。这样，当同类型的其他tasklet在其他CPU上运行时，这个tasklet就被延迟。
     - 如果TASKLETSTATE_RUN标志未被设置，tasklet就没有在其他CPU上运行，就需要设置这个标志，以便tasklet函数不能在其他CPU上执行。
   - 通过查看tasklet描述符的count字段，检查tasklet是否披禁止。如果是，就清TASKLET_STATE_RUN标志，并把任务描述符重新插入到由tasklet_vec[n]或tasklet_hi_vev[n]指向的链表中，然后函数再次激活TASKLET_SOFTIRQ或HI_SOFTIRQ软中断。
   - 如果tasklet被激活，清TASKLET_STATE_SCHED标志，井执行tasklet函数。


注意，除非tasklet函数重新激活自己。否则，tasklet的每次激活至多触发tasklet函数的一次执行。

### 工作队列

在Linux 2.6中引入了工作队列，用来代替之前的任务队列。它们允许内核函数(非常像可延迟函数)被激活，而且稍后由一种叫做工作者线程(worker thread)的特殊内核线程来执行。

尽管可延迟函数和工作队列非常相似，但是它们的区别还是很大的。主要区别在于：可延迟函数运行在中断上下文中，而工作队列中的函数运行在进程上下文中。执行可阻塞函数(例如:需要访问磁盘数据块的函数)的唯一方式是在进程上下文中运行。因为，正如本章前面“中断和异常处理程序的嵌套执行”一节所见，在中断上下文中不可能发生进程切换。可延迟函数和工作队列中的函数都不能访问进程的用户态地址空间。事实上，可延迟函数被执行时不可能有任何正在运行的进程。另一方面，工作队列中的函数是由内核线程来执行的，因此，根本不存在它要访问的用户态地址空间。

#### 工作队列的数据结构

与工作队列相关的主要数据结构是名为workqueue_struct的描述符，它包括一个有NR_CPUS个元素的数组。每个元素都是cpu_workqueue_struct类型的描述符。有关数据结构的字段如下表所示。

| 字段名          | 描述                               |
| ------------ | -------------------------------- |
| lock         | 保护该结构的自旋锁                        |
| worklist     | 挂入链表的头结点                         |
| more_work    | 等待队列，其中的工作者线程因等待更多的工作而处于睡眠状态     |
| current_work |                                  |
| wq           | 指向workqueue_struct结构的指针，其中包含该描述符 |
| thread       | 指向结构中工作者线程的进程描述符指针               |

cpu_workqueue_struct结构的worklist字段是双向链表的头，链表集中了工作队列中的所有挂起函数。work_struct数据结构用来表示每一个挂起函数，它的字段如下表所示。

| 字段名   | 描述                   |
| ----- | -------------------- |
| data  | 传递给挂起函数的参数，指针        |
| entry | 指向挂起函数链表前一个或后一个元素的指针 |
| func  | 挂起函数的地址              |

#### 工作队列函数

create_workqueue("foo")函数接收一个字符串作为参数，返回新创建工作队列的workqueue_struct描述符的地址。该函数还创建n个工作者线程(n是当前系统中有效运行的CPU的个数)，并根据传递给函数的字符串为工作者线程命名。create_singlethread_workqueue()函数与之相似，但不管系统中有多少个CPU，函数都只创建一个工作者线程。内核调用destroy_workqueue()函数撤消工作队列，它接收指向workqueue_struct数组的指针作为参数。

queue_work()(封装在work_struct描述符中)把函数插入工作队列，它接收wq和work两个指针。wq指向workqueue_struct描述符，work指向work_struct描述符。queue_work()主要执行下面的步骤：

1. 检查要插入的函数是否已经在工作队列中，如果是就结束。

2. 把work_struct描述符加到工作队列链表中。

3. 如果工作者线程在本地CPU的cpu_workqueue_struct描述符的more_work等持队列上睡眠，该函数唤醒这个线程。

queue_delayed_work()函数和queue_work()几乎是相间的，只是queue_delayed_work()函数多接收一个以系统滴答数来表示时间延迟的参数，它用于确保挂起函数在执行前的等待时间尽可能短。事实上，queue_delayed_work()依靠软定时器
把work_struct描述符插入工作队列链表的实际操作向后推迟了。如果相应的work_struct描述符还没有插入工作队列链表，cancel_delayed_work()就删除曾被调度过的工作队列函数。

每个工作者线程在worker_tread()函数内部不断地执行循环操作，因而，线程在绝大多数时间里处于睡眠状态并等待某些工作被插入队列。工作线程一旦被唤醒就调用run_workqueue()函数，该函数从工作者线程的工作队列链表中剧除所有work_struct描述符并执行相应的挂起函数。由于工作队列函数可以阻塞，因此，可以让工作者线程睡眠，甚至可以让它迁移到另一个CPU上恢复执行(一个工作者线程不仅仅被与cpu_workqueue_struct描述符相关的CPU执行，而且能被所有CPU执行)。

有些时候，内核必须等待工作队列中的所有挂起函数执行完毕。flush_workqueue()函数接收warkqueue_struct描述符的地址，并且在工作队列中的所有挂起函数结束之前使调用进程一直处于阻塞状态。但是该函数不会等待在调用flush_workqueue()之后新加入工作队列的挂起函数。

#### 预定义工作队列

在绝大多数情况下，为了运行一个函数而创建整个工作者线程开销太大了。因此，内核引入叫做events的预定义工作队列，所有的内核开发者都可以随意使用它。预定义工作队列只是一个包括不同内核层函数和I/O驱动程序的标准工作队列，它的workqueue_strut描述符存放在keventd_wq数组中。为了使用预定义工作队列，内核提供下表列出的函数。

| 预定义工作队列函数                | 等价的标准工作队列函数                              |
| ------------------------ | ---------------------------------------- |
| schedule_work            | queue_work(keventd_wq, work)             |
| schedule_delayed_work    | queue_delayed_work(keventd_wq, dwork, delay) 在任何CPU上 |
| schedule_delayed_work_on | queue_delayed_work_on(cpu, keventd_wq, dwork, delay) 在某个指定的CPU上 |
| flush_scheduled_work     | flush_workqueue(keventd_wq)              |

当函数很少被调用时，预定义工作队列节省了重要的系统资源。另一方面，不应该使在预定义工作队列中执行的函数长时间处于阻塞状态。因为工作队列链表中的挂起函数是在每个CPU上以串行的方式执行的，而太长的延迟对预定义工作队列的其他用户会产生不良影响。

除了一般的events队列，在Linux 2.6中还有一些专用的工作队列。其中最重要的是块设备层使用的kblockd工作队列。

### 从中断和异常返回

我们通过考察中断和异常处理程序的终止阶段来结束本章(从系统调用返回是个特例，将在第十章给予描述)。尽管终止阶段的主要目的很清楚，即恢复某个程序的执行，但是，在这样做之前，还需要考虑几个问题：

- 内核控制路径井发执行的数量

  如果仅仅只有一个，那么CPU必须切换到用户态。

- 挂起进程的切换请求

  如果有任何请求，内核就必须执行进程调度，否则，把控制权还给当前进程。

- 挂起的信号

  如果一个信号发送到当前进程，就必须处理它。

- 单步执行模式

  如果调试程序正在跟踪当前进程的执行，就必须在进程切换回到用户态之前恢复单步执行。

- Virtual-8086模式

  如果CPU处于virtual-8086模式，当前进程正在执行原来的实模式程序，因而必须以特殊的方式处理这种情况。

需要使用一些标志来记录挂起进程切换的请求、挂起信号和单步执行，这些标志被存放在thread_info描述符的flags字段中，这个字段也存放其他与从中断和异常返回无关的标志。下表完整地列出了与中断和异常返回相关的标志。

| 标志名               | 描述                        |
| ----------------- | ------------------------- |
| TIF_SYSCALL_TRACE | 正在跟踪系统调用                  |
| TIF_SIGPENDING    | 进程有挂起信号                   |
| TIF_NEED_RESCHED  | 必须执行调度程序                  |
| TIF_SINGLESTEP    | 临返回用户态前恢复单步执行             |
| TIF_IRET          | 通过iret而不是sysexit从系统调用强行返回 |
| TIF_SYSCALL_AUDIT | 系统调用正在被审计                 |
| TIF_MEMDIE        | 正在撤销进程以回收内存               |

从技术上说，完成所有这些事情的内核汇编语言代码并不是一个函数，因为控制权从不返回到调用它的函数。它只是一个代码片段，有两个不同的入口点，分别叫做ret_from_intr()和ret_from_exceptiont()。正如其名所暗示的，中断处理程序结束时，内核进入ret_from_intr()，而当异常处理程序结束时，它进入ret_from_exceptiont()。为了描述起来更容易一些，我们将把这两个人口点当作函数来讨论。

下图是关于两个入口点的完整的流程图。灰色的框图涉及实现内核抢占(参见第五章)的汇编语言指令，如果只想了解不支持枪占的内核都做了些什么，就可以忽略这些灰色的框图。在流程图中，入口点ret_from_intr()和ret_from_exceptiont()看起来非常相似，它们的唯一区别是：如果内核在编译时选择了支持内核抢占，那么从异常返回时要立刻禁用本地中断。

//待画图

流程图大致给出了恢复执行披中断的程序所必需的步骤。现在，我们要通过讨论汇编语言代码来详细描述这个过程。

#### 入口点

ret_from_intr()和ret_from_exceptiont()入口点从本质上相当于下面这段汇编语言代码：

```asm
ret_from_exception:
    cli;    missing if kernel preempt is not supported
ret_from_intr:
    movl $-8192, %esp;  -4096 if multiple Kernel Mode stacks are used
    andl %esp, %ebp
    movl 0x30(%esp), %eax
    movb 0x2c(%esp), %al
    testl $0x00020003, %eax
    jnz resume_userspace
    jmp resume_kernel
```

回忆前面对handle_IRQ_event()描述的第3步，在中断返回时，本地中断是被禁用的。因此，只有在从异常返回时才使用cli这条汇编语言指令。

内核把当前thread_info描述符的地址装载到ebp寄存器。

接下来，要根据发生中断或异常时压入栈中的cs和eflags寄存器的值，来确定被中断的程序在中断发生时是否运行在用户态，或确定是否设置了eflags的VM标志。在任何一种情况下，代码的执行就跳转到resume_userspace处。否则，代码的执行跳转到resume_kernel处。

#### 恢复内核控制路径

如果被恢复的程序运行在内核态，就执行resume_kernel处的汇编语言代码：

```asm
resume_kernel:
    cli         ; these three instructions are missing if kernel preemption is not supported
    cmpl $0, 0x14(%esp)
    jz need_resched
restore_all:
    popl %ebx
    popl %ecx
    popl %edx
    popl %esi
    popl %edi
    popl %ebp
    popl %eax
    popl %ds
    popl %es
    addl $4, %esp
    iret
```

如果thread_info描述符的preempt_counC字段为0(允许内核抢占)，则内核跳转到need_resched处，否则，被中断的程序重新开始执行。函数用中断和异常开始时保存的值装载寄存器，然后通过执行iret指令结束其控制。

#### 检查内核抢占

执行检查内核抢占的代码时，所有没执行完的内核控制路径都不是中断处理程序，否则preempt_count字段的值就会是大于0的。但是正如在本章“中断和异常处理程序的嵌套执行”一节所强调的，最多可能有两个与异常(其中个正在结束)相关的内核控制路径。

```asm
need_resched:
    movl 0x8(%ebp), %ecx
    testb $(1 << TIF_NEED_RESCHED), %cl
    jz restore_all
    testl $0x00000200, 0x30(%esp)
    jz restore_all
    call preempt_schedule_irq
    jmp need_resched
```

如果current->thread_info的flags字段中的TIF_NEED_RESCHED标志为0,说明没有需要切换的进程，因此，程序跳转到restore_all处。如果正在被恢复的内核控制路径是在禁用本地CPU的情况下运行，那么也跳转到restore_all处。在这种情况下，进程切换可能破坏内核数据结构(详情参见第五章“什么时候同步是必需的”一节)。

如果需要进行进程切换，就调用preempt_schedule_irq()数；它设置preempt_count字段的PREEMPT_ACTIVE标志，把大内核锁计数器暂时置为-l(参见第五章“大内核锁”一节)，打开本地中断并调用schedule()以选择另一个进程来运行。当前面的进程要恢复时，preempt_schedule_irq()使大内核计数器的值恢复为以前的值，清除PREEMPT_ACTIVE标志并禁用本地中断。但当前进程的TIF_NEED_RESCHED标志被设置，将继续调用schedule()函数。

#### 恢复用户态程序

如果要恢复的程序原来运行在用户态，就跳转到resume_userspace处：

```asm
resume_userspace:
    cli
    movl 0x8(%esp), %ecx
    andl $0x0000ff6e, %ecx
    je restore_all
    jmp work_pending
```

禁用本地中断之后，检测current->thread_info的flags字段的值。如果只设置了TIF_SYSCALL_TRACE、TIF_SYSCALL_AUDIT或TIF_SINGLESTEP标志，就不做任何其他的事情，只是跳转到restore_all，从而恢复用户态程序。

#### 检测重调度标志

thread_info descriptor描述符的flags表示在恢复被中断的程序之前，需要完成额外的工作。

```asm
work_pending:
    testb $(1 << TIF_NEED_RESCHED), %cl
    jz work_notifysig
work_resched:
    call schedule
    cli
    jmp resume_userspace
```

如果进程切换请求被挂起，就调用schedule()选择另外一个进程投入运行。当前面的进程要恢复时，就跳转回到resume_userspace处。

#### 处理挂起信号、虚拟8086模式和单步执行

除了处理进程切换请求，还有其他的工作需要处理：

```asm
work_notifysig:
    movl %esp, %eax
    testl $0x00020000, 0x30(%esp)
    je 1f
work_notifysig_v86:
    pushl %ecx
    call save_v86_state
    popl %ecx
    movl %eax, %esp
1:
    xorl %edx, %edx
    call do_notify_resume
    jmp restore_all
```


如果用户态程序eflags寄存器的VM控制标志被设置，就调用save_v86_state()函数在用户态地址空间建立虚拟8086模式的数据结构。然后，调用do_notify_resume()函数处理挂起信号和单步执行。最后，跳转到restore_all标记处，恢复被中断的程序。

## 第5章 内核同步

### 内核如何为不同的请求提供服务

#### 内核抢占

#### 什么时候同步是必需的

#### 什么时候同步是不必要的

### 同步原语

#### 每CPU变量

#### 原子操作

#### 优化和内存屏蔽

#### 自旋锁

##### 具有内核抢占的spin_lock宏

##### 非抢占式内核中的spin_lock宏

##### 读/写自旋锁

###### 为读获取和释放一个锁

###### 为写获取和释放一个锁

#### 顺序锁

#### 读-拷贝-更新（RCU）

#### 信号量

##### 获取和释放信号量

##### 读/写信号量

#### 补充原语（完成变量）

#### 禁止本地中断

#### 禁止和激活可延迟函数

### 对内核数据的同步访问

#### 在自旋锁、信号量及中断禁止之间选择

### 避免竞争条件的实例

#### 引用计数器

#### 大内核锁

#### 内存描述符读/写信号量

#### slab高速缓存链表的信号量

#### 索引节点的信号量

## 第6章 定时测量

## 第7章 进程调度

## 第8章 内存管理

## 第9章 进程地址空间

## 第10章 系统调用

## 第11章 信号

## 第12章 虚拟文件系统

## 第13章 I/O体系结构和设备驱动程序

## 第14章 块设备驱动程序

## 第15章 页高速缓存

## 第16章 访问文件

## 第17章 回收页框

## 第18章 Ext2和Ext3文件系统

## 第19章 进程通信

## 第20章 程序的执行

## 附录一 系统启动

## 附录二 模块